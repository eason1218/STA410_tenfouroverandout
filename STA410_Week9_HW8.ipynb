{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c83339c",
   "metadata": {},
   "source": [
    "# STA410 Week 9 Homework 8 (4 points)\n",
    "\n",
    "Due 12 PM Feb 28 (before class starts)\n",
    "\n",
    "1. **Paired or individual assignment.** Work may be shared within pairs without restriction, but collaborations beyond the pairs must be limited to \"hints\" and may not share complete solutions.\n",
    "\n",
    "\n",
    "2. You are encouraged to adapt code you find available online **(including using AI chat bot suppport tools)** into your notebook; however, if you do so please provide a link to the utilized resource. ***If you don't and multiple students have the same answer an academic integrity proceeding may be undertaken.***  \n",
    "\n",
    "\n",
    "3. **Library imports are limited** to only libraries imported in the starter code and the [standard python modules](https://docs.python.org/3/py-modindex.html). Automated code tests that fail because of additional library imports will not recieve credit. Unless a problem instructs differently you may use any functions available from the Python stdlib and the libraries imported in the starter code.\n",
    "\n",
    "\n",
    "<details><summary><span style=\"color: blue; text-decoration: underline; cursor: pointer;\">Additional Details</span></summary>\n",
    "\n",
    "> **Do not delete, replace, or rearranged cells.** This erases `cell ids` upon which automated code tests are based. The \"Edit > Undo Delete Cells\" option in the notebook editor might be helpful; otherwise, redownload the notebook (so it has the correct required `cells ids`) and repopulate it with your answers (assuming you don't overwrite them when you redownload the notebook). ***You may add cells for scratch work*** but if required answers are not submitted through the provided cells where the answers are requested your answers may not be marked. Due to potential problems with `cell ids` **the only environments supported in this class are** [UofT JupyterHub](https://datatools.utoronto.ca/) or [Google Colab](https://colab.research.google.com/)\n",
    ">\n",
    "> **No jupyter shortcut commands** such as `! python script.py 10` or `%%timeit` may be included in the final submission as they will cause subsequent automated code tests to fail.\n",
    ">\n",
    "> **No cells may have any runtime errors** because this causes subsequent automated code tests to fail and you will not get marks for tests which fail because of previous runtime errors. ***Restart and re-run the cells in your notebook to ensure there are no runtime errors before submitting your work.***\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6009d48a",
   "metadata": {},
   "source": [
    "## Student and Contribution\n",
    "\n",
    "Are you working with a partner to complete this assignment?  \n",
    "- If not, assign  the value of `None` into the variable `Partner`.\n",
    "- If so, assign the name of the person you worked with into the variable `Partner`.\n",
    "    - Format the name as `\"<First Name> <Last Name>\"` as a `str` type, e.g., \"Scott Schwartz\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e558c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Partner = #None\n",
    "# This cell will produce a runtime error until you assign a value to this variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8b75ad",
   "metadata": {},
   "source": [
    "What was your contribution in completing the code for this assignments problems?  \n",
    "Assign one of the following into each of the `Contribution` variable below.\n",
    "\n",
    "- `\"I worked alone\"`\n",
    "- `\"I contributed more than my partner\"`\n",
    "- `\"My partner and I contributed equally\"`\n",
    "- `\"I contributed less than my partner\"`\n",
    "- `\"I did not contribute\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3b54a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Contribution = #\"I worked alone\"\n",
    "# This cell will produce a runtime error until you assign a value to this variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51be707a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you may use any functions available from the following library imports\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692b7c21",
   "metadata": {},
   "source": [
    "## Part 1 (2 points): Alternating Conditional Expectation (ACE) algorithm\n",
    "\n",
    "The linear association of two variables is measured with correlation \n",
    "\n",
    "$$\\rho_{xy} = \\frac{Cov(X,Y)}{\\sqrt{Var(X)Var(Y)}} \\quad \\textrm{ estimated as } \\quad \n",
    "\\frac{\\sum_{i=1}^n(x_i-\\bar x)(y_i^2-\\bar y)}{\\sqrt{\\sum_{i=1}^n(x_i-\\bar x)^2}\\sqrt{\\sum_{i=1}^n(y_i-\\bar y)^2}}$$\n",
    "\n",
    "\n",
    "And two variables are **linearly independent** if $\\rho_{xy}=0$ but are not necessrily **statistically independent**.\n",
    "For example, $\\rho_{xy}=0$ for $Y=X^2+\\epsilon$ with $X\\sim \\text{Uniform}(-1,1)$ and $\\epsilon\\sim \\text{Normal}(\\mu=0, \\sigma=0.1)$, but these variables are clearly **statistically dependent** and $\\rho_{x^2y}\\approx0.948$.\n",
    "\n",
    "The **maximal correlelation** $\\displaystyle \\max_{\\phi, \\psi} \\rho_{\\phi(x)\\psi(y)}$ is the transformation of the random variables producing the larget **correlation** between the variables possible. The **maximal correlelation** in the above example is achieved at $\\phi(x)=x^2$ and $\\psi(y)=y$.\n",
    "\n",
    "Finding **maximal correlelation** means finding $\\phi$ and $\\psi$ which transform the **joint distribution** $p(X,Y)$ to $p(\\phi(X),\\psi(Y))$ which is as close as possible to a **standard bivariate normal distirbution** with $E[\\phi(X)] = 0$, $E[\\phi(X)^2] = 1$, $E[\\psi(Y)] = 0$, $E[\\psi(Y)^2] = 1$ and **maximal correlelation** $\\displaystyle \\max_{\\phi,\\psi} E[\\phi(X)\\psi(Y)] = \\rho_{\\max} \\in [0,1]$ which occurs when \n",
    "$$E[\\phi(X)|Y=y] =\\int \\phi(x) \\frac{p(x,y)}{p(y)}dx =\\psi(y)\\rho_{\\max}$$\n",
    "$$E[\\psi(Y)|X=x] =\\int \\psi(y) \\frac{p(x,y)}{p(x)}dy =\\phi(x)\\rho_{\\max}$$\n",
    "\n",
    "There is no closed form solution for ***maximal correlelation***, but the above requirements suggest iterative estimation through **backfitting** as\n",
    "\n",
    "- $\\hat{\\phi(x)} = E[\\hat{\\psi(y)}|x]$ followed by replacing $\\hat{\\phi(x)}$ with its standardization $\\frac{\\hat{\\phi(x)} - \\text{mean}(\\hat{\\phi(x)})}{\\text{std}(\\hat{\\phi(x)})}$\n",
    "- $\\hat{\\psi(y)} = E[\\hat{\\phi(x)}|y]$ followed by replacing $\\hat{\\psi(y)}$ with its standardization $\\frac{\\hat{\\psi(y)} - \\text{mean}(\\hat{\\psi(y)})}{\\text{std}(\\hat{\\psi(y)})}$\n",
    "\n",
    "where the expectations $\\hat{\\phi(x)} = E[\\hat{\\psi(y)}|x]$ and $\\hat{\\psi(y)} = E[\\hat{\\phi(x)}|y]$ suggest the need for flexible prediction which can be executed via **LOWESS smoothing** or **bsplines** regression.\n",
    "\n",
    "Initialize `phi_y` $=\\hat{\\psi_0(y)} = y$  and use the **standardized** output of `sm.nonparametric.lowess(phi_0_y, x, frac=p, return_sorted=False)` for `phi_x` $= \\hat{\\phi_1(x)}$ and then use the **standardized** output of `sm.nonparametric.lowess(phi_x, y, frac=p, return_sorted=False)` for `phi_y` $=\\hat{\\psi_1(y)}$ to complete the first update iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f3831f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(410)\n",
    "n = 1000\n",
    "x = stats.uniform(-1,2).rvs(n)\n",
    "y = x**2 + stats.norm(0,0.1).rvs(n)\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize=(8,4))\n",
    "ax[0].plot(x,y,'.'); ax[1].plot(x**2,y,'.')\n",
    "np.corrcoef(x,y)[0,1], np.corrcoef(x**2,y)[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c14a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,4,figsize=(16,4))\n",
    "\n",
    "psi_y = y; p = 0.5; T=5\n",
    "for s in range(T):\n",
    "    \n",
    "    # update phi_x\n",
    "    phi_x = x\n",
    "    ax[0].plot(x, phi_x, '.', c=str(1-(s+1)/T))\n",
    "    \n",
    "    # update psi_y\n",
    "    psi_y = y\n",
    "    ax[1].plot(y, psi_y, '.', c=str(1-(s+1)/T))\n",
    "\n",
    "ax[0].set_xlabel('$x$')\n",
    "ax[0].set_ylabel('$\\\\phi(x)$')\n",
    "ax[1].set_xlabel('$y$')\n",
    "ax[1].set_ylabel('$\\\\psi(y)$')\n",
    "ax[2].set_xlabel('$\\\\phi(x)$')\n",
    "ax[2].set_ylabel('$\\\\psi(y)$')\n",
    "ax[3].set_xlabel('$\\\\psi(y)-LOWESS(\\\\psi(y)|\\\\phi(x))$')\n",
    "ax[2].plot(phi_x, psi_y, '.')\n",
    "ax[2].set_title(\"estimated maximal correlation: \"+str(np.round(np.corrcoef(phi_x, psi_y)[0,1],5)))\n",
    "E_psi_y_G_phi_x = sm.nonparametric.lowess(psi_y, phi_x, frac=p, return_sorted=False)\n",
    "ax[2].plot(phi_x, E_psi_y_G_phi_x, 'r.')\n",
    "ax[3].hist(psi_y-E_psi_y_G_phi_x, density=True, bins=30)\n",
    "support = np.linspace(-1,1,100)*(psi_y-E_psi_y_G_phi_x).std()*3\n",
    "ax[3].plot(support, stats.norm((psi_y-E_psi_y_G_phi_x).mean(), np.std(psi_y-E_psi_y_G_phi_x)).pdf(support))\n",
    "ax[3].set_title(\"H0: normal p-value \"+\\\n",
    "      str(np.round(sm.stats.diagnostic.kstest_normal(psi_y-E_psi_y_G_phi_x, dist='norm', pvalmethod='table')[1],5)))\n",
    "np.corrcoef(phi_x, psi_y)[0,1]\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa41b4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: `c=str(1-(s+1)/T)` will let you visualize the iterative progress of the algorithm\n",
    "#       which will help you choose the number of iterations T that are needed to complete ACE\n",
    "T=5\n",
    "for s in range(T):\n",
    "    plt.plot(x+s, y, '.', c=str(1-(s+1)/T))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65678739",
   "metadata": {},
   "source": [
    "### Part 1 Questions\n",
    "1. For the data set above with `n=1000` based on `np.random.seed(410)`, find a two-decimal `p` for `frac=p` for which **ACE** algorithm indeed estimates the **maximal correlation** to be between $0.945$ and $0.950$ and demonstrate using the visualization template provided above.\n",
    "\n",
    "2. Adjust the **ACE** algorithm to fix `psi_y` under the assumption that $\\hat{\\psi(y)}=y$ and then find a two-decimal `p` for `frac=p` for which this adjsuted version of the algorithm estimates the **maximal correlation** to be between $0.947$ and $0.949$. Visually demonstrate this in the same manner as the previous problem. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41abdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.4 points each \n",
    "\n",
    "# p fraction between 0 and 1 with two decimal values \n",
    "p1q1 = None # e.g. `p=0.12` producing maximal correlation between 0.945 and 0.950 (caputring 0.948)\n",
    "\n",
    "# p fraction between 0 and 1 with two decimal values \n",
    "p1q2 = None # e.g. `p=0.12` producing maximal correlation between 0.947 and  0.949 (caputring 0.948)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f65346d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef665c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9dae12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell for scratch work\n",
    "\n",
    "# You are welcome to add cells into your notebook but\n",
    "# just don't runtime errors anywhere in your notebook\n",
    "# as cells are run sequentially for automated testing\n",
    "\n",
    "# Automated testing is looking for cells (`cell ids`)\n",
    "# where you're asked to enter answers or complete code \n",
    "# but all other cells like this one or cells you add\n",
    "# are safe to delete if you want a \"cleaner\" notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7029f55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell for scratch work\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a468b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1q3 = \"The (second) adjusted ACE algorithm <is|is not> an iterative backfitting method\"\n",
    "\n",
    "p1q4 = \"Initializing the ACE algorithm with the adjusted ACE algorithm <is|is not> just ACE\"\n",
    "\n",
    "p1q5 = \"The transformations of the ACE algorithm for `frac=0.01` could be described as <A|B|C|D>\"\n",
    "# A: smooth (continuously differentiable)\n",
    "# B: overfit (high variance)\n",
    "# C: underfit (high bias)\n",
    "# D: none of the above\n",
    "\n",
    "p1q6 = \"The transformations of the ACE algorithm for `frac=0.05` could be described as <A|B|C|D>\"\n",
    "# Same options as above\n",
    "\n",
    "p1q7 = \"The transformations of the ACE algorithm for `frac=0.50` could be described as <A|B|C|D>\"\n",
    "# Same options as above\n",
    "\n",
    "p1q8 = \"The transformations of the ACE algorithm for `frac=0.90` could be described as <A|B|C|D>\"\n",
    "# Same options as above\n",
    "\n",
    "p1q9 = 'The high bias version of this \"model\" has the character of a <U|V|W|stair step> shape'\n",
    "\n",
    "p1q10 = \"When the p-value for the `sm.stats.diagnostic.kstest_normal` test above is not significant \"+\\\n",
    "        \"then the bivariate normal assumption on the transformed data is <likely|still not> very good\"\n",
    "\n",
    "p1q11 = \"Assuming the p-value for the `sm.stats.diagnostic.kstest_normal` test being less than 0.05 \"+\\\n",
    "        'indicates \"over-transformation away from normality\" (and misestimated maximal correlation) '+\\\n",
    "        \"the values of `p` <A|B|C|D> <could|should not> be used for `frac=p` for the ACE algorithm \"\n",
    "# A: less than 0.55\n",
    "# B: between 0.6 and 0.75\n",
    "# C: greater than 0.8\n",
    "# D: all of the above \n",
    "\n",
    "p1q12 = \"Assuming the p-value for the `sm.stats.diagnostic.kstest_normal` test being less than 0.05 \"+\\\n",
    "        'indicates \"over-transformation away from normality\" (and misestimated maximal correlation) '+\\\n",
    "        \"the values of `p` <A|B|C|D> <could|should not> be used for `frac=p` for the (second) adjusted ACE algorithm\"\n",
    "# A: less than 0.55\n",
    "# B: between 0.6 and 0.75\n",
    "# C: greater than 0.8\n",
    "# D: All of the above \n",
    "\n",
    "p1q13 = \"The <A|B|C|D> in the given psi_y vs phi_x plot above suggest(s) good maximal correlation estimation\"\n",
    "# A: linearity \n",
    "# B: homoeskedasticity\n",
    "# C: Both of the above\n",
    "# D: None of the above\n",
    "\n",
    "p1q14 = \"The <A|B|C|D> in the analogous phi_x versus psi_y plot suggest(s) good maximal correlation estimation\"\n",
    "# A: linearity \n",
    "# B: homoeskedasticity\n",
    "# C: Both of the above\n",
    "# D: None of the above\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f98f20",
   "metadata": {},
   "source": [
    "## Part 2 (2 points): Gauss-Seidel and Coordinate Descent\n",
    "\n",
    "All **direct methods** solving $X^TX\\beta = X^Ty$ (our \"favorite\" $A_{p\\times p}x = b$ problem) \n",
    "are beholden by the $O(np^2)$ requirements of $X^TX$ (or **SVD** or **QR**). \n",
    "The $A_{p\\times p}x = b$ problem itself, however, can be solved either directly or with **Cholesky** in $O(p^3)$. But this will still be computationally intractable for large $p$. \n",
    "\n",
    "The solution can be iteratively approximated with $s$ steps of the $O(sp^2)$ **Gauss-Seidel** method based on the **additive factorization**\n",
    "\n",
    "\\begin{align*}\n",
    "\\color{gray}{X^T\\!X \\hat \\beta = X^T \\! y \\; \\Rightarrow \\;}\n",
    "A_{p \\times p}x = {} & b \\quad \\color{gray}{A = (L + D + U) \\text{ decomposition costs nothing}} \\\\\n",
    "(L + D + U)x = {} & b \\quad \\color{gray}{\\text{$D$ diagonal $L$/$U$ zero diagonal upper/lower triangular}} \\\\ \\hline\n",
    "(L+D)x^{(k+1)} = {} & b -  Ux^{(k)} \\\\\n",
    "x^{(k+1)} = {} & (D+L)^{-1}b - (D+L)^{-1}Ux^{(k)} \\color{gray}{= v + Cx^{(k)} \\leftarrow \\text{converge?}}\\\\\n",
    "\\color{blue}{\\text{or just } \\quad Dx^{(k+1)}} = {} & \\color{blue}{b - Lx^{(k+1)} - Ux^{(k)}} \n",
    "\\end{align*}\n",
    "\n",
    "This prescribes a sequence of iteratively updates which may improve(?) solution approximations\n",
    "\n",
    "\n",
    "$$\\color{blue}{x^{(k+1)}_i = \\frac{1}{a_{ii}}\\left( b_i - \\sum_{j=1}^{i-1} a_{ij}x_j^{(k+1)} - \\sum_{j=i+1}^{p} a_{ij}x_j^{(k)} \\right)} \\quad \n",
    "\\color{red}{\\text{started with some initial value $x^{(0)}$}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb27f5e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> **Gauss-Seidel** converges so that $\\quad\\Delta(b-Ax^{(k)}, b-Ax^{(k+1)}) \\rightarrow 0\\quad$ if any of the following hold:\n",
    ">\n",
    "> - the largest **eigenvalue**, referred to as the **spectral radius** $\\quad \\rho\\left((L + D)^{-1}U\\right)<1$\n",
    ">\n",
    ">     - and convergence is faster for smaller **spectral radius** which is proven using convergence properties of the **matrix power series** $\\sum_{k=0}^{\\infty}C^k = (I - C)^{-1}$\n",
    ">\n",
    "> - $A$ is **symmetric** and $A$ and $D$ and $D-L-U = D - L - L^T$ are ***positive definite***\n",
    ">\n",
    ">     - which is proven based on ***eigenvalues*** of $C= - (L + D)^{-1} U = - (L + D)^{-1}(D + L - A)$\n",
    ">\n",
    "> - $A$ is **strictly diagonally dominant** so  $\\quad |a_{ii}| > \\sum_{i \\neq j} |a_{ij}| \\; \\text{ for all } i$\n",
    "    - which is proven using the invertibilty of ***strictly diagonally dominant matrices***\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d44fc39",
   "metadata": {},
   "source": [
    "The **Jacobi** algorithm instead specifies \n",
    "\n",
    "$$x^{(k+1)}_i = \\frac{1}{a_{ii}}\\left( b_i - \\sum_{j=1}^{i-1} a_{ij}x_j^{\\overset{\\color{red}{\\text{or }(k)}}{(k+1)}} - \\sum_{j=i+1}^{p} a_{ij}x_j^{(k)} \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dec2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "p2q1 = \"The <Gauss-Seidel|Jacobi> algorithm requires twice as much memory \"+\\\n",
    "       \"as the <Gauss-Seidel|Jacobi> algorithm\"\n",
    "\n",
    "p2q2 = \"For the update of the ith element of x at step k+1 as given above \"+\\\n",
    "       \"these algorithms specify <p-1|p|p+1> multiplication-division operations\"\n",
    "# count division as if it was just multiplication\n",
    "\n",
    "p2q3 = \"For the update of the ith element of x at step k+1 as given above \"+\\\n",
    "       \"these algorithms specify <p-1|p|p+1> addition-substraction operations\"  \n",
    "# count subtraction as if it was just addition\n",
    "\n",
    "p2q4 = \"The total step k+1 update for these algorithms thus requires \"+\\\n",
    "       \"<2*p|p*p|2*p*p|2*p*(p+1)|2*(p-1)*p|2*(p-1)*(p-1)|2*(p-1)*(p+1)> total numeric operations\"\n",
    "\n",
    "p2q5 = \"For a sparse matrix A with m zeros on each row, if a sparse matrix data structure \"+\\\n",
    "       \"and corresponding algorithm that can ignore all zeros is used, the step k+1 update \"+\\\n",
    "       \"for these algorithms requires a total number of non-zero-based numeric operations of \"+\\\n",
    "       \"<use the formatting of the options to the previous problem but using m not 1>\"\n",
    "       #<2*p|p*p|2*p*p|2*p*(p+1)|2*(p-1)*p|2*(p-1)*(p-1)|2*(p-1)*(p+1)>\n",
    "\n",
    "p2q6 = \"The computational complexity for the sparse matrix A above and a \"+\\\n",
    "       'corresponding \"sparse Guass-Seidel algorithm\" is <A|B|C|D>'\n",
    "# A: O(p*p*p)\n",
    "# B: O((p-m)*p*p)\n",
    "# C: O((p-m)*p*s)\n",
    "# D: none of the above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9782062b",
   "metadata": {},
   "source": [
    "## Coordinate Descent \n",
    "\n",
    "---\n",
    "\n",
    "**Coordinate descent** finds an optimum of a scalar valued multivariate function $f(x)$ by iteratively optimizing one variable $x_j$ at a time, cycling through the elements of $x^{(k)}$ until stabilization. \n",
    "\n",
    "\n",
    "For example, for **symmetric positive definite** $A$ we have\n",
    "\n",
    "\\begin{align*}\n",
    "f(x) = \\frac{1}{2} x^TAx - x^Tb + c ={}& \\frac{1}{2}\\sum_j\\sum_k a_{jk}x_j x_k + \\sum_j b_j x_j + c\\\\\n",
    "\\frac{\\partial}{\\partial x_i } f(x) = {} & a_{ii}x_i - b_i + \\sum_{j \\not = i} a_{ij}x_j \\; \\overset{\\text{set to}}{= 0}\\\\\n",
    "\\Longrightarrow \\quad x_i =  {} &\\frac{1}{a_{ii}}\\left(b_i - \\sum_{j \\not = i} a_{ij}x_j\\right)\n",
    "\\end{align*}\n",
    "\n",
    "> ### Gradients\n",
    "> ---\n",
    ">\n",
    "> The **gradient operator** $\\nabla_{x}$ generalizes the **derivative** for scalar quantities as the vector of **partial derivatives** with respect to the input $x_i$ of its scalar valued multivariate function argument $f_x$\n",
    "> \n",
    "> $$ \\nabla_{x}(f_x) = \\frac{\\partial f_x}{\\partial x_1} e_1 + \\cdots + \\frac{\\partial f_x}{\\partial x_i} e_i + \\cdots + \\frac{\\partial f_x}{\\partial x_n} e_n \\quad \\text{for }\\textbf{standard basis vectors} \\;e_i$$\n",
    "> \n",
    "> The **derivative** and the **gradient operators** (functions of functions returning functions) are **linear**\n",
    "> \n",
    "> $$\\nabla_{x}(af_x+bg_x) = a\\nabla_{x}(f_x) + b\\nabla_{x}(g_x)$$\n",
    "> \n",
    "> and have some simple rules such as $\\quad \\nabla_{x} x^Tb = b \\quad \\text{ and } \\quad \\nabla_{x} x^TAx = 2Ax$\n",
    ">\n",
    "> $$\\scriptsize\n",
    "\\begin{align*}\n",
    "\\text{so for example} \\quad \\nabla_{x} \\left[ (b - Ax)^T A^{-1} (b - Ax) \\right]= {} & \n",
    "\\underbrace{\\nabla_{x} \\left[ x^TAx - 2b^Tx + b^TA^{-1}b \\right]}_{\\text{gradients have scalar-like derivative rules}\\quad\\quad\\quad\\quad\\quad}\\\\\n",
    "0 = {} & \\overbrace{- 2b^T + 2(x)^TA}\\\\\n",
    " = {} & -(b - Ax)  \\Longrightarrow Ax = b\n",
    "\\end{align*}$$\n",
    ">\n",
    "> ### Norms (and Metrics) \n",
    ">\n",
    "> --- \n",
    "> \n",
    "> For **symmetric positive definite** $A$ (such as $X^T\\!X \\hat \\beta = X^T \\! y$ for **full rank** $X$) all of the following have the same unique **global minimum** (due to their **convex** nature owing to the **positive definite** character of $A$\n",
    "> \n",
    "> - $\\underset{x}{\\text{argmin}} ||b - Ax||_p = \\overbrace{\\left( \\sum_{i=1}^n (b - Ax)_i^p \\right)^{\\frac{1}{p}}}^{L_p \\text{ norm or }\\Delta_p(b,Ax)\\text{ metric}} \\overset{p=2}{=} \\underset{1 \\times 1}{\\sqrt{(b - Ax)^T (b - Ax)}}$\n",
    "> - $\\underset{x}{\\text{argmin}} ||b - Ax||_2^{A^{-1}} = ||\\underbrace{(b - Ax)^T A^{-1} (b - Ax)}_{1 \\times 1: \\text{ mahalanobis distance}}||_2 \\quad \\text{the }\\textbf{elliptical norm}$\n",
    "> - $\\underset{x}{\\text{argmin}} \\frac{1}{2}x^TAx - x^Tb + c$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729b1e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "p2q7 = \"<A|B|C|D> below shows that coordinate descent optimization above \"+\\\n",
    "       \"solves Ax=b for symmetric positive definite A\"\n",
    "# A: grad_x [ x.T@(Ax - b) + c = Ax - b ]\n",
    "# B: grad_x [ x.TA.T@(Ax)-2b.T@Ax + b.T@b = 2x.TA.T@A-2b.T@A = A.T@(2Ax-2b) ]\n",
    "# C: both of the above\n",
    "# D: none of the above\n",
    "\n",
    "p2q8 = \"For the given quadratic function f(x) given above \"+\\\n",
    "       \"the coordinate descent algorithm is the same as the <A|B|C|D>\"\n",
    "# A: Gauss-Seidel algorithm\n",
    "# B: Jacobi algorithm\n",
    "# C: both of the above\n",
    "# D: none of the above\n",
    "\n",
    "p2q9 = \"Coordinate descent <is|is not> a form of backfitting; \"+\\\n",
    "       \"Gauss-Seidel <is|is not> a form of backfitting; \"+\\\n",
    "       \"The Jacobi algorithm <is|is not> a form of backfitting; \"+\\\n",
    "       \"Nonlinenear Gauss-Seidel <is|is not> a form of backfitting\"\n",
    "\n",
    "p2q10 = \"The eigenvalues for square symmetric positive definite matrices \"+\\\n",
    "        \"are the <same as|square of> the singular values\"\n",
    "\n",
    "# For Gauss-Seidel indidated as GS and successive overrelaxation as OR\n",
    "# and A and A_ as given below\n",
    "p2q11 = \"The fastest to slowest converging algorithm is \"+\\\n",
    "        \"GS on A1 -> GS on A2 -> GS with omega=0.5 OR on A1 -> GS with omega=0.5 OR on A2\"\n",
    "        # keep format above but order from fastest to slowest convergence\n",
    "\n",
    "p2q12 = 'Assuming \"reaches the black line\" in the plot below '+\\\n",
    "        \"indicates a sufficient approximation the then O(spp) < O(npp) \"+\\\n",
    "        \"objective can be acheived for <A1|A2|both A1 and A2|neither A1 nor A2>\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8388b5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "### Successive Overrelaxation \n",
    "---\n",
    "\n",
    "**Successive overrelaxation** uses weighted averages of current $x_i^{(k)}$ and iterative update, so for **Gauss-Seidel**  \n",
    "$$x_i^{(k+1)} = \\overbrace{\\underset{\\text{last answer $x_i^{(k)}$}}{\\underbrace{(1-\\omega)x_i^{(k)}}_{\\text{weighted with}}} + \\omega}^{\\text{if $\\omega=1$ this goes away}}\\underbrace{\\frac{1}{a_{ii}}\\left( b_i - \\sum_{j=1}^{i-1} a_{ij}x_j^{(k+1)} - \\sum_{j=i+1}^{p} a_{ij}x_j^{(k)} \\right)}_{\\text{and we're just left solving the Gauss-Seidel equations}}$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "Ax = (L + D + U )x = {} &  b \\\\ \n",
    "\\omega Lx + (\\omega - 1 + 1) D x = {} & \\omega (b -  Ux) \\\\\n",
    "\\omega Lx +  D x = {} & (1-\\omega) Dx + \\omega (b -  Ux) \\\\ \\hline\n",
    "(\\omega L +  D) x^{(k+1)} = {} & (1-\\omega) Dx^{(k)} + \\omega (b -  Ux^{(k)}) \\\\ \n",
    " D x^{(k+1)} = {} & (1- \\omega)Dx^{(k)} + \\omega( b -  Lx^{(k+1)} -  Ux^{(k)})  \n",
    "\\end{align*}$$\n",
    "\n",
    "providing faster convergence if **spectral radius** \n",
    "$\\rho\\left((\\omega L + D)^{-1}\\omega U\\right)<\\rho\\left((L + D)^{-1}U\\right)<1$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ca505c",
   "metadata": {},
   "source": [
    "### Complete the Gauss-Seidel algorithm<br>and Successive Overrelaxation version of the Gauss-Seidel algorithms below<br>for the two provided matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2030bf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 100\n",
    "A1 = np.diag(np.ones(p)) + 1.0\n",
    "A2 = np.diag(np.ones(p)) + 0.2\n",
    "\n",
    "print(np.linalg.cond(A1).round(), np.linalg.eigvals(A1).max().round(),np.linalg.svd(A1)[1].max().round())\n",
    "print(np.linalg.cond(A2).round(), np.linalg.eigvals(A2).max().round(),np.linalg.svd(A2)[1].max().round())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2207dc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "L1,D1,U1 = A1.copy(),np.diag(np.diag(A1)),A1.copy()\n",
    "L1[np.triu_indices(p)] = 0\n",
    "U1[np.tril_indices(p)] = 0\n",
    "# https://numpy.org/doc/stable/reference/generated/numpy.tril_indices.html\n",
    "\n",
    "L2,D2,U2 = A2.copy(),np.diag(np.diag(A2)),A2.copy()\n",
    "L2[np.triu_indices(p)] = 0\n",
    "U2[np.tril_indices(p)] = 0\n",
    "\n",
    "print(np.linalg.eigvals(np.linalg.inv(L1+D1)@U1)[:5])\n",
    "print(np.linalg.eigvals(np.linalg.inv(L2+D2)@U2)[:5])\n",
    "omega=0.5\n",
    "print(np.linalg.eigvals(np.linalg.inv(omega*L1+D1)@(omega*U1))[:5])\n",
    "print(np.linalg.eigvals(np.linalg.inv(omega*L2+D2)@(omega*U2))[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa869bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target\n",
    "\n",
    "x_true = np.ones((p,1))\n",
    "b1 = A1.dot(x_true)\n",
    "b2 = A2.dot(x_true)\n",
    "\n",
    "np.c_[np.linalg.solve(A1,b1),np.linalg.solve(A2,b2)][:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd8c452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize\n",
    "# For A1\n",
    "x1_GS = x_true+0.1\n",
    "x1_GSOR = x_true+0.1\n",
    "# For A2\n",
    "x2_GS = x_true+0.1\n",
    "x2_GSOR = x_true+0.1\n",
    "\n",
    "# convergence tracking\n",
    "x1true_GS_L2norm = np.zeros(int(p/2))\n",
    "x2true_GS_L2norm = np.zeros(int(p/2))\n",
    "x1true_GSOR_L2norm = np.zeros(int(p/2))\n",
    "x2true_GSOR_L2norm = np.zeros(int(p/2))\n",
    "\n",
    "for i in range(int(p/2)):\n",
    "\n",
    "    # Gauss-Seidel\n",
    "\n",
    "    x1_GS = x1_GS  # fix \n",
    "    # this can be done fairly directly with matrix multiplications but\n",
    "    # practice implementing with for loops might be helpful for the CC\n",
    "    # ...\n",
    "    # so maybe do this one based on the matrix multiplication specification\n",
    "    delta = x_true - x1_GS\n",
    "    x1true_GS_L2norm[i] = delta.T.dot(delta)[0][0]**0.5\n",
    "\n",
    "    x2_GS = x2_GS  # fix\n",
    "    # this can be done fairly directly with matrix multiplications but\n",
    "    # practice implementing with for loops might be helpful for the CC\n",
    "    # ...\n",
    "    # and then maybe do this one based on just using the for loop specification\n",
    "    delta = x_true - x2_GS\n",
    "    x2true_GS_L2norm[i] = delta.T.dot(delta)[0][0]**0.5\n",
    "    \n",
    "    # Successive Overrelaxation \n",
    "    omega=0.5\n",
    "\n",
    "    x1_GSOR = x1_GSOR\n",
    "    # this can be done fairly directly with matrix multiplications but\n",
    "    # practice implementing with for loops might be helpful for the CC\n",
    "    # ...\n",
    "    # so maybe do this one based on the matrix multiplication specification\n",
    "    delta = x_true - x1_GSOR\n",
    "    x1true_GSOR_L2norm[i] = delta.T.dot(delta)[0][0]**0.5\n",
    "\n",
    "    x2_GSOR = x2_GSOR\n",
    "    # this can be done fairly directly with matrix multiplications but\n",
    "    # practice implementing with for loops might be helpful for the CC\n",
    "    # ...\n",
    "    # and then maybe do this one based on just using the for loop specification\n",
    "    delta = x_true - x2_GSOR\n",
    "    x2true_GSOR_L2norm[i] = delta.T.dot(delta)[0][0]**0.5\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(x1true_GS_L2norm, label=\"Gauss-Seidel with $\\\\kappa$(A1)=\"+str(np.linalg.cond(A1).round()))\n",
    "plt.plot(x2true_GS_L2norm, label=\"Gauss-Seidel with $\\\\kappa$(A2')=\"+str(np.linalg.cond(A2).round()))\n",
    "plt.plot(x1true_GSOR_L2norm, label=\"Gauss-Seidel with $\\\\kappa$(A1)=\"+str(np.linalg.cond(A1).round())+\"\\n with $\\\\omega=$\" + str(omega) + \" Successive Overrelaxation\", linestyle='--')\n",
    "plt.plot(x2true_GSOR_L2norm, label=\"Gauss-Seidel with $\\\\kappa$(A2')=\"+str(np.linalg.cond(A2).round())+\"\\n with $\\\\omega=$\" + str(omega) + \" Successive Overrelaxation\", linestyle='--')\n",
    "plt.plot([0,p/2],2*[0],'k-')\n",
    "plt.title(\"$||x^{true}-x^{(k)}||_2^2$ for iterative updates to $Ax = b$\")\n",
    "plt.xlabel(\"k\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a1ba5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell for scratch work\n",
    "\n",
    "# You are welcome to add cells into your notebook but\n",
    "# just don't runtime errors anywhere in your notebook\n",
    "# as cells are run sequentially for automated testing\n",
    "\n",
    "# Automated testing is looking for cells (`cell ids`)\n",
    "# where you're asked to enter answers or complete code \n",
    "# but all other cells like this one or cells you add\n",
    "# are safe to delete if you want a \"cleaner\" notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af7f9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell for scratch work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbe8371",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224177f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
