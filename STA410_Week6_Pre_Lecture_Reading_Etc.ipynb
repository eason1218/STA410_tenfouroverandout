{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fe194c1",
   "metadata": {},
   "source": [
    "# Prereading\n",
    "\n",
    "Familiarize yourself with the following.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67c0149",
   "metadata": {},
   "source": [
    "\n",
    "## \"Big O\" Algorithmic Complexity\n",
    "\n",
    "---\n",
    "\n",
    "The \"Big O\" algorithmic complexity of an algorithm measures the *proportional* computational demands of the algorithm as a function of the input size $n$ of the problem. An algorithm requiring $f(n)$ operations to complete a problem with input size $n$ has \"Big O\" algorithmic complexity order $g(n)$, written as \n",
    "\n",
    "$$f(n) = O(g(n)) \\quad \\text{ if } \\quad f(n) \\leq M g(n) \\quad \\text{ for all $n>n_0$}$$\n",
    "\n",
    "for strictly positive $g$ and some constant $M$ that is not a function of $n$.\n",
    "\n",
    "Determining \"Big O\" algorithmic complexity requires counting operations and then dropping lower order terms and ignoring proportinality coefficients. Thus $f(n) = an^2+bn+c = O(n^2)$ and the $O(n^2)$ only characterizes the computational characteristics of an algorithm in the context of very large $n$. \n",
    "\n",
    "The actual runtime of an algorithm for a given input size $n$ depends on $a$ and the lower order terms $bn+c$ but these are not characterized by $O(n^2)$ since as $n$ grows large $an^2+bn+c< Mn^2$ for some large $M$. Of course for $n=100$ then the actual computational requirements for $f(n)=100n = O(n)$ and $g(n)=n^2 = O(n^2)$\n",
    "would be indistinguishable even thought $O(n) < O(n^2)$. \n",
    "\n",
    "The actual runtime of an algorithm for a given input size $n$ also depends on the cost of the operations it employs.  The speeds of the functions we've been considering of late, such as `sum`,`np.sum`,`math.fsum`,`fan1`, `fan2`, `ss_one_pass`, and `ss_two_pass` might all be considered $O(n)$ with respect to the addition operations they employ. But of course not all operations are created equal (as is particularly notable in the with respect to the vectorization employe by `np.sum`), and the addition operations are not the only operations these functions employ (which is especially distinct in terms of their memory usage and access aspects of the algorithms they implement).\n",
    "\n",
    "> How do the speeds of `ss_one_pass`, `ss_two_pass`, `x.var(ddof=0)*len(x)`, and `lambda x: (x**2).sum() - x.sum()**2/len(x) # x an np.array` compare and why? The `numpy` functions are certainly utilizing a \"fan\" based algorithm which is actually $n\\log(n)$ in its memory usage, so does that explain the speed differences?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84dd3598",
   "metadata": {},
   "source": [
    "### $O(n)$ and $O(1)$\n",
    "\n",
    "A fun edge case of \"Big O\" algorithmic complexity is $O(1)$, which means that the computational requirements do not change as a function of the size of the input data. Examples of $O(1)$ and $O(n)$ are \n",
    "\n",
    "- Storage requirements beyond an input buffer needed to calculate a running mean, sum, or variance from streaming data (since these computations all admit \"realtime\" streaming one pass calculations, as opposed to *online \"out of core\" algorithm)\n",
    "    - Variance calculations using only a single pass of the data are **real-time** algorithms; and, covariance matrix calculations also admit **real-time** forms; and, linear regression model fitting in fact also has a **real-time** form (which doesn't even requires any covariance matrix calculations). \n",
    "- Median calculations do not admit an $O(1)$ storage requirement. The storage requirements of a median calculation are $O(n)$ because to determine which data point is the median the complete data set must be sorted in memory. \n",
    "- An algorithm with $O(n)$ storage requirements is called a **batch** algorithm and requires constant access to the input data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc0ab7f",
   "metadata": {},
   "source": [
    "### $O(n^2)$ and $O(n^3)$\n",
    "\n",
    "Examples of \"Big O\" algorithmic complexity $O(n^2)$ and $O(n^3)$ are matrix-vector multiplication and matrix-matrix multiplication, respectively. \n",
    "\n",
    "- matrix-vector multiplication $A_{n \\times n} x_{n \\times 1}$ requires $n^2$ multiplications and $n(n − 1)$ additions: $n^2 + n(n − 1) = O(n^2)$\n",
    "\n",
    "  > $10,000^2 = 100,000,000$ might be computationally tractable; but, $n^2$ for larger $n$ quickly starts to look computationally unappealing...\n",
    "\n",
    "- matrix-matrix multiplication $A_{n \\times n} B_{n \\times n}$ requires $n$ matrix-vector multiplication: $n\\times(n^2 + n(n − 1)) = O(n^3)$\n",
    "\n",
    "  > $1000^3 = 1,000,000,000$ *might* be computationally tractable; but, $n^3$ for larger $n$ immediately looks like a computational death trap...\n",
    "\n",
    "In terms of the problem input size $n$, it becomes quickly apparent that the computational requirements of \"Big O\" algorithmic complexity $O(n^2)$ and $O(n^3)$ algorithms scale tremendously differently as $n$ increases, and that neither is particularly attractive as a function of $n$ relative to \"Big O\" algorithmic complexity $O(n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2551a5",
   "metadata": {},
   "source": [
    "### $O(n\\log(n)) \\text{ and } O(n^{\\log_2 7}) \\approx O(n^{2.81})$\n",
    "\n",
    "---\n",
    "\n",
    "\"Big O\" algorithmic complexity $O(n \\log n)$ arises from  algorithms that are able to leverage a **divide and conquer** stategy that can be readily seen to arise through [recursion](https://www.google.com/search?q=recursion) (but of course might still be implemented iteratively).\n",
    "\n",
    "```python\n",
    "divNconq_recursion(x):\n",
    "    if stopping_criterion_met:\n",
    "        return small_problem_answer\n",
    "    # otherwise extract `half_of_x` and `other_half_of_x` and\n",
    "    return DivnConq_recursion(half_of_x) + DivnConq_recursion(other_half_of_x)\n",
    "```\n",
    "\n",
    "If a **divide and conquer** stategy which reduces a problem into two separate problems, each of half the size as the original, can be identified then this algorithm can be repeatedly recursivesly applied until only a collection of small problems remain. And this will result in a computationally tractable $O(n\\log n)$ algorithm that is feasible for large $n$ compared to $O(n^2)$.\n",
    "\n",
    "- If a problem is $O(n^2)$ then the **divide and conquer** algorithm above instead solves two $O((n/2)^2) = O(n^2)$. And while this is still of course $O(n^2)$, it is for an input problem size that is half as small as the initial problem. And halfs of halfs of halfs, etc. get pretty small pretty fast. So `return small_problem_answer` is still $O(n^2)$ but the `n` has become very small...\n",
    "\n",
    "\n",
    "- An input of size $n$ be can recursively split in half $\\log_2 n$ times since definitionally $2^{\\log_2 n} = n$. At each stage of subsequent recursive splitting, all of the $n$ elements must still be accessed in order to determine the new splits.  So for each $2^{\\log_2 n}$ splitting stage, $n$ input elements are accessed, and this is what leads to the $O(n\\log n)$ \"Big O\" algorithmic complexity of ***divide and conquer*** algorithms. \n",
    "\n",
    "The famous **Fast Fourier Transform**\n",
    "- $\\tilde x = Ax, \\;A_{jk} = e^{-k\\left(\\frac{j}{n}\\right) 2\\pi i}$\n",
    "- $x = A^{-1}\\tilde x, \\;A^{-1}_{jk} = \\frac{1}{n}e^{k\\left(\\frac{j}{n}\\right) 2\\pi i}$\n",
    "\n",
    "is a special matrix-vector multiplication operation that can be computed in an $O(n \\log n)$ recursive ***divide and conquer*** algorithm, as opposed to the typical $O(n^2)$ computation of standard matrix-vector multiplication.  Thus, if the problem being solved is in some way benefited by working the transformed space,  then the **Fourier Transform** itself will likely not contribute a limiting computational cost. \n",
    "\n",
    "**Strassen's divide and conquer matrix multiplication algorithm** achieves $O(n^{\\log_2 7}) \\approx O(n^{2.81})$ which is a significant imporovement over $O(n^3)$ for moderate $n$.\n",
    "\n",
    "The `fan` summation methods considered of late requires $O(n)$ summations but $O(n \\log n)$ memory access actions. The additional computational factor of $O(n \\log n)$ relative to $O(n)$ of sequential summation provides a reduced roundoff error improvement that will be well worth the minor $\\log n$ computational cost factor for most $n$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de391d9",
   "metadata": {},
   "source": [
    "### An [OPTIONAL] Counting Example with Convolutions\n",
    "\n",
    "Determining $O(f(n))$ is simply a matter of defining what's being counted, and then carefully counting it. \n",
    "\n",
    "> Consider the following (***convolution***) algorithm $f(n)$ computing the distribution of $p(X_1+\\cdots+X_n)$ for discrete $X_i \\overset{i.i.d.}{\\sim} p(X_i=x)$ with $x \\in \\{0, \\cdots, K\\}$:\n",
    ">\n",
    "> $$\\begin{align*}\n",
    "p(X_1+X_2 = s) = & {} \\sum_{x=0}^{\\min(s,K)} p(X_1=s-x) p(X_2=x)\\\\\n",
    "p(X_1+X_2+X_3 = s) = & {} \\sum_{x=0}^{\\min(s,K)} p(X_1+X_2 = s-x) p(X_3=x)  \\\\\n",
    "p(X_1+X_2+X_3+X_4 = s) = & {} \\sum_{x=0}^{\\min(s,K)} p(X_1+X_2+X_3 = s-x) p(X_4=x)  \\\\\n",
    "\\vdots  & {}\n",
    "\\end{align*}$$\n",
    "\n",
    "For this example some possible counting targets might be \n",
    "- How many multiplications are required to define $p(X_1+\\cdots+X_n)$?\n",
    "- How many additional additions are required to define $p(X_1+\\cdots+X_n)$?\n",
    "- How much storage is required to compute $p(X_1+\\cdots+X_n)$?\n",
    "\n",
    "> To answer the first counting question about multiplication, note that at each stage \n",
    ">  1. $\\quad p(X_1+X_2)$\n",
    ">  2. $\\quad p(X_1+X_2+X_3)$\n",
    ">\n",
    ">    $\\quad\\quad\\quad\\quad\\vdots$\n",
    "> \n",
    "> $\\quad$k. $\\quad p(X_1+\\cdots+X_k = s)$ must be calculated for $s \\in \\{0, \\cdots, kK\\}$\n",
    "> \n",
    "> and each \n",
    "> - $p(X_1+\\cdots+X_k = s)$ calculation requires the sum of $\\min(s,K)+1$ multiplications of $p(X_k)$ with the previous $p(X_1+\\cdots+X_{k-1})$, i.e.,\n",
    ">\n",
    ">   $$\\sum_{x=0}^{\\min(s,K)} \\underbrace{p(X_1+\\cdots+X_{k-1} = s-x)}_{\\text{can be cached as a time-space tradeoff}} p(X_k=x)$$\n",
    ">\n",
    ">   which can be cached (as a ***time-space tradeoff***) if the algorithm is approached in the natural sequential manner.\n",
    ">\n",
    "> Thus, the total number of multiplications required is \n",
    ">\n",
    "> $$\\begin{align*}\n",
    "\\overset{\\text{stage}}{\\sum_{k=2}^n} \\overbrace{\\sum_{s=0}^{kK} \\big(\\min(s,K)+1\\big)}^{\\text{stage $k$ multiplications}} & \\leq {} \\sum_{k=1}^n \\sum_{s=0}^{kK} \\big(K+1\\big)\\\\\n",
    "& = {} \\sum_{k=1}^n (kK+1) (K+1)\\\\\n",
    "O\\left(\\sum_{k=1}^n (kK+1) (K+1)\\right) & = {} O\\left(\\sum_{k=1}^n kK^2+kK + K+1 \\right)\\\\\n",
    "& = {} O\\left(\\sum_{k=1}^n kK^2\\right) = O\\left( \\frac{n(n+1)}{2}K^2\\right)\\\\\n",
    "& = {} O\\left(n^2K^2\\right)\n",
    "\\end{align*}$$\n",
    "\n",
    "> There is an alternative algorithmic approach for this problem based on the ***moment generating function***\n",
    "> \n",
    "> $$\\begin{align*}\n",
    "E\\left[t^S\\right] = {} & \\sum_{s=0}^{nK} t^s p(S=s) \\\\\n",
    "= E\\left[t^{X_1 + \\cdots + X_n}\\right] = {} &  \\underbrace{E\\left[\\prod_{i=1}^n t^{X_i}\\right] = \\prod_{i=1}^nE\\left[ t^{X_i}\\right]}_{\\text{since } X_1, \\cdots, X_n \\text{ are independent}}\\\\  \n",
    "= {} & \\left(\\sum_{x=0}^{K} t^x p(X_i=x) \\right)^n\\\\ \n",
    "\\end{align*}$$\n",
    "> \n",
    "> which for $nK+1$ distinct values of $t$, results in $nK+1$ equations with $nK+1$ unknowns, i.e., $p(S=s)$ for $s=0,\\cdots,nK$; however, solving a sytem of $nK+1$ equations is generally an $O(n^3K^3)$ computation, so this formulation alone will not improve upon the initial algorithm.   \n",
    "\n",
    "<!-- \n",
    "*The example in this section is inspired by Keith Knight's STA410 [class notes](https://q.utoronto.ca/courses/244990/files?preview=18669454); however, rather than summing over the possible values of the accumulating random variable $(s)$, the summation in the formulation presented here is done only over all possible values of the newly contributed random variable $(x)$, which is an improved formulation since $O\\left(K^2 n^2\\right) < O\\left(K^2 n^3\\right)$;  indeed, the formulation achieves the same algorithmic complexity as a* ***fast Fourier transform*** *because in the discrete case the computation can be restricted to only the arithmetic necessary for the solution.*\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029cb6b0",
   "metadata": {},
   "source": [
    "## Direct Methods\n",
    "\n",
    "**Direct methods** execute a set sequence of predefined deterministic computational steps to arrive at the exact solution (to the limits of available precision) to a computational problem. The end of homework for the this week explores several **direct methods** to solve $X^TX\\hat \\beta = X^Ty$\n",
    "for $\\hat \\beta$ which provides the **least squares** approximation $X\\hat \\beta \\approx  y$. \n",
    "\n",
    "> All the various `sum` and `variance` computational algorithms we're considered were of course examples of **direct methods**. **Gaussian elimination**, **backward substiution**, the **singular value**, **Cholesky**, and **QR decompositions**, and **Gram-Schmidt orthogonalization**, etc. are also all **direct methods**.\n",
    "\n",
    "All of the methods providing the **least squares** estimate(for $X_{n\\times p}$ with $n>m$ are $O(np^2)$; although, they have quite different computational run times for a given $n$ and $m$ since $M$ in $f(n,m)\\leq Mnm^2$ can be (and is) quite different for each algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7799dec9",
   "metadata": {},
   "source": [
    "###  \"Big O\" Algorithmic Complexity for \"Least Squares\" methods\n",
    "\n",
    "---\n",
    "\n",
    "The \"***Big O***\" ***algorithmic complexity*** for $n>m$ of all of the ***direct methods*** above is $O(nm^2)$. \n",
    "\n",
    "- $(X^TX)^{-1} (X_{n\\times m}^Ty) = O(nm^2)$ since $X^TX$ dominates the total operations together which are sequentially $O(nm^2) + O(m^3) + O(nm) + O(m^2)$ \n",
    "    - $X_{n\\times m}^TX_{n\\times m} = O(nm^2)$ \n",
    "    - $(A_{m\\times m})^{-1} = O(m^3)$ \n",
    "    - $X_{n\\times m}^Ty = O(nm)$\n",
    "    - $A_{m\\times m}B_{m\\times 1} = O(m^2)$.\n",
    "\n",
    "\n",
    "- `np.linalg.solve(X.T@X, X.T@y)` is again $O(nm^2)$ with $X_{n\\times m}^TX_{n\\times m} = O(nm^2)$ again dominating all operations together which are sequentially $O(nm^2) + O(nm) + O(m^3) + O(m^2)$\n",
    "    - $X_{n\\times m}^TX_{n\\times m} = O(nm^2)$ \n",
    "    - $X_{n\\times m}^Ty = O(nm)$\n",
    "    - **Gaussian elimination** with a required operation count of $2 \\times \\left(\\frac{m(m+1)(2m+1)}{6} - m + \\frac{m(mm+1)}{2} - m \\right) = O(m^3)$ \n",
    "    - **backward substitution** with a required operation count of $m^2 = O(m^2)$ \n",
    "\n",
    "\n",
    "- The **QR decomposition** `Q,R = np.linalg.qr(X)` using [**Householder transformations**](https://math.stackexchange.com/questions/501018/what-is-the-operation-count-for-qr-factorization-using-householder-transformatio) which are more numerically accurate than the **(modified) Graham-Schmidt procedure** is $O(nm^2)$ and dominates the subsequent \n",
    "    - **backward substitution** is again $m^2 = O(m^2)$ \n",
    "\n",
    "\n",
    "- The **Cholesky decomposition** is a **Gaussian elimination** like process of $O(m^3)$, but the prerequesite  $X_{n\\times m}^TX_{n\\times m} = O(nm^2)$ again dominates all operations together which are sequentially $O(nm^2) + O(m^3) + O(nm) + O(m^2) + O(m^2)$\n",
    "    - $X_{n\\times m}^TX_{n\\times m} = O(nm^2)$\n",
    "    - $X_{n\\times m}^Ty = O(nm)$\n",
    "    - **Cholesky decomposition** is $O(m^3)$\n",
    "    - and the two rounds of **backward substitution** are each $m^2 = O(m^2)$ \n",
    "\n",
    "\n",
    "- The [standard computation](https://www.mathworks.com/matlabcentral/answers/420057-what-is-the-complexity-of-matlab-s-implementation-of-svd) of the ***SVD*** [implemented](https://scicomp.stackexchange.com/questions/6979/how-is-the-svd-of-a-matrix-computed-in-practice) by `np.linalg.lstsq(X, y)` is $O(nm^2)$\n",
    "    - If **SVD** has been computed then $\\hat \\beta = VD^{-1}U^T y$ would be $O(m)$ + $O(mn)$ + $O(m^2)$\n",
    "        - $VD^{-1} = O(m)$\n",
    "        - $U^T y = O(mn)$\n",
    "        - $A_{m\\times m}B_{m\\times 1} = O(m^2)$\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a05d3c",
   "metadata": {},
   "source": [
    "## SVD \"review\"\n",
    "\n",
    "---\n",
    "\n",
    "For any matrix $A_{n\\times m}$ there exists a **(compact) singular value decomposition (SVD)** $A_{n\\times m} = U_{n\\times r} D_{r\\times r} (V^T)_{r\\times m}$\n",
    "\n",
    "> $U_{n\\times r}$ and $V_{m\\times r}$ are ***semi-orthonormal matrices***\n",
    "> \n",
    "> $$\\begin{align*}\n",
    "U_{\\cdot j}^TU_{\\cdot j} & {} = \\,\\;1\\;\\,  = V_{\\cdot j}^TV_{\\cdot j} & {} \\textbf{normal vectors}\\\\\n",
    "U_{\\cdot j}^TU_{\\cdot k} & {} = \\,\\;0\\;\\,  = V_{\\cdot j}^TV_{\\cdot k}, j\\not=k & {} \\textbf{orthogonality}\\\\\n",
    "U^TU & {} = I_{r\\times r}  = V^TV& {} \\textbf{semi-orthonormality} &\\;\\quad \\text{ so } \\quad UU^T \\neq I_{r \\times r} \\left\\{\\begin{array}{ll} = VV^T & \\text{if }r=m\\\\ \\neq VV^T & \\text{if }r\\neq m\\\\  \\end{array}\\right.\n",
    "\\end{align*}$$\n",
    "> \n",
    "> $D_{r\\times r}$ is a ***nonnegative diagonal matrix*** with $D_{ij}=0$ for $i \\not = j$ and with ***singular values*** $\\lambda_i$ ordered on the diagonal \n",
    "> \n",
    "> $$D_{11}=\\lambda_1 \\geq D_{22}=\\lambda_2 \\geq \\cdots \\geq D_{rr}=\\lambda_r > 0$$  where $$r = \\text{rank}(A_{n\\times m}) \\leq \\min(n,m)   \\text{ and }  \\lambda_j = 0 \\text{ for } j>r$$\n",
    "> \n",
    "> which is what gives rise to the ***compact*** version of the ***SVD***.\n",
    "\n",
    "---\n",
    "\n",
    "For $f_A(x)=Ax=b$ the ***condition*** of $f_A$ depends on the relative magnitudes of the ***singular values*** of the $A$ matrix \n",
    "\n",
    "From the SVD we see that \n",
    "$$b = Ax = (UDV^T)x = \\sum_{k=1}^n \\lambda_k \\left[V^Tx\\right]_{k} U_{\\cdot k}$$\n",
    "is a weighted sum of the columns $U_{\\cdot k}$, partially driven by the inner (dot) product of the $k^{th}$ column of $V$ with $x$\n",
    "\n",
    "$$\\left[V^Tx\\right]_k = |V_{\\cdot k}|\\cdot|x|\\cdot\\cos(\\theta_{V_{\\cdot k},x}) = \\cos(\\theta_{V_{\\cdot k},x}) \\quad \\textrm{ as orthonormal $V_{\\cdot k}=1$ $|x|=1$ may be assumed w.l.o.g.}$$\n",
    "\n",
    "which ranges from $0$ to $1$ (for **orthognal** $V_{\\cdot k}$ and $x$ to perfectly aligned vectors with $\\theta_{V_{\\cdot k},x}=0$).\n",
    "\n",
    "If all **singular values** $\\lambda_k$ have the similar magnitudes then small changes in the input $x+\\epsilon$ changing the angles $\\theta_{V_{\\cdot k},x}=0$ will result in relatively uniform changes in $b=Ax$. But if a single $\\lambda_k$ is very large while another $\\lambda_{k'\\neq k}$ is very small the changes in the contribution of $U_{\\cdot k}$ will be extremely dominant while changes in the contribution of $U_{\\cdot k'}$ will be relatively insignificant. This will altogether be determined by the relative spacing of ***singular values***, but the most extreme relative changes will be between the smallest and largest ***singular values***. Thus, the general ***matrix condition number*** is $\\kappa(A) = \\frac{\\lambda_{max}}{\\lambda_{min}}$ the ratio of the largest and smallest ***singular values***. And if this  ***matrix condition number*** is small then the matrix is said to be ***well-conditioned*** otherwise it's said to be ***ill-conditioned***.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab28f9d",
   "metadata": {},
   "source": [
    "## Condition \"review\"\n",
    "\n",
    "---\n",
    "\n",
    "The **condition of a function** is a mathematical property characterizing how dramatically the answer changes for small differences in the inputs.\n",
    "\n",
    "$$\\underset{\\large \\text{$f$ is well-conditioned}}{x+\\epsilon_x \\approx x \\Longrightarrow f(x+\\epsilon_x) \\approx f(x)} \\quad \\text{ or } \\quad \\underset{\\large \\text{$f$ is ill-conditioned}}{x+\\epsilon_x \\approx x \\cancel{\\Longrightarrow} f(x+\\epsilon_x) \\approx f(x)}$$\n",
    "\n",
    "Since the reason for small changes $\\epsilon_x$ in the input $x$ of a problem in a numerical context is **roundoff error**, it is very common to simply define a **condition number** which simply characterizes when a computation becomes \"numerically volatile\" solely as a result of problems with \"numerical accuracy\". The **stiffness** \"condition number\" statistic above is an example of this kind of **condition number**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96239080",
   "metadata": {},
   "source": [
    "## Condition and Numerical Accuracy\n",
    "\n",
    "---\n",
    "\n",
    "Since $ \\hat \\beta = (X^T\\!X)^{-1}X^T\\!y$ the **condition** of the problem $X^T\\!X \\beta = X^T\\!y$ is $\\kappa(X^TX) = \\frac{\\lambda^{\\max}_{X^TX}}{\\lambda^{\\min}_{X^TX}}$ regardless of how the problem is solved. So if $\\kappa(X^TX)$ is very large then the problem itself is **ill-conditioned**. \n",
    "- Thus, in the context of an **ill-conditioned** problem, even the most minor **roundoff error** can lead to wildly inaccurate solutions, and so even the most accurate computations can still fail miserably for **ill-conditioned** problems. \n",
    "\n",
    "Nonetheless, the different **direct method** algorithms for finding the **least squares** solution to a **linear regression** problem do indeed actually perform different computations and factorizations, and so when there is low **multicollinearity** in $X$ (and the **singular values** are similarly scaled and $X$ is **well-conditioned**), the different methods do have different amounts of **roundoff error**. But none of the methods really incur prohibitive **roundoff error** when $X$ is **well-conditioned**.\n",
    "\n",
    "|  | Grammian  | Algorithmic Computational Requirements | **Roundoff Error** Considerations for Small $\\kappa(X)$ | and for Very Large $\\kappa(X)$ |\n",
    "|----------------------|-----------------------------|---------------------|----|---|\n",
    "| `np.linalg.inv`      | ✘ $\\kappa(X^TX)$ | $O(nm^2) \\underset{\\text{[vectorizable]}}{ +\\; O(m^3)}$ | $X^TX$ computation will be sufficiently accurate but<br>$(X^TX)^{-1}$ introduces unnecessary **roundoff error** | **Ill-Conditioned Problem**| \n",
    "| `np.linalg.solve` | ✘ $\\kappa(X^TX)$ | $O(nm^2) + O(nm) + O(m^3) + O(m^2)$ | no unneeded $(X^TX)^{-1}$ operations **roundoff error** | ***Ill-Conditioned Problem***|\n",
    "| `np.linalg.qr`       |  $\\checkmark \\;\\kappa(X)$  | $\\underset{>>1}{a}\\times O(nm^2)$ | ***QR*** is often a little bit more accurate than $X^TX$ | ***Ill-Conditioned Problem***|\n",
    "| `np.linalg.cholesky` | ✘ $\\kappa(X^TX)$ | $O(nm^2) + \\times O(m^3)/2$ (Cholesky requires half of<br>the operations of Gaussian elimination) | As good or better than using `np.linalg.solve` | ***Ill-Conditioned Problem***|\n",
    "| `np.linalg.svd`      |  $\\checkmark \\;\\kappa(X)$              | $\\underset{>>a}{b}\\times O(nm^2)$ | ***SVD*** is typically thought to be the most accurate | ***Ill-Conditioned Problem***|\n",
    "\n",
    "<!--\n",
    "1. complete computation of $\\hat \\beta = (X^T\\!X)^{-1} X^Ty$\n",
    "2. ***Gaussian elimination*** of the ***gramian*** $X^TX$ and $X^T\\!y$ followed by ***backward substitution*** to solve $\\hat \\beta$\n",
    "3. ***QR decomposition*** of $X$ followed by ***backward substitution*** with $R$ and $Q^Ty$ to solve for $\\hat \\beta$\n",
    "4. ***Cholesky decomposition*** of the (***symmetric positive definite***) ***gramian*** $X^TX=CC^T$ followed by two rounds of ***backward substitution*** based on $C$ and $C^T$ to subsequently solve for $\\hat \\beta$\n",
    "5. ***SVD*** of $X$ and computation of $\\hat \\beta = V (D^{+})^2 V^T X^T y$\n",
    "-->\n",
    "\n",
    "\n",
    "\n",
    "- `np.linalg.inv(X.T@X) @ X^Ty`\n",
    "\n",
    "  > If $X$ is **full rank** then $(X^TX)^{-1}$ exists; however, if columns of $X$ are highly **multicollinear** the **condition number** $\\kappa(X^TX)$ will be large and the problem will be **ill-conditioned** so that **roundoff error** in $X^Ty$ and $(X^TX)^{-1}$ may lead to a solution $\\hat \\beta = (X^TX)^{-1}X^Ty$ which is very inaccurate.\n",
    "  > - For a **well-conditioned** problem with small $\\kappa(X^TX)$, this solution is *still not recommended* because it requires unnecessary invesion computations which can add **roundoff error**.\n",
    "\n",
    "\n",
    "- `np.linalg.solve(X.T@X, X^Ty)` \n",
    "\n",
    "  > Rather than unnecessarily computing $(X^TX)^{-1}$ for **full rank** $X$ which is both computationally wasteful and can add additional **roundoff error**, **Gaussian Elimination** followed by **backward substitution** can be used to solve for $\\hat \\beta$ in $X^TX \\hat \\beta = X^T Y$. \n",
    "  > - For a **well-conditioned** problem with small $\\kappa(X^TX)$ this solution will be perfectly numerically satisfactory and result in negligable **roundoff error**.\n",
    "\n",
    "- `Q,R = np.linalg.qr(X)`\n",
    "\n",
    "  > If $X$ is **full rank** then **QR decomposition** $X=QR$ transforms $X^TX \\hat \\beta = X^T Y$ to $R \\hat \\beta = Q^T Y$ where $R$ is already a **triangular matrix** so $\\hat \\beta$ can be immediately solved for **backward substitution**.\n",
    "  > \n",
    "  > If $\\kappa(X)$ is small then $Q$ and $R$ produced by the **QR decomposition** will have minimal **roundoff error**, and subsequently the **condition numbers** $\\kappa(X) = \\kappa(R) < \\kappa(R^TR) = \\kappa(X^TX)$ for solving for $\\hat \\beta$ in $R \\hat \\beta = Q^T Y$\n",
    "  and $\\kappa(Q)=\\kappa(Q^T)=1$ for computing $Q^T y$, all of which leads to a typically reduced **roundoff error** compared to computing $(X^TX)$ and solving for $\\hat \\beta$ in $(X^TX)\\hat \\beta = X^Ty$.\n",
    "  > \n",
    "  > Regardless, since $\\hat \\beta = R^{-1}Q^T y = (X^TX)^{-1}X^Ty$ is the answer to $f_{R^{-1}Q^T}(y) = f_{(X^TX)^{-1}X^T}(y)$, even when $y+\\epsilon \\approx y$ so that\n",
    "  >   - $Q^T (y+\\epsilon)$ is expected to have less **roundoff error** than $X^T(y+\\epsilon)$\n",
    "  >   - solving for $\\hat \\beta$ in $R\\hat \\beta = Q^T (y+\\epsilon)$ is also expected to have less **roundoff error** than solving for $\\hat \\beta$ in $X^T\\!X\\hat \\beta = X^T (y+\\epsilon)$\n",
    "  > \n",
    "  > it's still not the case that $\\hat \\beta_y \\overset{?}{\\approx} \\hat \\beta_{y+\\epsilon} = f_{R^{-1}Q^T}(y+\\epsilon) = f_{(X^TX)^{-1}X^T}(y+\\epsilon)$ when the problem is ***ill-conditioned*** to begin with. The reduced ***roundoff error*** from the use of the ***QR decomposition*** will not overcome problem ***ill-conditioning***\n",
    " \n",
    "- `C = np.linalg.cholesky(X.T@X)`\n",
    "\n",
    "  > For **full rank** $X$, the **Cholesky decomposition** can be substituted in place of **Gaussian elimination** and then $\\hat \\beta$ in \n",
    "  >\n",
    "  > $$X^T\\!X\\hat \\beta = CC^T \\hat \\beta =  \\underset{L}{C} (\\underbrace{C^T \\hat \\beta}_{\\gamma}) = X^T\\!y$$ \n",
    "  > can be found by solving two **backward substitution** problems. \n",
    "  >\n",
    "  > Following computation of $X^TX$, the choice of **Gaussian elimination** or **Cholesky decomposition** does not greatly impact numerical accuracy. Still, the **Cholesky decomposition** require about half as many required arithmetic steps the and the **backward substitution** solving for $\\gamma$ in $C \\gamma = X^T\\!y$ and then $\\hat \\beta$ in $C \\hat \\beta = \\gamma$ does have a reduced **condition number** $\\kappa(C) < \\kappa(X^TX)$, so the **Cholesky decomposition** approach is an advisable choice for **well-conditioned** problems with small $\\kappa(X^TX)$. \n",
    "  > \n",
    "  > For the same reasons as discussed in the context of a **QR decomposition**, the problem **condition** of $\\hat \\beta = (CC^T)^{-1}X^Ty = (X^TX)^{-1}X^Ty$ is not a function of whether or not the **Cholesky decomposition** is used.\n",
    "  >   - No method has the ability to overcome solution volatility caused by **roundoff error** in the context of **ill-conditioned** problem; but, interestingly, the problem **ill-conditioned** can be numerically diagnosed from the ***Cholesky decomposition*** itself. Even if $\\text{rank}(X)=p$ when $\\kappa(X^T\\!X)$ is large the problem, the **ill-conditioning** will be immediately obvious because the **symmetric** $X^T\\!X = \\frac{X^T\\!X +(X^T\\!X)^T}{2}$ will not be **positive definite** and substantial reconstruction **roundoff error** will be observable in $CC^T$.\n",
    "  \n",
    "\n",
    "- `np.linalg.lstsq(X, y)`\n",
    "\n",
    "  > For the **SVD** of any matrix $X = UDV^T$ the **Moore-Penrose (generalized) inverse** $(X^TX)^+ = V (D^{+})^2 V^T \\underset{\\text{full rank}}{\\overset{\\text{if $X$ is}}{=}} (X^TX)^{-1}$ specifies $\\hat \\beta=VD^{-1}U^T y$ so that only the **SVD** of $X$ and some final matrix multiplications are needed for this approach.\n",
    "  > - Since $\\kappa(D_{ii}) = \\sqrt{\\kappa(X^TX)}$ and $\\kappa(V) = \\kappa(U^T) = 1$ the **roundoff error** is generally less than computing $X^TX$, while $U$ and $V$ similarly tends to have less **roundoff error** than $Q$ and $R$ of the **QR decomposition**. \n",
    "  > - Since all of this leads to the **SVD** approach generally being the most accurate of all the approaches, the increased computational costs of the **SVD** do not represent unnecessary computations, but additional computations which have the affect of minimizing **roundoff error** during the decomposition. \n",
    "  \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e47b5f8",
   "metadata": {},
   "source": [
    "# Lecture\n",
    "\n",
    "First hour of class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb49764",
   "metadata": {},
   "source": [
    "## Singular Value Decomposition\n",
    "\n",
    "--- \n",
    "\n",
    "We can see from the **SVD** $X = UDV^T$ that magnitude differences in the ***singular values*** lead to\n",
    "- **numerical roundoff error** (due to summing \"big values with small values\") and \n",
    "- **ill-conditioned** matrix multiplication \n",
    "\n",
    "both of which are defined by the **matrix condition number** $\\frac{\\lambda_{\\max}}{\\lambda_{\\min}}$ of the ratio of the largest over smallest **singular values**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aba35ff",
   "metadata": {},
   "source": [
    "### Multicollinearity\n",
    "---\n",
    "\n",
    "**Singular values** also determine \n",
    "\n",
    "- **multicollinearity** in the **design matrix** $X_{n\\times p} = U_{n\\times r} D_{r \\times r} (V^T)_{r\\times p}$ of **linear model regression** \n",
    "\n",
    "> characterizing the degree to which the covariates are linear combinations of each other\n",
    "\n",
    "\n",
    "- $r<p$ means some columns of $X$ are perfectly **collinear** making $X$ statistically intractable \n",
    "\n",
    "- $r=p$ means $X$ is technically **full rank** (so $X^TX$ is invertible)\n",
    "\n",
    "But if a **singular values** $\\lambda_\\min = D_{pp}$ is very small, the numerical contribution of **loadings** $U_{\\cdot j}$ and **basis vectors** $V_{\\cdot j}$ constructing $X$ degrade due **roundoff error** \n",
    "\n",
    "And if they are totally lost from **roundoff error**, then $[X_{n\\times p} = U_{n\\times p} D_{p \\times p} (V^T)_{p\\times p}]_c \\approx U_{n\\times r} D_{\\underset{r<p}{r \\times r}} (V^T)_{r\\times p}$ is **numerically non-full rank**.\n",
    "\n",
    "The relative magnitudes of the **singular values** of $X$ characterize the degree of ***multicollinearity*** present in $X$. Highly non-homogenous **singular values** mean **high multicollinearity** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e55f93",
   "metadata": {},
   "source": [
    "### Variance Inflation Factors (VIFs)\n",
    "\n",
    "---\n",
    "\n",
    "**Variance Inflation Factors (VIFs)** $\\frac {1}{1-R_{j}^{2}}$ quantify **multicollinearity** <font style=\"color:gray\">(which might also be called \"observed covariate confounding\")</font>\n",
    "\n",
    "> $R_{j}^{2}$ is the **coefficient of determination** for the regression of $X_{j}$ on all others except $X_{j}$\n",
    "> \n",
    "> $${\\displaystyle X_{j}= \\alpha_{0}+\\alpha_{1}X_{1} + \\alpha_{2}X_{2}+\\cdots + \\underset{\\text{no } X_j}{\\alpha_{j-1}X_{j-1} + \\alpha_{j+1}X_{j+1}}+\\cdots\\alpha_{k}X_{p}}+\\epsilon$$\n",
    "> so the more predictive $X_{-j}$ is of $X_{j}$ the larger $R^2_j$ the more the coefficient uncertainty *inflates*\n",
    "\n",
    "\n",
    "**VIFs** are as advertised and capture the estimation uncertainty inflation relative to a **linearly independent** predictor that is attributable to **multicollinearity** \n",
    "\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\widehat{\\textrm{Var}}[{\\hat {\\beta}}_{j}] & ={} s^{2}[(X^{T}X)^{-1}]_{jj} = {\\frac {s^{2}}{(n-1)\\widehat{\\textrm{Var}}(X_{j})}}\\cdot \\underset{VIF}{\\frac {1}{1-R_{j}^{2}}} \\quad \\begin{array}{ll}\\text{$s^2$ is the }\\textbf{residual variance}\\\\\\text{ of the original regression}\\end{array}\\\\\n",
    "&={}s^{2}[(VD^2V^T)^{-1}]_{jj}\\\\\n",
    "&={}s^{2}[(VD^{-2}V^T)]_{jj} = s^2 \\sum_{i=1}^{p}V_{ji}^2D_{ii}^{-2}\n",
    "\\end{align}$\n",
    "\n",
    "which shows that it's small **singular values** $\\lambda_i = D_{jj}$ of **design matrix** $X$ can lead to *inflated* ${\\widehat {\\operatorname {var} }}({\\hat {\\beta }}_{j})$ (unless inversely masked by corresponding $V_{j \\cdot }$)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cc8275",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor as VIF \n",
    "# https://en.wikipedia.org/wiki/Variance_inflation_factor#Calculation_and_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b35b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100 \n",
    "#low multicollinearity so there's minimal variance inflation\n",
    "X = stats.multivariate_normal(cov=np.array(((1,.09),(.09,1)))).rvs(n)\n",
    "X = (X-X.mean(axis=0))/X.std(axis=0)\n",
    "print(\"design matrix X columns ARE standardized:\", \n",
    "      \"\\ncolumn means\", X.mean(axis=0),\n",
    "      \"\\ncolumn standard deviations\",  X.std(axis=0),\n",
    "      \"\\ncondition number\", np.linalg.cond(X))\n",
    "X = sm.add_constant(X)\n",
    "Y = X@np.ones(3) + stats.norm().rvs(n)\n",
    "model = sm.OLS(Y,X)\n",
    "results = model.fit()\n",
    "\n",
    "print(\"\\nVariance Inflation Factors\", VIF(X,0),VIF(X,1),VIF(X,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2167ab3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.summary2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d659d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.summary2().tables[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6cb3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.summary2().tables[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60de317d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(results.summary2().tables[2]).values[3,2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46028011",
   "metadata": {},
   "source": [
    "#### Reminder and Confirmation\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\widehat{\\textrm{Var}}[{\\hat {\\beta}}_{j}] & ={} s^{2}[(X^{T}X)^{-1}]_{jj} = {\\frac {s^{2}}{(n-1)\\widehat{\\textrm{Var}}(X_{j})}}\\cdot \\underset{VIF}{\\frac {1}{1-R_{j}^{2}}} \\quad \\begin{array}{ll}\\text{$s^2$ is the }\\textbf{residual variance}\\\\\\text{ of the original regression}\\end{array}\\\\\n",
    "&={}s^{2}[(VD^2V^T)^{-1}]_{jj}\\\\\n",
    "&={}s^{2}[(VD^{-2}V^T)]_{jj} = s^2 \\sum_{i=1}^{p}V_{ji}^2D_{ii}^{-2}\n",
    "\\end{align}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e5265e",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = (float(results.summary2().tables[0].iloc[-1,-1]))**.5 # residual error\n",
    "\n",
    "XtXinv = np.linalg.inv(X.T.dot(X))\n",
    "print('s*(X^TX)^{-1/2}                 ', s*np.diag(XtXinv)**0.5)\n",
    "\n",
    "VIF_formula = np.array([VIF(X,0),VIF(X,1),VIF(X,2)]) \n",
    "# This formula doesn't apply to Var(X_intercept) = 0\n",
    "VIF_formula[1:] = s*(VIF_formula[1:]/((n-1)*X.var(ddof=1,axis=0)[1:]))**0.5\n",
    "VIF_formula[0] = s*XtXinv[0,0]**0.5\n",
    "print('s*(VIF/(n-1)*Var[X])^{-1/2}     ', VIF_formula)\n",
    "\n",
    "U,D,Vt = np.linalg.svd(X, full_matrices=False)\n",
    "print(\"\\nSingular Values     \", list(D))\n",
    "print(\"Singular Values^{-2}\", list(D**(-2)))\n",
    "\n",
    "XtXinv_by_svd = (Vt.T*D**(-2)).dot(Vt) # equal to XtXinv (sans numeric error of XtXinv)\n",
    "XtXinv_by_svd_diag = ((Vt.T**2)*(D**-2)).sum(axis=1)  # same as *(D**(-2)).reshape(1,3)\n",
    "\n",
    "print('\\ns*(X^TX)^{-1/2} [but by SVD]    ', s*np.diag(XtXinv_by_svd)**0.5)\n",
    "print('s*(SUM_i V_ij^2 D_ii^{-2})^{1/2}', s*XtXinv_by_svd_diag**0.5) \n",
    "# confirming that these coefficient variances depend on the singular values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f6152f",
   "metadata": {},
   "outputs": [],
   "source": [
    "XtXinv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ee7b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "XtXinv_by_svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7a2d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(2,2, figsize=(12,6))\n",
    "ax[0,0].bar(x=range(3), height=D)\n",
    "ax[0,0].set_title(\"Singular Values\"); \n",
    "\n",
    "# This shows that the approximate reconstruction of X is very poor \n",
    "ax[1,0].hist(np.abs(U.dot(np.diag(D)).dot(Vt) - X).flatten())\n",
    "ax[1,1].hist(np.abs(U[:,:2].dot(np.diag(D[:2])).dot(Vt[:2,:])-X).flatten())\n",
    "ax[1,0].set_title(\"Roundoff error in full reconstruction of X\")\n",
    "ax[1,1].set_title(\"Approximation error in compact reconstruction of X\")\n",
    "ax[0,1].plot((U[:,:2].dot(np.diag(D[:2])).dot(Vt[:2,:])).ravel(), X.ravel(), '.', \n",
    "             label='Compact Reconstruction:\\nX~ = (U[:,:2]*D[:2]).dot(Vt[:2,:])')\n",
    "ax[0,1].plot((U.dot(np.diag(D)).dot(Vt)).ravel(), X.ravel(), '.', \n",
    "             label='Full reconstruction:\\nX~ = (U*D).dot(Vt)')\n",
    "ax[0,1].set_title(\"X is poorly reconstructed with a lower rank approximation:\\nX is characteristically FULL RANK\")\n",
    "ax[0,1].set_ylabel(\"Original X\")\n",
    "ax[0,1].set_xlabel(\"Reconstructed X\")\n",
    "ax[0,1].legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1222de42",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100 \n",
    "#low multicollinearity so there's minimal variance inflation\n",
    "np.random.seed(410)\n",
    "X = stats.multivariate_normal(cov=np.array(((1,.09),(.09,1)))).rvs(n)\n",
    "X = (X-X.mean(axis=0))/X.std(axis=0)\n",
    "print(\"design matrix X columns ARE standardized:\", \n",
    "      \"\\ncolumn means\", X.mean(axis=0),\n",
    "      \"\\ncolumn standard deviations\",  X.std(axis=0),\n",
    "      \"\\ncondition number\", np.linalg.cond(X))\n",
    "X = sm.add_constant(X)\n",
    "np.random.seed(411)\n",
    "Y = X@np.ones(3) + stats.norm().rvs(n)\n",
    "model = sm.OLS(Y,X)\n",
    "results = model.fit()\n",
    "results.summary2().tables[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5812aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nVariance Inflation Factors\", VIF(X,0),VIF(X,1),VIF(X,2))\n",
    "\n",
    "U,D,Vt = np.linalg.svd(X, full_matrices=False)\n",
    "print(\"\\nCondition (two ways)\", D[0]/D[-1], np.linalg.cond(X))\n",
    "\n",
    "print(\"\\nSingular Values      \", list(D))\n",
    "\n",
    "print(\"Singular Values**(-2)\", list(D**(-2)))\n",
    "\n",
    "print(\"\\n[V]_ij\")\n",
    "print(Vt.T)\n",
    "print(\"\\n[V]_ij**2\")\n",
    "print(Vt.T**2)\n",
    "\n",
    "print(\"\\nCoef. Std.Err. s (sum_j [V]_ij**2 * Singular Values**(-2)_j)**(-0.5)\")\n",
    "s = (float(results.summary2().tables[0].iloc[-1,-1]))**.5 # residual error\n",
    "print(s*(Vt.T**2*D**(-2)).sum(axis=1)**0.5)  # same as *(D**(-2)).reshape(1,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3132bba2",
   "metadata": {},
   "source": [
    "#### Reminder and Confirmation\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\widehat{\\textrm{Var}}[{\\hat {\\beta}}_{j}] & ={} s^{2}[(X^{T}X)^{-1}]_{jj} = {\\frac {s^{2}}{(n-1)\\widehat{\\textrm{Var}}(X_{j})}}\\cdot \\underset{VIF}{\\frac {1}{1-R_{j}^{2}}} \\quad \\begin{array}{ll}\\text{$s^2$ is the }\\textbf{residual variance}\\\\\\text{ of the original regression}\\end{array}\\\\\n",
    "&={}s^{2}[(VD^2V^T)^{-1}]_{jj}\\\\\n",
    "&={}s^{2}[(VD^{-2}V^T)]_{jj} = s^2 \\sum_{i=1}^{p}V_{ji}^2D_{ii}^{-2}\n",
    "\\end{align}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad9a5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the coefficient of varaition sigma/mu in the covariate columns is now \n",
    "# very small making these columns highly predictive of the intercept column\n",
    "# sm.OLS(X[:,:1],X[:,1:]).fit().summary()  # is now highly predictive with X[:,1:].sum(axis=1)/20000\n",
    "# sm.OLS(X[:,2:],X[:,:2]).fit().summary()  # is still not very predictive \n",
    "\n",
    "X[:,1:] += 10000\n",
    "print(X[:5,:])\n",
    "\n",
    "np.random.seed(411)\n",
    "Y = X@np.ones(3) + stats.norm().rvs(n)\n",
    "\n",
    "# centering (and scaling) the X's does the trick here\n",
    "# X[:,1:] = (X[:,1:]-X[:,1:].mean(axis=0))/X[:,1:].std(axis=0)\n",
    "\n",
    "model = sm.OLS(Y,X)\n",
    "results = model.fit()\n",
    "results.summary2().tables[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ff5ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nVariance Inflation Factors\", VIF(X,0),VIF(X,1),VIF(X,2))\n",
    "\n",
    "U,D,Vt = np.linalg.svd(X, full_matrices=False)\n",
    "print(\"\\nCondition (two ways)\", D[0]/D[-1], np.linalg.cond(X))\n",
    "\n",
    "print(\"\\nSingular Values      \", list(D))\n",
    "\n",
    "print(\"Singular Values**(-2)\", list(D**(-2)))\n",
    "\n",
    "print(\"\\n[V]_ij\")\n",
    "print(Vt.T)\n",
    "print(\"\\n[V]_ij**2\")\n",
    "print(Vt.T**2)\n",
    "\n",
    "print(\"\\nCoef. Std.Err. s * (sum_j [V]_ij**2 * Singular Values**(-2)_j)**(-0.5)\")\n",
    "s = (float(results.summary2().tables[0].iloc[-1,-1]))**.5 # residual error\n",
    "print(s*(Vt.T**2*D**(-2)).sum(axis=1)**0.5)  # same as *(D**(-2)).reshape(1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbc81d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(2,2, figsize=(12,6))\n",
    "ax[0,0].bar(x=range(3), height=D)\n",
    "#ax[0,0].set_yscale('log')\n",
    "\n",
    "ax[0,0].set_title(\"Singular Values\")\n",
    "\n",
    "# This shows that the approximate reconstruction of X is very poor \n",
    "ax[1,0].hist(np.abs(U.dot(np.diag(D)).dot(Vt) - X).flatten())\n",
    "ax[1,1].hist(np.abs(U[:,:2].dot(np.diag(D[:2])).dot(Vt[:2,:])-X).flatten())\n",
    "\n",
    "ax[1,0].set_title(\"Roundoff error in full reconstruction of X\")\n",
    "ax[1,1].set_title(\"Approximation error in compact reconstruction of X\")\n",
    "\n",
    "offset = np.zeros(X.shape)\n",
    "offset[:,1:] += 10000\n",
    "ax[0,1].plot((U[:,:2].dot(np.diag(D[:2])).dot(Vt[:2,:])-offset).ravel(), (X-offset).ravel(), '.', \n",
    "             label='Compact Reconstruction:\\nX~ = (U[:,:2]*D[:2]).dot(Vt[:2,:])')\n",
    "ax[0,1].plot((U.dot(np.diag(D)).dot(Vt)-offset).ravel(), (X-offset).ravel(), '.', \n",
    "             label='Full reconstruction:\\nX~ = (U*D).dot(Vt)')\n",
    "ax[0,1].set_title(\"X is accurately reconstructed with a lower rank approximation:\\nX is (numerically) characteristically SINGULAR\")\n",
    "ax[0,1].set_ylabel(\"Original X\")\n",
    "ax[0,1].set_xlabel(\"Reconstructed X\")\n",
    "ax[0,1].legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa17e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919d9814",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "U*D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60eccf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "Vt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c836f12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(U*D)@Vt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13934dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The problem here is not introducing artificial multicollinearity\n",
    "# it's working with values on with different orders of magnitudes\n",
    "\n",
    "n = 100 \n",
    "#low multicollinearity so there's minimal variance inflation\n",
    "np.random.seed(410)\n",
    "X = stats.multivariate_normal(cov=np.array(((1,.09),(.09,1)))).rvs(n)\n",
    "X = (X-X.mean(axis=0))/X.std(axis=0)\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "X[:,1:2] /= 10000\n",
    "X[:,2:] *= 10000\n",
    "\n",
    "print(X[:5,:])\n",
    "\n",
    "np.random.seed(411)\n",
    "Y = X@np.ones(3) + stats.norm().rvs(n)\n",
    "\n",
    "# scaling (and centering) the X's does the trick here\n",
    "# X[:,1:] = (X[:,1:]-X[:,1:].mean(axis=0))/X[:,1:].std(axis=0)\n",
    "\n",
    "model = sm.OLS(Y,X)\n",
    "results = model.fit()\n",
    "results.summary2().tables[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea457f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nVariance Inflation Factors\", VIF(X,0),VIF(X,1),VIF(X,2))\n",
    "\n",
    "U,D,Vt = np.linalg.svd(X, full_matrices=False)\n",
    "print(\"\\nCondition (two ways)\", D[0]/D[-1], np.linalg.cond(X))\n",
    "\n",
    "print(\"\\nSingular Values      \", list(D))\n",
    "\n",
    "print(\"Singular Values**(-2)\", list(D**(-2)))\n",
    "\n",
    "print(\"\\n[V]_ij\")\n",
    "print(Vt.T)\n",
    "print(\"\\n[V]_ij**2\")\n",
    "print(Vt.T**2)\n",
    "\n",
    "print(\"\\nCoef. Std.Err. s * (sum_j [V]_ij**2 * Singular Values**(-2)_j)**(-0.5)\")\n",
    "s = (float(results.summary2().tables[0].iloc[-1,-1]))**.5 # residual error\n",
    "print(s*(Vt.T**2*D**(-2)).sum(axis=1)**0.5)  # same as *(D**(-2)).reshape(1,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8227fc54",
   "metadata": {},
   "source": [
    "#### Reminder and Confirmation\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\widehat{\\textrm{Var}}[{\\hat {\\beta}}_{j}] & ={} s^{2}[(X^{T}X)^{-1}]_{jj} = {\\frac {s^{2}}{(n-1)\\widehat{\\textrm{Var}}(X_{j})}}\\cdot \\underset{VIF}{\\frac {1}{1-R_{j}^{2}}} \\quad \\begin{array}{ll}\\text{$s^2$ is the }\\textbf{residual variance}\\\\\\text{ of the original regression}\\end{array}\\\\\n",
    "&={}s^{2}[(VD^2V^T)^{-1}]_{jj}\\\\\n",
    "&={}s^{2}[(VD^{-2}V^T)]_{jj} = s^2 \\sum_{i=1}^{p}V_{ji}^2D_{ii}^{-2}\n",
    "\\end{align}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493a6540",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(2,2, figsize=(12,6))\n",
    "ax[0,0].bar(x=range(3), height=D)\n",
    "ax[0,0].set_title(\"Singular Values\")\n",
    "# ax[0,0].set_yscale('log')\n",
    "\n",
    "# This shows that the approximate reconstruction of X is very poor \n",
    "ax[1,0].hist(np.abs(U.dot(np.diag(D)).dot(Vt) - X).flatten())\n",
    "ax[1,1].hist(np.abs(U[:,:2].dot(np.diag(D[:2])).dot(Vt[:2,:])-X).flatten())\n",
    "ax[1,0].set_title(\"Roundoff error in full reconstruction of X\")\n",
    "ax[1,1].set_title(\"Approximation error in compact reconstruction of X\")\n",
    "ax[0,1].plot((U[:,:2].dot(np.diag(D[:2])).dot(Vt[:2,:])).ravel(), X.ravel(), '.', \n",
    "             label='Compact Reconstruction:\\nX~ = (U[:,:2]*D[:2]).dot(Vt[:2,:])')\n",
    "ax[0,1].plot((U.dot(np.diag(D)).dot(Vt)).ravel(), X.ravel(), '.', \n",
    "             label='Full reconstruction:\\nX~ = (U*D).dot(Vt)')\n",
    "ax[0,1].set_title(\"X is poorly reconstructed with a lower rank approximation:\\nX is (numerically) characteristically FULL RANK\")\n",
    "ax[0,1].set_ylabel(\"Original X\")\n",
    "ax[0,1].set_xlabel(\"Reconstructed X\")\n",
    "ax[0,1].legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f43244a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The standard errors are inflated for highly multicolliner X\n",
    "# Now we can't escape the poor conditioning / singularity of X\n",
    "# just by centering and scaling the design matrix \n",
    "\n",
    "n = 100\n",
    "X = stats.multivariate_normal(cov=np.array(((1,.99),(.99,1)))).rvs(n)\n",
    "X=(X-X.mean(axis=0))/X.std(axis=0)\n",
    "print(\"design matrix X columns ARE standardized:\", \n",
    "      \"\\ncolumn means\", X.mean(axis=0),\n",
    "      \"\\ncolumn standard deviations\",  X.std(axis=0),\n",
    "      \"\\ncondition number\", np.linalg.cond(X))\n",
    "X = sm.add_constant(X)\n",
    "Y = X.dot(np.ones(3)) + stats.norm().rvs(n)\n",
    "model = sm.OLS(Y,X); results = model.fit()\n",
    "display(results.summary2().tables[1])\n",
    "results.summary2().tables[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f762f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nVariance Inflation Factors\", VIF(X,0),VIF(X,1),VIF(X,2))\n",
    "\n",
    "U,D,Vt = np.linalg.svd(X, full_matrices=False)\n",
    "print(\"\\nCondition (two ways)\", D[0]/D[-1], np.linalg.cond(X))\n",
    "\n",
    "print(\"\\nSingular Values      \", list(D))\n",
    "\n",
    "print(\"Singular Values**(-2)\", list(D**(-2)))\n",
    "\n",
    "print(\"\\n[V]_ij\")\n",
    "print(Vt.T)\n",
    "print(\"\\n[V]_ij**2\")\n",
    "print(Vt.T**2)\n",
    "\n",
    "print(\"\\nCoef. Std.Err. s * (sum_j [V]_ij**2 * Singular Values**(-2)_j)**(-0.5)\")\n",
    "s = (float(results.summary2().tables[0].iloc[-1,-1]))**.5 # residual error\n",
    "print(s*(Vt.T**2*D**(-2)).sum(axis=1)**0.5)  # same as *(D**(-2)).reshape(1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd9a527",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(2,2, figsize=(12,6))\n",
    "ax[0,0].bar(x=range(3), height=D)\n",
    "ax[0,0].set_title(\"Singular Values\")\n",
    "# ax[0,0].set_yscale('log')\n",
    "\n",
    "# This shows that the approximate reconstruction of X is very poor \n",
    "ax[1,0].hist(np.abs(U.dot(np.diag(D)).dot(Vt) - X).flatten())\n",
    "ax[1,1].hist(np.abs(U[:,:2].dot(np.diag(D[:2])).dot(Vt[:2,:])-X).flatten())\n",
    "ax[1,0].set_title(\"Roundoff error in full reconstruction of X\")\n",
    "ax[1,1].set_title(\"Approximation error in compact reconstruction of X\")\n",
    "ax[0,1].plot((U[:,:2].dot(np.diag(D[:2])).dot(Vt[:2,:])).ravel(), X.ravel(), '.', \n",
    "             label='Compact Reconstruction:\\nX~ = (U[:,:2]*D[:2]).dot(Vt[:2,:])')\n",
    "ax[0,1].plot((U.dot(np.diag(D)).dot(Vt)).ravel(), X.ravel(), '.', \n",
    "             label='Full reconstruction:\\nX~ = (U*D).dot(Vt)')\n",
    "ax[0,1].set_title(\"X is NOT SO poorly constructed with a lower rank approximation:\\nX is NOT SO (numerically) characteristically full rank\");\n",
    "ax[0,1].set_ylabel(\"Original X\")\n",
    "ax[0,1].set_xlabel(\"Reconstructed X\")\n",
    "ax[0,1].legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51366fe1",
   "metadata": {},
   "source": [
    "\n",
    "### Principal Components Regression (PCR)\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Each **data point** $x_i = X_{i \\cdot}$ in ***full rank*** $X_{n \\times p}$ is represented in the **standard basis** $[e_1,e_2,\\cdots, e_p] x_i = I x_i = x_i$\n",
    "\n",
    "The ***SVD*** $X=UDV^T$ defines each **data point** $X_{i \\cdot} = U_{i \\cdot} DV^T$ according to a different **orthogonal basis** $x_i = {VD}u_i$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f38f4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "U,d,Vt = np.linalg.svd(X, full_matrices=False)\n",
    "\n",
    "i=1\n",
    "# U[i,:]*d is the \"broadcasted\" version of U[i,:]@np.diag(D) # https://numpy.org/doc/stable/user/basics.broadcasting.html\n",
    "print(X[i,:], \"\\n\", (U[i,:]*d)@Vt, \"\\n\", Vt.T@(d*U[i,:]).reshape(3,1).flatten(), sep=\"\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2597b554",
   "metadata": {},
   "source": [
    "Data point $x_i = X_{i \\cdot}$ is represented in the **standard basis** but the columns $X_{\\cdot j}$ are not necessarily **linearly independent** resulting in **multicolinearity**\n",
    "\n",
    "In a different **orthogonal basis** the very same   \n",
    "data point $x_i = X_{i \\cdot}$ is represented as $u_i = U_{i \\cdot}$, but $U$ (unlike $X$) *does have* **linearly independent**  \n",
    "(as the columns $U_{\\cdot j}$ since **semi-orthonormal** $U^TU = I_{m \\times m}$) and hence no **multicollinearity**\n",
    "\n",
    "> A point in $p$-dimensional space can be equivalently defined by different **bases**, so rows of $X$ and $U$ represent the same collection of points under different **bases**; but, $U$ has **linearly independent** columns so it has no **multicollinearity**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3271eb4",
   "metadata": {},
   "source": [
    "#### Latent Factors \n",
    "\n",
    "---\n",
    "\n",
    "Columns $U_{\\cdot j}$ have a \"latent factors\" interpretation as $U$ columns are linear combinations of the original columns of $X$\n",
    "\n",
    "$U_{\\cdot j} = \\left[X  V D^{-1}\\right]_{\\cdot j} = \\sum_{k=1}^m X_{\\cdot k} V_{k j}/\\lambda_j \\quad \\text{ and } \\quad X_{\\cdot j} = \\left[U D V^T \\right]_{\\cdot j} = \\sum_{k=1}^m \\lambda_k U_{\\cdot k} [V^T]_{k j} \\quad \\text{ are linear combinations of each other...}$\n",
    "\n",
    "therefore \n",
    "\n",
    "$\\hat y_i = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\beta_3 X_{i3} + \\cdots =  \\beta_0 + \\tilde\\beta_1 U_{i1} + \\tilde\\beta_2 U_{i2} + \\tilde\\beta_3 U_{i3} + \\cdots$\n",
    "\n",
    "but now coefficient estimation attribution uncertainty driven by variance inflation from **multicollinearity**\n",
    "has been removed by replacing an **ill-conditioned** $X$ with **perfectly conditioned** $U$ (where the **singular values** of $X$ have been factored out into the orthogonal basis representation $VD$)\n",
    "\n",
    "> As with all **orthogonal matrices**, **SVD** $U=U\\Lambda W^T = U_{n\\times m}I_{m \\times m}I_{m \\times m}$\n",
    "> so the **singular values** of $U$ are $\\lambda_i = \\Lambda_{ii} = I_{ii} = 1$ and $\\kappa(U) = 1\\;$ ($<< \\kappa(X)$ **ill-conditioned**). \n",
    ">\n",
    "> $U$ from the ***SVD*** of $X$ is also computable from $V$ and $D$ of the **eigendecomposition** of the **gramian** $X^TX$ \n",
    ">\n",
    "> $$X = UDV^T \\Longrightarrow \\underbrace{XVD^{-1} = UDV^TVD^{-1} = U}_{\\text{expressed for a single data point }D^{-1}V^Tx_i = u_i}$$\n",
    "\n",
    "\n",
    "<!-- - $x_i = VDu_i$ but the $u_i$ are exactly empirically uncorrelated, and hence have no **multicollinearity** -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf84e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The standard errors are inflated for highly multicolliner X\n",
    "# Now we can't escape the poor conditioning / singularity of X\n",
    "# just by centering and scaling the design matrix \n",
    "\n",
    "n = 100\n",
    "X = stats.multivariate_normal(cov=np.array(((1,.99),(.99,1)))).rvs(n)\n",
    "X=(X-X.mean(axis=0))/X.std(axis=0)\n",
    "print(\"design matrix X columns ARE standardized:\", \n",
    "      \"\\ncolumn means\", X.mean(axis=0),\n",
    "      \"\\ncolumn standard deviations\",  X.std(axis=0),\n",
    "      \"\\ncondition number\", np.linalg.cond(X))\n",
    "X = sm.add_constant(X)\n",
    "Y = X@np.ones(3) + stats.norm().rvs(n)\n",
    "model = sm.OLS(Y,X); results = model.fit()\n",
    "display(results.summary2().tables[1])\n",
    "results.summary2().tables[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3075a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nVariance Inflation Factors\", VIF(X,0),VIF(X,1),VIF(X,2))\n",
    "\n",
    "U,D,Vt = np.linalg.svd(X, full_matrices=False)\n",
    "print(\"\\nCondition (two ways)\", D[0]/D[-1], np.linalg.cond(X))\n",
    "\n",
    "print(\"\\nSingular Values      \", list(D))\n",
    "\n",
    "print(\"Singular Values**(-2)\", list(D**(-2)))\n",
    "\n",
    "print(\"\\n[V]_ij\")\n",
    "print(Vt.T)\n",
    "print(\"\\n[V]_ij**2\")\n",
    "print(Vt.T**2)\n",
    "\n",
    "print(\"\\nCoef. Std.Err. s * (sum_j [V]_ij**2 * Singular Values**(-2)_j)**(-0.5)\")\n",
    "s = (float(results.summary2().tables[0].iloc[-1,-1]))**.5 # residual error\n",
    "print(s*(Vt.T**2*D**(-2)).sum(axis=1)**0.5)  # same as *(D**(-2)).reshape(1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690df3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nVariance Inflation Factors\", VIF(X,0),VIF(X,1),VIF(X,2))\n",
    "\n",
    "U,D,Vt = np.linalg.svd(X[:,1:], full_matrices=False)\n",
    "print(\"\\nCondition (two ways)\", D[0]/D[-1], np.linalg.cond(X))\n",
    "\n",
    "print(\"\\nSingular Values      \", list(D))\n",
    "\n",
    "print(\"Singular Values**(-2)\", list(D**(-2)))\n",
    "\n",
    "print(\"\\n[V]_ij\")\n",
    "print(Vt.T)\n",
    "print(\"\\n[V]_ij**2\")\n",
    "print(Vt.T**2)\n",
    "\n",
    "print(\"\\nCoef. Std.Err. s * (sum_j [V]_ij**2 * Singular Values**(-2)_j)**(-0.5)\")\n",
    "s = (float(results.summary2().tables[0].iloc[-1,-1]))**.5 # residual error\n",
    "print(s*(Vt.T**2*D**(-2)).sum(axis=1)**0.5)  # same as *(D**(-2)).reshape(1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d075b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "U = sm.add_constant(U)\n",
    "model = sm.OLS(Y,U); results = model.fit()\n",
    "display(results.summary2().tables[1])\n",
    "results.summary2().tables[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb072735",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nVariance Inflation Factors\", VIF(U,0),VIF(U,1),VIF(U,2))\n",
    "\n",
    "U_,D,Vt = np.linalg.svd(U, full_matrices=False)\n",
    "print(\"\\nCondition (two ways)\", D[0]/D[-1], np.linalg.cond(U))\n",
    "\n",
    "print(\"\\nSingular Values      \", list(D))\n",
    "\n",
    "print(\"Singular Values**(-2)\", list(D**(-2)))\n",
    "\n",
    "print(\"\\n[V]_ij\")\n",
    "print(Vt.T)\n",
    "print(\"\\n[V]_ij**2\")\n",
    "print(Vt.T**2)\n",
    "\n",
    "print(\"\\nCoef. Std.Err. s * (sum_j [V]_ij**2 * Singular Values**(-2)_j)**(-0.5)\")\n",
    "s = (float(results.summary2().tables[0].iloc[-1,-1]))**.5 # residual error\n",
    "print(s*(Vt.T**2*D**(-2)).sum(axis=1)**0.5)  # same as *(D**(-2)).reshape(1,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4c3faf",
   "metadata": {},
   "source": [
    "### Principal Components Analysis (PCA) $X=UDV^T$ \n",
    "\n",
    "---\n",
    "\n",
    "[**Principal Components Analysis**](https://en.wikipedia.org/wiki/Principal_component_analysis) (**PCA**) is an **unsupervised learning** methodology uncovering linear structure in $X_{n \\times m}$. \n",
    "\n",
    "a **PCA** is computed as either\n",
    "\n",
    "$$\\underbrace{\\tilde X \\div \\sqrt{n} = U_{n \\times m}D_{m \\times m}(V^T)_{m \\times m}}_{\\text{singular value decomposition of the data matrix}}\\quad\\text{ or }\\quad\n",
    "\\underbrace{\\hat \\Sigma = (\\tilde X^T\\tilde X \\div n) = V_{m \\times m}D^2_{m \\times m}V^T_{m \\times m}}_{\\text{eigendecomposition of the data covariance matrix}}$$\n",
    "\n",
    "where the data columns of $X_{n \\times m}$ are centered and scaled by their column means $\\overline{X_{\\cdot j}}$ and standard deviations $\\hat \\sigma_{X_{\\cdot j}}$ as $\\tilde X_{\\cdot j}  = (X_{\\cdot j} - \\overline{X_{\\cdot j}}) / \\hat \\sigma_{X_{\\cdot j}}$  \n",
    "<font style=\"color:gray\">(unless the original scales of the data are part of the information that should be extracted)</font>\n",
    " \n",
    "This shows the [connection](https://math.stackexchange.com/questions/3869/what-is-the-intuitive-relationship-between-svd-and-pca) between **SVD** $X=UDV^T$ and the **eigendecomposition** of the  **gramian matrix** $X^TX = V D^2 V^T$ whose **eigenvalues** are the square of the **singular values** of $X$ and whose **(semi-)orthonormal** matrix $V^T$ is equal to that of the **SVD** and actually **orthonormal** for **full rank** $X$ (if the **gramian** is **positive definite** so all **eigenvalues** are **nonnegative**)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec29eb8",
   "metadata": {},
   "source": [
    "#### SVD Versus Eigendecomposition for PCA (or PCR) ([Return to TOC](#cell-solving))\n",
    "\n",
    "---\n",
    "\n",
    "As discussed in this [stackoverflow](https://stats.stackexchange.com/questions/314046/why-does-andrew-ng-prefer-to-use-svd-and-not-eig-of-covariance-matrix-to-do-pca) conversation there are multiple distinct algorithms \n",
    "\n",
    "- `np.linalg.svd(X)`/`np.linalg.svd(X.T@X)`/`np.linalg.eig(X.T@X)`/`np.linalg.eigh(X.T@X)`\n",
    "\n",
    "which could be used to compute $V^T$ and $D$ (and thus **PCA**), but `np.linalg.svd(X)` is the best choice for two reasons:\n",
    "\n",
    "1. All computations generally entail ***roundoff error*** so $X^TX$ \n",
    "    1. not only requires additional computation\n",
    "    2. it also unneccesarily introduces numerical inaccuracy \n",
    "\n",
    "> Even the **singular values**/**eigenvalues** $D_{ii}^2$ (of $X^TX$) tend to be less precice compared to the numerical accuracy of $D_{ii}$ (of $X$) since squaring introducess the possibility for **roundoff error**\n",
    "  \n",
    "2. The **condition** of $X$ for **spectral radius** ${\\rho(X) = \\lambda_\\max = \\max_i D_{ii}} > 1 > \\min_j D_{jj} = \\lambda_\\min$ is better than<br>the **condition** of the **gramian** $X^TX$ with **spectral radius** ${\\rho(X^TX) = \\lambda_\\max^2 = \\max_i D_{ii}^2} >> 1 >> \\min_j D_{jj}^2 = \\lambda_\\min^2$ since\n",
    "   \n",
    "   $$\\kappa(X^TX) = \\frac{\\lambda_{max}^2}{\\lambda_{min}^2} >> \\frac{\\lambda_{max}}{\\lambda_{min}} = \\kappa(X) \\quad \\text{ or } \\quad \\kappa(X) = \\frac{\\max_i D_{ii}}{\\min_j D_{jj}} << \\frac{\\max_i D_{ii}^2}{\\min_j D_{jj}^2} = \\kappa(X^TX)$$\n",
    "\n",
    "Thus, because of the generally increased **condition number** of the **covariance matrix** $ X^T  X$ relative to the scaled data matrix $X$, applying **SVD** to $ X$ is more numerically stable than computing the **eigendecomposition** or **SVD** of $ X^T  X$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b117fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=7)\n",
    "n,m = 100,7; X = stats.norm().rvs(size=(n,m)); X = (X - X.mean(axis=0))/X.std(axis=0)\n",
    "print(\"The total variance\", X.var(axis=0).sum(), end=\" \"); print(\"should match the sum of the eigenvalues\"); print(\"which is also equal to the sum of squares of singular values.\\n\")\n",
    "# Traditional PCA: eigendecomposition of the covariance matrix; np.linalg.eigh # \"complex Hermitian (conjugate symmetric) or a real symmetric matrix.\"\n",
    "eignvals = np.sort(np.linalg.eigh(np.cov(X.T, ddof=0))[0])[::-1]; print(eignvals, eignvals.sum())\n",
    "# Traditional PCA: eigendecomposition of the covariance matrix # np.linalg.eig: general eigendcomposition function\n",
    "eignvals = np.sort(np.linalg.eig(np.cov(X.T, ddof=0))[0])[::-1]; print(eignvals, eignvals.sum())\n",
    "# Traditional PCA: eigendecomposition of the covariance matrix # SVD/eigendecomposition equivalence for symmetric positive definite matrices\n",
    "eignvals = np.linalg.svd(np.cov(X.T, ddof=0))[1]; print(eignvals, eignvals.sum())\n",
    "# Traditional PCA: eigendecomposition of the covariance matrix # np.linalg.eigh with matrix-based covariance computation\n",
    "eignvals = np.sort(np.linalg.eigh(X.T.dot(X)/n)[0])[::-1]; print(eignvals, eignvals.sum())\n",
    "# Traditional PCA: eigendecomposition of the covariance matrix # np.linalg.eig with matrix-based covariance computation\n",
    "eignvals = np.sort(np.linalg.eig(X.T.dot(X)/n)[0])[::-1]; print(eignvals, eignvals.sum())\n",
    "# Traditional PCA: eigendecomposition of the covariance matrix # np.linalg.eig with matrix-based covariance computation\n",
    "eignvals = np.linalg.svd(X.T.dot(X)/n)[1]; print(eignvals, eignvals.sum())\n",
    "# PCA with SVD \n",
    "singvals = np.linalg.svd(X/n**0.5)[1]; print(singvals**2, (singvals**2).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1569f8c9",
   "metadata": {},
   "source": [
    "However, be that as it may..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5689b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "np.linalg.eigh(np.cov(X.T, ddof=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88195fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "np.linalg.eig(np.cov(X.T, ddof=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90196b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "np.linalg.svd(np.cov(X.T, ddof=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c36a94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "np.linalg.eigh(X.T.dot(X)/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d85275f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "np.linalg.eig(X.T.dot(X)/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7052e0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "np.linalg.svd(X.T.dot(X)/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc93b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "np.linalg.svd(X/n**0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca43259",
   "metadata": {},
   "source": [
    "### Interpreting PCA\n",
    "\n",
    "---\n",
    "\n",
    "We first used **SVD** above to replace a **linearly dependent** $X$ with **linearly independent (semi-orthogonal)** $U$ for the purposes. Here we consider the use of **SVD** to perform **PCA** which is an ***unsupervised learning*** methodology.\n",
    "\n",
    "\n",
    "**PCA** is the **SVD** of $X_{n \\times m}=U_{n \\times m}D_{m \\times m}(V^T)_{m \\times m}$ which provides \n",
    "\n",
    "- **Principle directions** of $X \\quad \\longrightarrow \\quad V_{\\cdot j}$ (columns of $V$) which are **semi-orthonormal** so $V_{\\cdot j}V_{\\cdot j}^T=1$\n",
    "\n",
    "- **Loadings (projections)** of $X_{i \\cdot} \\quad \\longrightarrow \\quad U_{i \\cdot} \\quad$ onto this coordinate system  $\\quad U=XVD^{-1}$\n",
    "\n",
    "- **Principle components** of $X \\quad \\longrightarrow \\quad U_{\\cdot j}$ (columns of $U$) which are **semi-orthonormal** so ($U_{\\cdot j}^TU_{\\cdot j}=1$) \n",
    "    - which also implies they are **linearly independent**\n",
    "    \n",
    "- **Squared singular values** $\\lambda_j^2=D_{jj}^2$ giving the \"proportion of total variance explained\"  $\\require{cancel}\\lambda_j^2\\cancel{\\text{Var}[U_j]}^1$ by each $V_{\\cdot j}$ **principal direction**\n",
    "  \n",
    "  \n",
    "The collection of $X_{\\cdot j}$ are assumed to be (likely **standardized**) **independent correlated random variables** now represented by the **standardized** ($E[U_{\\cdot j}] = 0$ and $\\text{Var}(U_{\\cdot j}) = 1$) collection of **observations** $U_{\\cdot j}$ (scaled by $\\lambda_j^2$) indexed in a different coordinate system (defined by orthogonal basis vectors $V_{\\cdot j}$)\n",
    "such that the $U_{\\cdot j}$ vectors are exactly **uncorrelated** (and hence empirically exactly **linearly independent**)\n",
    "\n",
    "Each column of $U$ and $X$ are linear combinations of each other...\n",
    "\n",
    "- $U_{\\cdot j} = \\left[X  V D^{-1}\\right]_{\\cdot j} = \\sum_{k=1}^m X_{\\cdot k} V_{k j}/\\lambda_j$ <font style=\"color:gray\">is a \"weighted average variable\" of the columns of $X$</font>\n",
    "- $X_{\\cdot j} = \\left[U D V^T \\right]_{\\cdot j} = \\sum_{k=1}^m \\lambda_k U_{\\cdot k} [V^T]_{k j} \\approx \\sum_{k=1}^r \\lambda_k U_{\\cdot k} [V^T]_{k j}$<font style=\"color:gray\"> is a \"weighted average variable\" of the columns of $U$</font><br>or can be a low rank approximation based on a ***compact SVD*** reconstruction\n",
    "\n",
    "and $\\require{cancel} \\text{Var}[\\sum_{j=1}^m \\lambda_j U_{\\cdot j}] = \\sum_{j=1}^m \\lambda_j^2 \\text{Var}[ U_{\\cdot j}] = \\sum_{j=1}^m \\sum_{k=1}^m  \\text{Var}[X_{\\cdot k} V_{k j}] = \\sum_{j=1}^m \\sum_{k=1}^m V_{k j}^2 \\cancel{\\text{Var}[X_{\\cdot k} ]}^1 = \\sum_{j=1}^m 1 =m $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1eeda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.array([[1,.2,.3],\n",
    "              [.2,1,.4],\n",
    "              [.3,.4,1]])\n",
    "n = 1000\n",
    "xyz = stats.multivariate_normal(mean=np.zeros(3), cov=C).rvs(n)\n",
    "xyz = (xyz-xyz.mean(axis=0))/xyz.std(axis=0)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.scatter(xyz[:,0],xyz[:,1],xyz[:,2], alpha=0.2, c=xyz[:,2], cmap=cm.coolwarm)\n",
    "\n",
    "U,d,Vt = np.linalg.svd(C)\n",
    "\n",
    "for i,c in enumerate(['red','green','blue']):\n",
    "    Principle_Direction = Vt[i,:]\n",
    "    ax.plot3D([0,Principle_Direction[0]*d[i]*2], \n",
    "              [0,Principle_Direction[1]*d[i]*2], \n",
    "              [0,Principle_Direction[2]*d[i]*2], c)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064ed0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.offline as pyo\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "pyo.init_notebook_mode()\n",
    "fig = go.Figure(data=go.Scatter3d(x=xyz[:,0], y=xyz[:,1], z=xyz[:,2], \n",
    "                                  mode='markers', \n",
    "                                  marker=dict(color=xyz[:,2], colorscale ='Viridis')))\n",
    "fig.update_traces(opacity=.25)\n",
    "\n",
    "for i,c in enumerate(['red','green','blue']):\n",
    "    Principle_Direction = Vt[i,:]\n",
    "    fig.add_scatter3d(x=[0,Principle_Direction[0]*d[i]*2], \n",
    "                      y=[0,Principle_Direction[1]*d[i]*2], \n",
    "                      z=[0,Principle_Direction[2]*d[i]*2], mode='lines',\n",
    "                      line=dict(color=c, width=4))    \n",
    "for i,c in enumerate(['red','green','blue']):\n",
    "    Principle_Direction = Vt[i,:]\n",
    "    fig.add_scatter3d(x=[0,Principle_Direction[0]*-d[i]*2], \n",
    "                      y=[0,Principle_Direction[1]*-d[i]*2], \n",
    "                      z=[0,Principle_Direction[2]*-d[i]*2], mode='lines',\n",
    "                      line=dict(color=c, width=4))    \n",
    "fig.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3792e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mtcars = sm.datasets.get_rdataset(\"mtcars\")\n",
    "Xtilde = (mtcars.data - mtcars.data.mean(axis=0))/mtcars.data.std(axis=0, ddof=0)\n",
    "# Xtilde.dot(Xtilde)/n is the sample covariance matrix for \"centered\" Xtilde \n",
    "U,D,Vt = np.linalg.svd(Xtilde/np.sqrt(Xtilde.shape[0])) # The actually PCA computation\n",
    "\n",
    "# The trace of the sample covariance matrix is the sum of the diagonal of the covariance matrix\n",
    "# which is the sum of the variances.\n",
    "# There are 11 variables and the Xtilde has been \"standardized\" so each variance is 1\n",
    "# so the trace is 11.\n",
    "Xtilde.values.std(axis=0,ddof=0)  # Xtilde.std(ddof=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665427ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sum of the eigenvalues of a square matrix equals the trace of the square matrix\n",
    "# The sum of the eigenvalues is thus the sum of the variances. \n",
    "(Xtilde.values.var(axis=0).sum(), (D**2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3233e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The eigenvalues are the square of the singular values\n",
    "D**2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b14fa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard PCA analyses for so called \"Principle Components\": \n",
    "# i^th column of U from the SVD is the i^th Principle Component\n",
    "# A compact SVD approximate reconstruction of Xtilde would use 1st through i^th Principle Components\n",
    "plt.figure(figsize=(18,5))\n",
    "\n",
    "plt.subplot(131) \n",
    "plt.plot(D**2,'.')\n",
    "plt.plot(D**2)\n",
    "plt.xlabel(\"Principle Component\")\n",
    "plt.ylabel(\"Singular Values of ~X\")\n",
    "plt.title(\"Variation Explained by Each Principle Component\")\n",
    "\n",
    "plt.subplot(132)\n",
    "p=2\n",
    "plt.plot(U[:,:p].dot(np.diag(D[:p])).dot(Vt[:p,:]).flatten(), \n",
    "         Xtilde.values.flatten()/Xtilde.shape[0]**.5, '.', \n",
    "         label=\"Rebuild ~X with 2 PCs\")\n",
    "p=4; plt.plot(U[:,:p].dot(np.diag(D[:p])).dot(Vt[:p,:]).flatten(), \n",
    "              Xtilde.values.flatten()/Xtilde.shape[0]**.5, '.', \n",
    "              label=\"Rebuild ~X with 4 PCs\")\n",
    "p=11\n",
    "plt.plot(U[:,:p].dot(np.diag(D[:p])).dot(Vt[:p,:]).flatten(), \n",
    "         Xtilde.values.flatten()/Xtilde.shape[0]**.5, '.', \n",
    "         label=\"Rebuild ~X with 11 PCs\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Rebuilt value\")\n",
    "plt.ylabel(\"Original value\")\n",
    "plt.title(\"Reconstructing ~X from its Principle Components\")\n",
    "\n",
    "plt.subplot(133); plt.plot(U[:,0],U[:,1],'.')\n",
    "plt.xlabel(\"Principle Component 1\")\n",
    "plt.ylabel(\"Principle Component 2\")\n",
    "plt.title(\"Interpreting Principle Components of ~X\")\n",
    "for i in range(0, U.shape[0]):\n",
    "    if all(np.sum(((U[:i,:2]-U[i,:2])**2).dot([[1],[15]]), axis=1) > .02):\n",
    "        plt.text(U[i,0],U[i,1],mtcars.data.index[i], horizontalalignment=\"right\")\n",
    "for j in range(Vt.shape[0]):\n",
    "    plt.plot([0,Vt[0,j]],[0,Vt[1,j]],'k'); plt.text(Vt[0,j], Vt[1,j], mtcars.data.columns[j], color='r')\n",
    "# Third plot interprets the \"Principle Directions\" associated with each \"Principle Component\"\n",
    "# The first \"Principle Component\" (the first column of U from the SVD) points in the direction of cyl, disp negative mpg axes\n",
    "# The elements of the first column of U are the coordinates of the data points along this direction\n",
    "# The blue points are data points, and you can see their locations relative to the Priniple Directions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ed75e6",
   "metadata": {},
   "source": [
    "### Independent Components Analysis (ICA) \n",
    "\n",
    "---\n",
    "\n",
    "**Linear independence** is not the same as **statistical independence**\n",
    "\n",
    "\n",
    "**Principal components** (columns of $U$ in $X=UDV^T$) are (**semi-**)**orthogonal** $\\Longrightarrow$ **linear independent** $\\Longrightarrow$ (empirically) ***uncorrelated***\n",
    "\n",
    "- But elements of the **principal components** (the entries in the rows of $U$) are **statistically dependent**\n",
    "\n",
    "The **principal directions** (columns of $V$) are **orthogonal** $\\Longrightarrow$  **linearly independent** \n",
    "\n",
    "- **Independent Component Analysis (ICA)** instead finds directions which may be **linearly dependent** but which nonetheless instead form a **basis** under which the data actually IS (empirically) **statistically independent** \n",
    "\n",
    "**PCA** identifies sequential **principal directions** of maximal data variation; whereas, **ICA** finds directions maximizing **kurtosis** (**heavy tailedness**); so, **PCA** naturally identifies **Gaussian covariance structure**; whereas, **ICA** contrarily works in the context of **Anti-Gaussianity**... the less **Gaussian** the data is the better **ICA** can find \"pointy independent directions\"\n",
    "\n",
    "\n",
    "![](https://alliance.seas.upenn.edu/~cis520/dynamic/2022/wiki/uploads/Lectures/PCA_ICA.png)\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5198f94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af47ee12",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
