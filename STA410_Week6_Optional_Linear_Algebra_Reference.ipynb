{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "921ac289",
   "metadata": {},
   "source": [
    "# Optional Reference Material: Linear Algebra \n",
    "\n",
    "A curious collection of various and sundry linear algebra things that have in some way at some point in time related to STA410...\n",
    "\n",
    "*Totally optional and just here for your reference in the event that you're chasing some clarity on something referenced in our homework or pre-lecture reading material for the week (and beyond?).*\n",
    "\n",
    "- Linear Independence and Orthogonality\n",
    "    - Rank and Bases\n",
    "- Eigenvalues and Eigenvectors\n",
    "    - Eigendecomposition \n",
    "    - Eigen Analysis: Understanding $Ax$ by its Eigenvalues\n",
    "- Stuff about $A^{-1}$ \n",
    "    - Inverse computation is unnecessarily wasteful\n",
    "    - Inversion computation is actually very often prone to numerical inaccuracy \n",
    "        - Sherman-Morrison-Woodbury Formula\n",
    "    - Backward Substitution\n",
    "    - Gaussian Elimination\n",
    "        - Elementary Operations\n",
    "        - LU Decomposition\n",
    "    - Generalized Inverses\n",
    "\n",
    "- Vector and Matrix Norms\n",
    "- Deriving the Matrix Condition Number\n",
    "    - The Matrix Condition Number using the $L_2$ norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd63f3e7",
   "metadata": {},
   "source": [
    "## Linear Independence and Orthogonality\n",
    "\n",
    "---\n",
    "\n",
    "The columns of a matrix $A_{\\cdot j}$ are **linearly indepdendent** if\n",
    "\n",
    "  $$ \\underbrace{\\sum_{j = 1}^n c_j A_{\\cdot j} = 0  \\;\\; \\Longrightarrow  \\;\\; c_j = 0 \\text{ for all } j}_{Ac \\;=\\; 0 \\;\\;\\Longrightarrow \\;\\;c \\;=\\; 0}$$\n",
    "\n",
    "A stronger condition than **linear independence** is **orthogonality** where \n",
    "\n",
    "$$ (A_{\\cdot j})^T A_{\\cdot k} =  \\sum_i A_{ij} A_{ik} = 0 \\text{ for all } j \\neq k \\quad \\text{ and } \\quad (A_{\\cdot j\n",
    "})^T A_{\\cdot k} \\neq 0 \\text{ for all } j$$\n",
    "\n",
    "since for nonzero columns $A_{\\cdot j}$ \n",
    "\n",
    "$$  \\underbrace{(A_{\\cdot j})^T A_{\\cdot k} = 0}_{\\text{Orthogonality}} \\quad \\Longrightarrow \\quad \\underbrace{c_jA_{\\cdot j} + c_kA_{\\cdot k} = 0 \\Longrightarrow c_j=c_k=0}_{\\text{Linear Independence}}$$\n",
    "\n",
    "but \n",
    "\n",
    "$$\\require{\\cancel} \\underbrace{c_jA_{\\cdot j} + c_kA_{\\cdot k} = 0 \\Longrightarrow c_j=c_k=0}_{\\text{Linear Independence}} \\quad \\cancel{\\Longrightarrow} \\quad  \\underbrace{(A_{\\cdot j})^T A_{\\cdot k} = 0}_{\\text{Orthogonality}}$$\n",
    "\n",
    "It is possible to transform two **linearly independent** columns $A_{\\cdot j}$ and $A_{\\cdot k}$ so that they are **orthogonal**, and the most common way to do this is known as the **(modified) Gram-Schmidt procedure**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6a55df",
   "metadata": {},
   "source": [
    "### Rank, and Bases \n",
    "\n",
    "The **rank** of the matrix $A_{n\\times m}$ is the number of **linearly independent** columns (and equivalently, rows) of $A$. The matrix $A$ is said to be **full rank** if $\\text{rank}(A_{n \\times m}) = \\min(n,m)$.  When $A$ is **square** so $A_{n\\times m} = A_{n\\times n}$, if $A$ is **full rank** then $\\text{rank}(A_{n \\times n}) = n$ and the $n$ columns of a $A_{n \\times n}$ are **linearly independent** and form a **basis** in $n$-dimensional space. \n",
    "\n",
    "- A **basis** formed by the $n$ **linearly independent** columns of a **square** matrix $A$ is a set of axes defining a coordinate system from which to index the $n$-dimensional space.  \n",
    "\n",
    "  >Any vector of an $n$-dimensional space $x$ may be given in terms of the coordinates of any **basis** formed by a **full rank square matrix** as \n",
    ">\n",
    ">$$b = \\sum_j c_j A_{\\cdot j}$$\n",
    ">\n",
    ">which illustrates that a ***basis*** does not define the space; rather, the ***basis*** just defines the way points $x$ in the space are referenced.\n",
    "Changing the ***basis*** does not change the space itself. \n",
    "\n",
    "The **standard basis** is $A_{n \\times n}=I$. The columns of $I$ are the **standard basis vectors** $e_j$.  The $e_j$ are **linearly independent** and **orthogonal**; and, because the length of these vectors in the n-dimensional space is 1, they are called **normal vectors**. \n",
    "\n",
    ">The (**Euclidean distance**) length of a vector is given by its the square root of its **inner (dot) product** with itself, so a column vector $A_{\\cdot j}$ is a **normal vector** if \n",
    ">\n",
    "> $$\\sqrt{A_{\\cdot j} \\cdot A_{\\cdot j}} = \\sqrt{(A_{\\cdot j})^T A_{\\cdot j}} = \\sqrt{\\sum_{j = 1}^n A_{i j}^2} = 1 \\quad \\text{ e.g., } \\quad e_j \\cdot e_j = e_j^T e_j =1$$ \n",
    "\n",
    "Vectors which are both **normal** and **orthogonal** are called **orthonormal**. The **standard basis** is thus an **orthonormal basis**. Two standard convensions that are common in this context are\n",
    "\n",
    "1. since two vectors are **linearly independent** regardless of their length, it is usual to specify the vectors of a **basis** in their **normal form**, and\n",
    "2. an **orthonormal basis** is often just called an **orthogonal basis** as the **orthogonality** is a much more crucial property of such a basis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99dbaf9",
   "metadata": {},
   "source": [
    "## Eigenvalues and Eigenvectors\n",
    "\n",
    "---\n",
    "\n",
    "**Eigenvalue** and **eigenvector** analysis of the linear transformation $A_{n\\times n}$ examines the rate of the expansion (and/or contraction) along the invariant directions of the transformation, respectively, as\n",
    "\n",
    "$$A_{n\\times n} V_{\\cdot j} = \\lambda_j V_{\\cdot j} \\quad \\text{often usefully encountered as} \\quad (A_{n\\times n} - \\lambda_j I) V_{\\cdot j} = 0$$\n",
    "\n",
    "Thus, for $x = \\sum_{j} c_j V_{\\cdot j}$ expressed in an **eigenvector basis** (regardless of the **rank** of $A_{n\\times n}$) \n",
    "\n",
    "   $$Ax= A\\left(\\sum_{j} c_j V_{\\cdot j}\\right) = \\sum_{j} c_j A  V_{\\cdot j} = \\sum_{j} c_j \\lambda_j V_{\\cdot j}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7acac7d",
   "metadata": {},
   "source": [
    "### Eigendecomposition \n",
    "\n",
    "---\n",
    "\n",
    "Any matrix $\\Sigma$ such that\n",
    "\\begin{align*}\n",
    "\\Sigma =  {}& \\Sigma^T & \\textbf{symmetric}\\\\\n",
    "x^T\\Sigma x {}& > 0 & \\textbf{positive definite}\\\\ \n",
    "\\end{align*}\n",
    "\n",
    "is **full rank** (so $\\text{rank}(\\Sigma_{n\\times n}) = n$) and may be a **covariance matrix**. For such matrices, there exists an **eigendecomposition** (or synonymously, **spectral decomposition** or **diagonal factorization**)\n",
    "\n",
    "\\begin{align*}\n",
    "\\Sigma_{n\\times n} = {} & V_{n\\times n} \\Lambda_{n\\times n} (V^T)_{n\\times n}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "such that  \n",
    "- **orthonormal eigenvectors** of $\\Sigma$ form the columns of the **orthonormal matrix** $V_{n\\times n}$  \n",
    "\n",
    "\n",
    "  $$\\begin{align*}\n",
    "  V_{\\cdot j}^TV_{\\cdot j} & {} = \\,\\;1\\;\\,  = V_{j\\cdot}^TV_{j\\cdot} & {} \\textbf{normal vectors}\\\\\n",
    "  V_{\\cdot j}^TV_{\\cdot k} & {} = \\,\\;0\\;\\,  = V_{j\\cdot}^TV_{k\\cdot}, j\\not=k & {} \\textbf{orthogonality}\\\\\n",
    "  V^TV & {} = I_{n\\times n}  = VV^T & {} \\textbf{orthonormality}\n",
    "  \\end{align*}$$\n",
    "\n",
    "- and corresponding positive **eigenvalues** \n",
    "\n",
    "  $$\\Lambda_{11}=\\lambda_1 \\geq \\Lambda_{22}=\\lambda_2 \\geq \\cdots \\geq \\Lambda_{nn}=\\lambda_n > 0$$\n",
    "\n",
    "  comprise the entries of the diagonal matrix $\\Lambda_{n\\times n}$ \n",
    "\n",
    "> The case of **symmetric positive definite** is quite distinct compared to more general **eigendecomposition**. \n",
    ">\n",
    "> [**Eigendecomposition**](https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix) exists more generally for (**square**) [**diagnalizable matrices**](https://math.stackexchange.com/questions/1811983/diagonalizable-vs-full-rank-vs-nonsingular-square-matrix) which might be neither **positive definite** nor **symmetric**. In this case, the  **eigendecomposition** is $V\\Lambda V^{-1}$ where $V^{-1}\\neq V^T$, and the  **eigenvalues** may not all be positive and the **eigenvectors** may not all be **orthogonal**:\n",
    "- the **eigenvectors** are **orthogonal** when $\\Sigma$ is **symmetric** since this means $V^{-1} = V^T$\n",
    "- the **eigenvalues** $\\lambda_i > 0$ of $\\Sigma$ are positive when **symmetric** $\\Sigma$ is **positive definite**\n",
    ">\n",
    "> **Eigendecomposition** also exists for **diagonalizable matrices** which are not **full rank**. In this case, $r>0$ **eigenvalues** will be nonzero $\\Lambda_{ii} = \\lambda_{i}$ for $i\\leq n-r$ and $\\Lambda_{ii} = 0$ for $r < i \\leq n$ in the diagonal matrix $\\Lambda$. The **eigendecomposition** then has the **compact** form\n",
    ">\n",
    "> $$A_{n\\times n} = V_{n \\times n} \\Lambda_{n \\times n} V^{-1}_{n \\times n} = V_{n \\times r} \\Lambda_{r \\times r} V^{-1}_{r \\times n}$$\n",
    "> \n",
    "> and the columns of $V_{n \\times r}$ will be **linearly independent** [so long as](https://math.stackexchange.com/questions/157382/are-the-eigenvectors-of-a-real-symmetric-matrix-always-an-orthonormal-basis-with) all **non-zero eigenavalues are unique** (i.e., have multiplicity $1$). The remaining columns in $V^T_{n \\times n}$ may be chosen arbitrarily, e.g., to also be **linearly independent** since they will not contribute to $V \\Lambda V^T$ for any diagonal element $\\Lambda_{ii} = 0$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef08553",
   "metadata": {},
   "source": [
    "### Eigen Analysis: Understanding $Ax$ by its Eigenvalues\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**Eigenvalues** determine many properties of an $A_{n \\times n}$ matrix.\n",
    "\n",
    "1. The **determinant** of the matrix $A_{n \\times n}$ is the product of the **eigenvalues** \n",
    "\n",
    "   $$\\det(A_{n\\times n}) = \\prod_{i=1}^n \\lambda_i$$ \n",
    "\n",
    "   and so characterizes the multiplicative change in the \"geometric volume\" of the space under the linear transformation $Ax$.\n",
    "\n",
    "2. The **spectral radius** of $A_{n\\times n}$ is the largest absolute **eigenvalue**\n",
    "\n",
    "   $$\\rho(A_{n\\times n}) = \\underset{i=1,...,n}{\\max} |\\lambda_i| \\leq \\begin{array}{c}\\underset{i=1,...,n}{\\max} \\sum_{j=1}^n |A_{ij}| \\\\ \\underset{j=1,...,n}{\\max} \\sum_{i=1}^n |A_{ij}|\\end{array}$$\n",
    "   \n",
    "   which represents the maximul \"radius\" of the transformation of the space under $A_{n\\times n}$ and influences many statistical and computational characteristics of $A_{n\\times n}$.\n",
    "\n",
    "3. The **trace** (sum of diagonal elements) of $A_{n\\times n}$ is the sum of the **eigenvalues** \n",
    "  \n",
    "   $$\\text{tr}(A_{n\\times n}) = \\sum_{i=1}^n A_{ii} = \\sum_{i=1}^n \\lambda_i$$\n",
    "\n",
    "   so, e.g., the \"total variance\" (sum of the diagonal elements) of a **covariance matrix** is the sum of the **eigenvalues** of the covariance matrix.\n",
    "\n",
    "<!--\n",
    "   > which can be shown using the [Jordan canonical form](https://math.stackexchange.com/questions/546155/proof-that-the-trace-of-a-matrix-is-the-sum-of-its-eigenvalues) $A=P J P^{-1}$ (whose diagonal elements $J_{ii}$ are the ***eigenvalues*** of $A$) and the cyclical $\\text{trace}(AB)=\\text{trace}(BA)$ property of the ***trace*** operator \n",
    "   >\n",
    "   > $$\\begin{align*}\\text{tr}(A) = {} & \\text{tr}(PJP^{-1}) = \\text{tr}(JP^{-1}P) = \\text{tr}(J) = \\sum_{i=1}^n J_{ii} = \\sum_{i=1}^n \\lambda_i \\end{align*}$$\n",
    "   -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d0466f",
   "metadata": {},
   "source": [
    "## Stuff about $A^{-1}$ \n",
    "\n",
    "---\n",
    "\n",
    "If we write $x=A^{-1}b$ as the solution to the system of linear equations $Ax = b$ we are implying that $A$ is an $n\\times n$ square matrix which is **full rank** or (synonymously) **invertible or nonsingular**. $A^{-1}$ doesn't exist for **non full rank** or (synonymously) **non-invertible or singular** matrices.\n",
    "\n",
    "> The use of the synonym **nonsingular** in place of the more straightforward term **invertible** is because if $A^{-1}$ does not exist, then the **inverse function**\n",
    ">\n",
    "> $$f(A) = A^{-1}$$\n",
    ">\n",
    "> is not defined at $A$ and so then $A$ is a point of [mathematical singularity](https://en.wikipedia.org/wiki/Singularity_(mathematics)) in the **domain** of $f$. \n",
    "\n",
    "$A_{n\\times n}^{-1} = $ [$\\det(A_{n\\times n})^{-1}\\operatorname {adj}(A_{n\\times n})$](https://en.wikipedia.org/wiki/Adjugate_matrix#Definition) and the **determinant** is the product of the **eigenvalues** of $A$ a well as the product of the (absolute) **singular values** of $A$. Thus, for **singular values** (and **eigenvalues**) of $A_{n\\times n}, \\lambda_j, 1 \\leq j \\leq n$, if\n",
    "\n",
    "| $\\lambda_j = 0$ for some $j$| $\\lambda_j \\not = 0$ for all $j$ |\n",
    "|-|-|\n",
    "| $\\det A = 0$ | $\\det A \\neq 0$ |\n",
    "| division by $0$ | no division by $0$ | \n",
    "|$A$ is **singular** | $A$ is **nonsingular** |\n",
    "| $A$ is **not invertible** | $A$ is **invertible** |\n",
    "| $A$ is **not full rank** | $A$ is **full rank** |\n",
    "| Some columns (rows) are | All columns (rows) are\n",
    "| **linearly dependent** | **linearly independent** |\n",
    "| $A^{-1}$ does not exist | $A^{-1}$ exists |\n",
    "\n",
    "Even if $A^{-1}$ exists, there are three problems:\n",
    "\n",
    "0. Inverse computation is **(usually)** not a simple algorithm, like **transpose** $A^T$ \n",
    "  - which just reverse the indexing scheme $\\quad[A^T]_{ij} = A_{ji}$\n",
    "  - and have simple higher order properties $\\quad (AB)^T = B^TA^T$\n",
    "\n",
    "    > However, notice that for **orthonormal** matrices (which are often simply just referred to as **orthogonal** matrices since the columns can be easily **standardized** into **normal vectors**)\n",
    "    > $$W_{n \\times n}^TW_{n \\times n}=W_{n \\times n}W_{n \\times n}^T = I_{n \\times n}.$$    \n",
    "    >\n",
    "    > and **inversion*** is **transposition**; and, this is partially true for \n",
    "    > **semi-orthogonal** (or **semi-orthonormal**) matrices where \n",
    "    >\n",
    "    >   $$S_{n \\times m}^TS_{n \\times m}=I_{m \\times m}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62caf5b7",
   "metadata": {},
   "source": [
    "### Inverse computation is unnecessarily wasteful\n",
    "\n",
    "  - since solving for $x$ in $Ax = b$ means solving $$A \\left[\\begin{array}{c}x_1\\\\\\vdots\\\\x_n\\end{array}\\right] = \\left[\\begin{array}{c}b_1\\\\\\vdots\\\\b_n\\end{array}\\right]$$\n",
    "  - but computing $x=A^{-1}b$ means either knowing or solving for $A^{-1}$\n",
    "  $$A \\left[\\!\\!\\!\\!\\!\\!\\begin{array}{c:c:c:c} & \n",
    "  \\begin{array}{c}A_{11}^{-1}\\\\\\vdots\\\\A_{n1}^{-1}\\end{array}& \n",
    "  \\begin{array}{c}A_{12}^{-1}\\\\\\vdots\\\\A_{n2}^{-1}\\end{array}&\\cdots&\n",
    "  \\begin{array}{c}A_{1n}^{-1}\\\\\\vdots\\\\A_{nn}^{-1}\\end{array}& \n",
    "  \\end{array}\\!\\!\\!\\!\\!\\!\\right] = \n",
    "  \\left[\\!\\!\\!\\!\\!\\!\\begin{array}{c:c:c:c} & \n",
    "  \\begin{array}{c}1\\\\0\\\\\\vdots\\\\\\vdots\\\\0\\end{array}& \n",
    "  \\begin{array}{c}0\\\\1\\\\0\\\\\\vdots\\\\0\\end{array}&\\cdots&\n",
    "  \\begin{array}{c}0\\\\\\vdots\\\\\\vdots\\\\0\\\\1\\end{array}& \n",
    "  \\end{array}\\!\\!\\!\\!\\!\\!\\right]$$\n",
    "\n",
    "     which requires solving the $n$ equations $AA_{\\cdot j}^{-1} = e_{j}$ for $j = 1, \\cdots, n$ for $A_{\\cdot j}^{-1}$ where $A_{\\cdot j}^{-1}$ is the $j^{th}$ column of $A^{-1}$ and $e_{j}$ is the **standard basis vector** with all elements equal to $0$ except the $j^{th}$ element which is equal to $1$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfebf3fe",
   "metadata": {},
   "source": [
    "### Inversion computation is actually very often prone to numerical inaccuracy \n",
    "\n",
    "<!-- *as is seen in this example taken from Keith Knight's STA410 [notes7.pdf](https://q.utoronto.ca/courses/296804/files?preview=24300633) document*\n",
    "--> \n",
    "\n",
    "   $$A = \\left[\\begin{array}{cc}1&1-\\epsilon\\\\1+\\epsilon&1\\end{array}\\right] \\quad \\text{with analytical inverse} \\quad \n",
    "A^{-1} = \\left[\\begin{array}{cc}\\epsilon^{-2}&\\epsilon^{-1}-\\epsilon^{-2}\\\\-\\epsilon^{-1}-\\epsilon^{-2}&\\epsilon^{-2}\\end{array}\\right]$$\n",
    "  - and $\\det(A) = |A| = A_{11}A_{22} - A_{12}A_{21} = \\epsilon^{2} \\not = 0 $ so $1/\\det(A) = \\det(A^{-1}) \\not = 0$ so $A$ is mathematically **invertible**\n",
    "   \n",
    "\n",
    "  - but if the magnitude of $\\epsilon^{-2}$ outranges that of $\\epsilon^{-1}$, the $\\epsilon^{-1}$ terms are lost due to **roundoff error** and so $A^{-1}$ can never be accurately represented since in that case \n",
    "\n",
    "  $$A^{-1} \\approx [A^{-1}]_c = \\left[ \\left[\\begin{array}{cc}\\epsilon^{-2}&\\epsilon^{-1}-\\epsilon^{-2}\\\\-\\epsilon^{-1}-\\epsilon^{-2}&\\epsilon^{-2}\\end{array}\\right] \\right]_c = \\left[\\begin{array}{cc}\\epsilon^{-2}&-\\epsilon^{-2}\\\\-\\epsilon^{-2}&\\epsilon^{-2}\\end{array}\\right]$$\n",
    "\n",
    "  so $\\det([A]_c)=0$ so $[A]_c$ is no longer ***invertible***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec061a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4861f4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "[[1.    0.995]\n",
      " [1.005 1.   ]]\n",
      "Condition(A)\n",
      "160001.99999113867\n",
      "\n",
      "\n",
      "A**-1\n",
      "[[ 40000. -39800.]\n",
      " [-40200.  40000.]]\n",
      "\n",
      "\n",
      "A @ A_inv\n",
      "[[ 1.0000000000001785e+00 -1.7763568394002505e-13]\n",
      " [-7.2759576141834259e-12  1.0000000000072760e+00]]\n",
      "\n",
      "\n",
      "I\n",
      "[[1. 0.]\n",
      " [0. 1.]]\n",
      "\n",
      "\n",
      "(A @ A_inv) == I\n",
      "[[False False]\n",
      " [False False]]\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/2891790/how-to-pretty-print-a-numpy-array-without-scientific-notation-and-with-given-pre\n",
    "np.set_printoptions(precision=16)\n",
    "\n",
    "# For the matrix\n",
    "epsilon =  .5/100 #  2**-30 # which is not that extreme, e.g., 2**1023 # \n",
    "A = np.array([[1, 1-epsilon], \n",
    "              [1+epsilon, 1]])\n",
    "# (some other potentially helpful matrix functionality:\n",
    "#  e.g, np.ones, np.diag_indices, np.fill_diagonal, etc.)\n",
    "print(\"A\")\n",
    "print(A)\n",
    "\n",
    "print(\"Condition(A)\")\n",
    "print(np.linalg.cond(A))\n",
    "\n",
    "# The analytical inverse is\n",
    "A_inv = np.array([[epsilon**-2, 1/epsilon-epsilon**-2],\n",
    "                  [-1/epsilon-epsilon**-2, epsilon**-2]])\n",
    "print(\"\\n\\nA**-1\")\n",
    "print(A_inv)\n",
    "\n",
    "# Which can be confirmed\n",
    "print(\"\\n\\nA @ A_inv\")\n",
    "print(A @ A_inv) # matrix multiplication\n",
    "print(\"\\n\\nI\")\n",
    "print(np.eye(2)) # identity matrix\n",
    "print(\"\\n\\n(A @ A_inv) == I\")\n",
    "print(A @ A_inv == np.eye(2)) # Confirmation\n",
    "\n",
    "# However, this breaks because \n",
    "# (0) general roundoff error; but, even for numbers that exactly representable  \n",
    "# (1) the magnitude of epsilon**-2 will outrange epsilon**-1 if epsilon is small..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ef8832",
   "metadata": {},
   "source": [
    "### Sherman-Morrison-Woodbury Formula\n",
    "\n",
    "---\n",
    "\n",
    "<!--\n",
    "- *The presentation in this section is taken from Keith Knight's STA410 [notes7.pdf](https://q.utoronto.ca/courses/296804/files?preview=24300633) document*. \n",
    "-->\n",
    "\n",
    "Also known as the ***Woodbury Matrix Identity***,\n",
    "\n",
    "$$(A + UCV)^{−1} = A^{−1} − A^{−1}U (C^{−1} + VA^{−1}U)^{−1}VA^{−1}$$\n",
    "\n",
    "makes inversion simple if $A$ and $C$ are diagonal.\n",
    "\n",
    "Thus, ***low rank*** $m<n$ matrix approximations, with $A=I$ and $C=1$ \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\Sigma_{n \\times n}^{-1} \\approx {} & (I_{n \\times n} + \\mathbf{u}_{n\\times m}(\\mathbf{v}^T)_{n\\times m})^{-1}\\\\\n",
    "= {} & I - \\mathbf{u}(1+\\mathbf{v}^T\\mathbf{u})^{-1}\\mathbf{v}^T\\\\\n",
    "= {} & I - \\frac{\\mathbf{u}\\mathbf{v}^T}{1+\\mathbf{v}^T\\mathbf{u}} \\quad \\text{ if } m=1\n",
    "\\end{align*}$$\n",
    "\n",
    "can be used to trivialize matrix inversion approximation calculations.\n",
    "\n",
    "In fact, performing computations on the basis of this identity can even avoid numeric problems.  Returning to the example of the previous section \n",
    "\n",
    "$$A = \\left[\\begin{array}{cc}1 & 1 - \\epsilon\\\\ 1\n",
    "+ \\epsilon & 1 \\end{array}\\right] = \\left[\\begin{array}{cc}0 & - \\epsilon\\\\  \\epsilon & 0 \\end{array}\\right] + \\left[\\begin{array}{c}1 \\\\1 \\end{array}\\right]\n",
    "\\left[\\begin{array}{c}1 \\\\1 \\end{array}\\right]^T$$\n",
    "\n",
    "and $x=A^{-1}b$ is not a computation that will work to solve $Ax = b$ if $A^{-1}$ cannot be accurately computed; however, by instead computing\n",
    "\n",
    "$$\n",
    "x = A^{-1}b = \\left(\\left[\\begin{array}{cc}0 & - \\frac{1}{\\epsilon}\\\\  \\frac{1}{\\epsilon} & 0 \\end{array}\\right] - \n",
    "\\frac{\n",
    "\\left[\\begin{array}{cc}0 & - \\frac{1}{\\epsilon}\\\\  \\frac{1}{\\epsilon} & 0 \\end{array}\\right] \\left[\\begin{array}{c}1 \\\\1 \\end{array}\\right] \\left[\\begin{array}{c}1 \\\\1 \\end{array}\\right]^T \\left[\\begin{array}{cc}0 & - \\frac{1}{\\epsilon}\\\\  \\frac{1}{\\epsilon} & 0 \\end{array}\\right]  \n",
    "}{1 +  \\left[\\begin{array}{c}1 \\\\1 \\end{array}\\right]^T   \n",
    "\\left[\\begin{array}{cc}0 & - \\frac{1}{\\epsilon}\\\\  \\frac{1}{\\epsilon} & 0 \\end{array}\\right]\n",
    "\\left[\\begin{array}{c}1 \\\\1 \\end{array}\\right]}\\right) b$$\n",
    "\n",
    "$Ax = b$ can be accurately solved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44c2344d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epsilon 0.01\n",
      "Condition number 40001.99997498112\n",
      "\n",
      "A^-1 @ b = ?\n",
      "@ means matrix multiply\n",
      "\n",
      "True Answer\n",
      "[[ 100.]\n",
      " [-100.]]\n",
      "\n",
      "Analytical Inverse\n",
      "[[ 100.]\n",
      " [-100.]]\n",
      "\n",
      "Woodbury's Identity\n",
      "[[ 100.]\n",
      " [-100.]]\n",
      "\n",
      "Calcluated Inverse\n",
      "[[ 100.]\n",
      " [-100.]]\n",
      "\n",
      "Linear Equation Solver\n",
      "[[ 100.]\n",
      " [-100.]]\n",
      "\n",
      "Calcluated Genearlized Inverse\n",
      "[[ 99.99999999994907]\n",
      " [-99.99999999994725]]\n"
     ]
    }
   ],
   "source": [
    "ep = 1e-2 #5, 7, 8,9,12\n",
    "A = np.array([[1,1-ep],[1+ep,1]])\n",
    "A_inv = np.array([[ep**-2, 1/ep - ep**-2],[-1/ep + -ep**-2,  ep**-2]])\n",
    "b = np.array([[1],[1]])\n",
    "print(\"epsilon\", ep)\n",
    "print(\"Condition number\", np.linalg.cond(A))\n",
    "print(\"\\nA^-1 @ b = ?\")\n",
    "print(\"@ means matrix multiply\")\n",
    "\n",
    "print(\"\\nTrue Answer\")\n",
    "print(np.array([[1/ep],[-1/ep]])) \n",
    "print(\"\\nAnalytical Inverse\")\n",
    "print(A_inv@b)\n",
    "B = np.array([[0,-ep],[ep,0]])\n",
    "u = np.ones((2,1))\n",
    "v = u.T\n",
    "B_inv = np.linalg.inv(B)\n",
    "print(\"\\nWoodbury's Identity\")\n",
    "print((B_inv - (B_inv @ u @ v @ B_inv) / (1 + v @ B_inv @ u) ) @ b)\n",
    "print(\"\\nCalcluated Inverse\")\n",
    "print(np.linalg.inv(A) @ b)\n",
    "print(\"\\nLinear Equation Solver\")\n",
    "print(np.linalg.solve(A, b))\n",
    "print(\"\\nCalcluated Genearlized Inverse\")\n",
    "print(np.linalg.pinv(A) @ b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1aef55",
   "metadata": {},
   "source": [
    "### Backward Substitution\n",
    "\n",
    "---\n",
    "\n",
    "Consider the easier problem of solving for $x$ in $A_{n \\times n}x = b$ when $A_{n \\times n}$ is given in ***upper triangular form***, where everything below the diagonal is zero and everything on the diagonal is non-zero.\n",
    "\n",
    "$$\\left[\\begin{array}{cccccc} \n",
    "a_{11}&a_{12}&a_{13}& \\cdots & a_{1(n-1)} & a_{1n}\\\\\n",
    " &a_{22} &a_{23} & \\cdots &a_{2(n-1)} & a_{2n} \\\\ \n",
    " &&a_{33} & \\cdots &a_{3(n-1)} & a_{3n} \\\\ \n",
    " &&& \\ddots & \\vdots & \\vdots \\\\\n",
    "& &&& a_{(n-1)(n-1)}& a_{(n-1)n}\\\\\n",
    "0 & &&& & a_{nn}\\\\\n",
    "\\end{array}\\right] \n",
    "\\left[\\begin{array}{c} \n",
    "x_1\\\\x_2\\\\x_3\\\\\\vdots\\\\x_{n-1}\\\\x_{n}\\\\\n",
    "\\end{array}\\right] = \n",
    "\\left[\\begin{array}{c} \n",
    "b_1\\\\b_2\\\\b_3\\\\\\vdots\\\\b_{n-1}\\\\b_{n}\\\\\n",
    "\\end{array}\\right]$$\n",
    "\n",
    "In this form $x$ can be solved for using ***backward substitution*** as\n",
    "\n",
    "$$x_n = \\frac{b_n}{a_{nn}} \\quad x_{n-1} = \\frac{b_{n-1} - a_{(n-1)n}x_n}{a_{(n-1)(n-1)}} \\quad \\cdots \\quad x_{n-j} = \\frac{b_{n-j} - \\sum_{i=n}^{n-j+1}a_{(n-j)i}x_i}{a_{(n-j)(n-j)}}$$\n",
    "\n",
    "so long as (the so-called ***pivot points***) $a_{jj} \\neq 0$ so there is no division by zero. \n",
    "\n",
    "For $x_j$, the final formula shows that there is $1$ division and $n-j$ multiplications and $n-j$ subtractions, so the total number of arithmetic computations to solve for all $x_j$ is \n",
    "\n",
    "$$\\sum_{j=n}^1 1 + 2(n-j) = \\sum_{j=0}^{n-1} (1 + 2j) = n + 2 \\sum_{j=0}^{n-1} j = n + 2\\frac{n(n-1)}{2} = n^2$$\n",
    "\n",
    "> The presentation above is given for ***square invertible*** A; but, the ***backward substitution*** algorithm can also find solutions to ***non-square*** systems of equations based on $A_{n\\times m}$ were the ***upper triangular form*** is exchanged with [row echelon form](https://en.wikipedia.org/wiki/Row_echelon_form) (where every row must have more leading zeros than the row above it). A system $A_{n\\times m}x = b$ itself may be \n",
    "> \n",
    "> - ***overdetermined*** ($n>m$) with more equations (rows) than the unknown variables (and the \"triangle\" completes before the final row of $A$)\n",
    "> - ***underdetermined*** $(n<m)$ so there are more free unknown variables than the number of equations (and the triangle doesn't complete before before the final row of $A$)\n",
    "> \n",
    "> and the system $A_{n\\times m}x = b$ may be \n",
    "> \n",
    "> - ***consistent*** *with a single solution*, e.g.,    \n",
    ">\n",
    ">   $$\\left[\\begin{array}{cc}\n",
    "1 & 1\\\\\n",
    "0 & 1\n",
    "\\end{array}\\right]\n",
    "\\left[\\begin{array}{c}\n",
    "x_1\\\\\n",
    "x_2\n",
    "\\end{array}\\right] = \n",
    "\\left[\\begin{array}{c}\n",
    "1\\\\\n",
    "1\n",
    "\\end{array}\\right] \n",
    "$$\n",
    "> \n",
    ">   in which case $\\text{rank}(A) = \\text{rank}(A|b)$, and the columns (and rows) of $A$ are ***linearly independent*** so no columns (and rows) will be linear combinations of each other\n",
    "> \n",
    "> - ***consistent*** *with infinitely many solutions*, e.g., \n",
    ">\n",
    ">   $$\\left[\\begin{array}{cc}\n",
    "1 & 1\\\\\n",
    "0 & 0\n",
    "\\end{array}\\right]\n",
    "\\left[\\begin{array}{c}\n",
    "x_1\\\\\n",
    "x_2\n",
    "\\end{array}\\right] = \n",
    "\\left[\\begin{array}{c}\n",
    "1\\\\\n",
    "0\n",
    "\\end{array}\\right] \n",
    "$$\n",
    "> \n",
    ">   in which case $\\text{rank}(A) = \\text{rank}(A|b)$, but some columns (and rows) of $A$ are ***linearly dependent*** so some columns (and rows) will be linear cominations of each other\n",
    ">   \n",
    ">   > i.e., for some sets of indices $\\mathcal{J} = \\{j_k: k=1,...,K\\}$ and $\\mathcal{I} = \\{i_k: k=1,...,K\\}$\n",
    ">   >\n",
    ">   > $$\\sum_{j \\in \\mathcal{J}} c_j A_{*j} = 0  \\;\\; \\not \\! \\Longrightarrow  \\;\\; c_j = 0 \\quad \\text{ and } \\quad \\sum_{i \\in \\mathcal{I}} c_i A_{i*} = 0  \\;\\; \\not \\! \\Longrightarrow  \\;\\; c_i = 0$$\n",
    "> \n",
    "> - ***inconsistent*** *with no solutions at all*, e.g., \n",
    "> \n",
    ">   $$\\left[\\begin{array}{cc}\n",
    "1 & 1\\\\\n",
    "0 & 0\n",
    "\\end{array}\\right]\n",
    "\\left[\\begin{array}{c}\n",
    "x_1\\\\\n",
    "x_2\n",
    "\\end{array}\\right] = \n",
    "\\left[\\begin{array}{c}\n",
    "0\\\\\n",
    "1\n",
    "\\end{array}\\right] \n",
    "$$\n",
    "> \n",
    ">   in which case $\\text{rank}(A) < \\text{rank}(A|b)$, and the column $b$ cannot be constructed as a linear combination of the columns of $A$.\n",
    "> \n",
    "> where $A|b$ is the $n \\times (m+1)$ [*augmented matrix*](https://en.wikipedia.org/wiki/Augmented_matrix)\n",
    "> \n",
    "> $$\\left[\\begin{array}{ccc:c} \n",
    "a_{11} & \\cdots & a_{1m} & b_1 \\\\\n",
    "\\vdots & \\ddots & \\vdots  & \\vdots \\\\\n",
    "a_{n1} & \\cdots & a_{nm} & b_m \\end{array}\\right]$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dc375c",
   "metadata": {},
   "source": [
    "### Gaussian Elimination\n",
    "\n",
    "---\n",
    "\n",
    "Converting a system of linear equations into ***upper triangular form*** (or the more general ***row echelon form***) is itself a quite simple process known as ***Gaussian elimination***.\n",
    "\n",
    "- Multiplying a row of the augmented matrix $A|b$ by a constant and adding to another row of the augmented matrix produces an equivalent system of linear equations to the one originally defined by the augmented matrix, i.e.,\n",
    "\n",
    "  $$x \\quad \\text{ solving } \\quad E^{ci+j}Ax = E^{ci+j}b \\quad \\text{ also solves } \\quad Ax = b$$\n",
    "\n",
    "- Multiplying and adding rows in this manner can produce leading zeros, e.g.,\n",
    "\n",
    "  $$\\left[\\begin{array}{ccc:c} \n",
    "a_{11} & \\cdots & a_{1m} & b_1 \\\\\n",
    "\\vdots & \\ddots & \\vdots  & \\vdots \\\\\n",
    "a_{n1} & \\cdots & a_{nm} & b_n \\end{array}\\right]\n",
    "\\quad \\overset{A|b \\; \\rightarrow \\; E^{c1+m}[A|b]}{\\longrightarrow} \\quad\n",
    "\\left[\\begin{array}{ccc:c} \n",
    "a_{11} & \\cdots & a_{1m} & b_1 \\\\\n",
    "\\vdots & \\ddots & \\vdots  & \\vdots \\\\\n",
    "a_{n1} + ca_{11} & \\cdots & a_{nm} +c a_{1m} & b_n + cb_1\\end{array}\\right]$$\n",
    "\n",
    "  and if $c = -\\frac{a_{n1}}{a_{11}}$ then $a_{n1} + ca_{11} = 0$ and the bottom left element of the resulting matrix vanishes (i.e., becomes $0$).\n",
    "\n",
    "When the column elements below a ***pivot point*** have been turned into zeros, the column and row of the ***pivot point*** are completed, the the ***Gaussian elimination*** process recurrsively restarts on the next ***pivot points*** in the top left corner of the submatrix without the completed row and column. \n",
    "\n",
    "$$\\left[\\begin{array}{c|ccc:c} \n",
    "a_{11} & a_{12} &  \\cdots & a_{1m} & b_1 \\\\\\hline\n",
    "0 & a_{22} + c_2 a_{2m} & \\cdots & a_{2m} + c_2 a_{1m} & b_2 + c_n b_1 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots  & \\vdots \\\\\n",
    "0 & a_{n2} + c_n a_{n1} & \\cdots & a_{n2} + c_n a_{nm} & b_n + c_n b_1\\end{array}\\right]$$\n",
    "\n",
    "For a ***square*** matrix $A$ where $m=n$, the above formulation [shows](http://www.it.uom.gr/teaching/linearalgebra/chapt6.pdf) that the number of divisions and multiplication-additions that are required to create an ***upper triangular form*** matrix (augmented with the transformed $b$ column) are\n",
    "\n",
    "$$\\sum_{j=1}^n (j+1)(j-1) + \\underset{\\text{due to } b}{(j-1)} = \\sum_{j=1}^n j^2 - 1 + (j-1) = \\frac{n(n+1)(2n+1)}{6} - n + \\frac{n(n+1)}{2} - n $$\n",
    "\n",
    "> #### Pivoting\n",
    "> An important computational caveat is that when the scalar multiplier $c$ is large, the numerical precision of the floating point-operation will be insufficient if \n",
    "> $$[b_{i'} + cb_i]_c = [cb_i]_c$$ \n",
    "> e.g., for three digits of precision, one step of ***Gaussian elimination*** on \n",
    ">\n",
    "> \\begin{align*}\n",
    "0.0001 x_1 + x_2 & {} = 1\\\\\n",
    "x_1 + x_2 & {} =  2\\\\\n",
    "\\quad \\quad \\quad \\quad \\quad \\; \\text{produces } \\quad \\quad \\quad & {}    \\\\\n",
    " \\quad 0.0001 x_1 + x_2 & {} = 1\\\\\n",
    " -10000x_2 & {} = -10000 \\quad \\text{ (the \"$+x_2$\" and the 2 are lost due to precision!)}\\\\\n",
    "\\end{align*}\n",
    ">\n",
    "> To fix this issue the rows may be reordered with a ***partial pivot*** so $c$ will be as small as possible, and ***Gaussian elimination*** step will then be \n",
    ">\n",
    "> \\begin{align*}\n",
    "x_1 + x_2 & {} =  2\\\\\n",
    "0.0001 x_1 + x_2 & {} = 1\\\\\n",
    "\\text{instead produces } \\quad \\quad \\quad & {}    \\\\\n",
    "x_1 + x_2 & {} =  2\\\\\n",
    "x_2 & {} = 1 \\quad \\text{ (has roundoff error but solution's more accurate)}\\\\\n",
    "\\end{align*}\n",
    ">\n",
    "> which gives $x_1=x_2=1$ which is a more accurate solution than $x_1=0$ $x_2=1$.\n",
    "> \n",
    "> If this was not sufficient, a ***full pivot*** which reorders both the rows and the columns as well could be used to produce an even smaller $c$.\n",
    ">\n",
    "> *This example is inspired by the **Pivoting** subsection of the \n",
    "Section 5.2 **Gaussian Elimination and Elementary Operator Matrices** in Chapter 5 **Numerical Linear Algebra** on page 212 of James E. Gentle's **Computational Statistics** textbook.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec822bc2",
   "metadata": {},
   "source": [
    "#### Elementary Operations\n",
    "\n",
    "---\n",
    "\n",
    "Mathematically, multiplying row $i$ by scalar $c$ and adding it to row $j$, and ***partial pivoting*** two rows $i$ and $j$ are so-called ***elementary operations*** and are represented by simple matrix multiplications $E^{ci+j}[A|b]$ and $E^{i\\leftrightarrow j}[A|b]$, respectively, where\n",
    "\n",
    "- $E^{ci+j} = I + c e_je_i^T$, so $E^{ci+j}_{kk}=1$ and $E^{ci+j}_{ji}=c$ and all other entries of $E^{ci+j}$ are $0$.\n",
    "\n",
    "  - $\\left(E^{ci+j}\\right)^{-1} = E^{-ci+j}$ since $E^{ci+j} E^{(-c)i+j} =  E^{(-c)i+j}E^{ci+j} = I$.\n",
    "\n",
    "- $E^{i\\leftrightarrow j} = I^{i\\leftrightarrow j}$ where row $i$ and $j$ have in the identity matrix $I$ have been switched, so all $E^{i\\leftrightarrow j}_{kk}=1$ except $E^{i\\leftrightarrow j}_{ii} = E^{i\\leftrightarrow j}_{jj} = 0$ and all other elements are $0$ except $E^{i\\leftrightarrow j}_{ij}=E^{i\\leftrightarrow j}_{ji}=1$.\n",
    "\n",
    "  - $(E^{i\\leftrightarrow j})^{-1} = E^{i\\leftrightarrow j}$ since $E^{i\\leftrightarrow j}E^{i\\leftrightarrow j}=I$ so it's (of course) easy to \"undo\" row interchanges.\n",
    "\n",
    "\n",
    "$$\n",
    "E^{ci+j} = \\left[\\begin{array}{ccccccc}\n",
    "1 & &&&&&0\\\\\n",
    "&1&&&&&\\\\\n",
    " && \\ddots&&\\\\\n",
    "& && 1 &&\\\\\n",
    " &&c&& \\ddots\\\\\n",
    "&&\\uparrow&&&1&\\\\\n",
    "0&&E^{ci+j}_{ji}&&&&1\\\\\n",
    "\\end{array}\\right] \n",
    "\\quad\\quad\n",
    "E^{i\\leftrightarrow j} = \\left[\\begin{array}{ccccccc}\n",
    "1 &0&&&&0&0\\\\\n",
    "0& \\ddots &&&&&0\\\\\n",
    " &\\cdots & 0 &\\cdots& 1&\\cdots \\\\\n",
    " &&& \\ddots \\\\\n",
    " &\\cdots & 1 &\\cdots& 0 &\\cdots \\\\\n",
    " 0 &&&&& \\ddots &0\\\\    \n",
    "0  &0&&&&0& 1 \\\\  \n",
    "\\end{array}\\right]\n",
    "\\begin{array}{c}\\leftarrow \\text{ row }i\\\\\\\\\\leftarrow \\text{ row }j\\\\\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6752c488",
   "metadata": {},
   "source": [
    "#### The LU Decomposition\n",
    "\n",
    "---\n",
    "\n",
    "Ignoring row interchanges $E^{i\\leftrightarrow j}$ which are easily applied and undone, ***Gaussian elimination*** transformation sequence \n",
    "\n",
    "$$\\prod E^{ci+j} \\quad \\text{and} \\quad \\left(\\prod E^{ci+j}\\right)^{-1} = \\prod E^{-ci+j} = L$$ \n",
    "\n",
    "will be ***lower triangular matrices*** and \n",
    "\n",
    "  $$U = \\left(\\prod E^{ci+j}\\right) A$$ \n",
    "\n",
    "[may](https://math.stackexchange.com/questions/218770/when-does-a-square-matrix-have-an-lu-decomposition/2274657) (if ***Gaussian elimination*** is working) be an ***upper triangular***, and when so\n",
    "\\begin{align*}\n",
    "Ax = {} & b\\\\\n",
    "\\left(\\prod E^{ic+j}\\right) Ax = {} & \\left(\\prod E^{ic+j}\\right) b\\\\\n",
    "Ux = {} & L^{-1}b\n",
    "\\end{align*}\n",
    "\n",
    "and $x$ may be solved for by simple ***backward substitution***.\n",
    "\n",
    "***LU decomposition*** is thus seen to be a byproduct of solving for $x$ using ***Gausian elimination*** where operation is done using the extended augmated matrix [$A|I|b$](https://en.wikipedia.org/wiki/Gaussian_elimination#Finding_the_inverse_of_a_matrix) instead of only $A|b$  \n",
    "\n",
    "$$\\left[\\begin{array}{ccc:ccc:c} \n",
    "a_{11} & \\cdots & a_{1m} & 1 & \\cdots & 0 & b_1 \\\\\n",
    "\\vdots & \\ddots & \\vdots  & \\vdots & \\ddots & \\vdots & \\vdots  \\\\\n",
    "a_{n1} & \\cdots & a_{nm} & 0 & \\cdots & 1 & b_n \\end{array}\\right]\n",
    "\\quad \\overset{[A|I|b] \\;\\rightarrow \\;L^{-1}[A|I|b]}{\\longrightarrow} \\quad \n",
    "\\left[ \\!\\begin{array}{c:c:c}  U & L^{-1} & b' \\!\\end{array}  \\right]$$\n",
    "\n",
    "Computationally all that needs to be kept track of is the sequence of the ***elementary operations*** $\\left(\\prod E^{ic+j}\\right)$ actualized during the ***Gausian elimination*** process, since \n",
    "\n",
    "$$U = \\left(\\prod E^{ic+j}\\right) A \\quad \\text{and} \\quad\n",
    "\\left(\\prod E^{ic+j}\\right)I = L^{-1} \\quad \\text{and} \\quad\n",
    "\\left(\\prod E^{ic+j}\\right)b = b'$$\n",
    "\n",
    "> The ***LU decomposition*** will be [unique](https://math.stackexchange.com/questions/1799854/is-the-l-in-lu-factorization-unique) if\n",
    ">\n",
    "> $$x^TAx \\geq 0 \\quad\\quad \\text{ subject to the constraint } \\quad \\quad L_{kk} = 1 \\; \\text{ or } \\; U_{kk} = 1 \\; \\text{ for all $k$}$$\n",
    ">\n",
    "> i.e., if $A$ is a ***square nonnegative definite matrix***, and the diagonals of either $L$ or $U$ all one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e5d025",
   "metadata": {},
   "source": [
    "#### Generalized Inverses\n",
    "\n",
    "---\n",
    "\n",
    "Returning to our system of linear equations $Ax = b$, it is very straightforward to analyticall calculate $x = A^{-1} b$ for ***full rank square*** matrices $A$ if we have the ***eigendecomposition*** or ***SVD*** of $A$ since \n",
    "\n",
    "- if $A$ is a ***symmetric*** and ***full rank*** then $A = V \\Lambda V^T$ and $A^{-1} = V \\Lambda^{-1} V^T$ with $\\Lambda^{-1}_{ii} = \\frac{1}{\\Lambda_{ii}}$ since\n",
    "\n",
    "\\begin{align*}\n",
    "A^{-1}A = {} & V \\Lambda^{-1} V^T V \\Lambda V^T\\\\\n",
    "= {} & V \\Lambda^{-1} \\;\\;\\, I \\;\\;\\, \\Lambda V^T\\\\\n",
    "= {} & V \\quad \\;\\;\\, I \\;\\;\\, \\quad V^T = I\\\\\n",
    "\\end{align*}\n",
    "\n",
    "- if $A$ is ***square*** and ***full rank*** $A = U D V^T$ and $A^{-1} = V D^{-1} U^T$ with $D^{-1}_{ii} = \\frac{1}{D_{ii}}$ since\n",
    "\n",
    "\\begin{align*}\n",
    "A^{-1}A = {} & V^T D^{-1} U U^T D V\\\\\n",
    "= {} & V^T D^{-1} \\;\\;\\, I \\;\\;\\, D V\\\\\n",
    "= {} & V^T \\quad \\;\\;\\; I \\;\\;\\;\\quad V = I\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Further, $Ax = b$ can be ***consistent*** (i.e., have at least one solution) even if $A$ is not squre or full rank.\n",
    "\n",
    "- $Ax = b$ is ***consistent*** if \n",
    "\n",
    "  $$\\text{rank}(A|b) = \\text{rank}(A)$$\n",
    "\n",
    "  where $A|b$ is the matrix made by appending the column $b$ as the rightmost column of $A$.\n",
    "\n",
    "If $Ax = b$ is ***consistent***, then a solution $x = A^{-}b$ based on $A^{-}$ only requires that \n",
    "\n",
    "$$A = A A^{-}A $$\n",
    "\n",
    "since\n",
    "\n",
    "\\begin{align*}\n",
    " Ax = {} & A A^{-}Ax\\\\\n",
    "  = {} & A A^{-}b\\\\\n",
    " \\Longrightarrow x = {} & A^- b \\quad \\text{ is a possible solution}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Matrices which can play the role of $A^{-}$ above, from strongest to weakest, are\n",
    "\n",
    "0. $A^{-1}$:  ***inverses*** which satisfy $A^{-1}A = AA^{-1} = I$\n",
    "1. $A^{+}$: (unique) ***Moore-Penrose inverses*** for which $A^{+}A$ and $AA^{+}$ are ***symmetric*** and which are also ***g1*** and ***g2 inverses***\n",
    "  > also called ***p-inverses***, ***normalized generalized inverses***, or ***pseudoinverses***\n",
    "2. $A^{*}$: ***g2 inverses*** for which $A^{*}AA^{*} = A^{*}$ and which are also ***g1 inverses***\n",
    "  > also called ***reflexive generalized inverses*** and ***outer pseudoinverses***\n",
    "3. $A^{-}$: ***g1 inverses*** for which $A A^{-}A = A$\n",
    "  > also called ***conditional inverses*** or ***inner pseudoinverses***\n",
    "\n",
    "Now, if $Ax = b$ is ***consistent***, then \n",
    "\n",
    "$$x = A^{+}b \\quad \\text{is a solution to} \\quad Ax = b$$\n",
    "\n",
    "where \n",
    "- $A^{+} = V D^{+} U^T$ and $D^{+}_{ii}=\\frac{1}{D_{ii}}$ is taken from the SVD $A = U D V^T$ \n",
    "and can be seen to satisfy $AA^{+}A = A$ and be the (unique) ***Moore-Penrose inverse***."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe194c1",
   "metadata": {},
   "source": [
    "## Vector and Matrix Norms\n",
    "---\n",
    "\n",
    "For $f(x) = Ax$ the statement $f(x) \\approx f(x+\\epsilon_x)$ means that $y = f(x) - f(x+\\epsilon_x) \\approx 0$ which is judged on the basis of the ***norm*** (or magnitude, or size) of the ***vector*** $y$, notated as $||y||$. The most common ***vector norms*** are the ubiquetous $L_p$ ***norms***\n",
    "\n",
    "$$||x||_p = \\left(\\sum_i |x_i|^{p}\\right)^{\\frac{1}{p}}$$\n",
    "\n",
    "$$||x||_2 = \\underset{L_2 \\text{: Euclidean}}{\\sqrt{\\sum_i x_i^2}} \\quad   \\quad ||x||_1 = \\underset{L_1 \\text{: Manhattan}}{\\sum_i |x_i|} \\quad  \\quad ||x||_\\infty = \\underset{L_\\infty \\text{: Chebyshev}}{\\max_i |x_i|}$$\n",
    "\n",
    "\n",
    "We can also define the ***norm*** (or magnitude, or size) of a matrix $A$.  One common method to do so is to induce a ***matrix norm*** from the $L_p$ ***vector norms*** as \n",
    "\n",
    "$$||A||_p = \\underset{x\\not=0}{\\max} \\frac{||Ax||_p}{||x||_p}$$\n",
    "\n",
    "Another common ***matrix norm*** is the ***Frobenius norm***\n",
    "$$||A||_F = \\left(\\sum_i\\sum_j A_{ij}^2\\right)^{\\frac{1}{2}} =  \\underbrace{\\sqrt{\\text{tr}(AA^T)} = \\sqrt{\\text{tr}(A^TA)} }_{\\text{trace doesn't care about transposes}} = \\underset{\\text{singular values of } A}{\\overset{\\text{The $L_2$ norm of the}}{\\sqrt{ \\sum_i \\lambda_i^2}}}$$\n",
    "\n",
    "which is just a direct extension of $L_2$ ***Euclidean distance***. \n",
    "\n",
    "<!-- ***Norms*** will be considered again in the context of function (vector) spaces. Some additional considerations regarding ***norms*** are available in Keith Knight's STA410 [notes8.pdf](https://q.utoronto.ca/courses/296804/files?preview=24301082) document. \n",
    "\n",
    "- Further analysis of ***condition*** in the context of $L_p$ ***induced matrix norms*** and of $L_p$ ***induced matrix norms*** themselves (e.g., using ***Manhattan*** and ***Chebyshev distance*** ***induced matrix norms*** to derive bounds on the ***spectral radius***) is available in Keith Knight's STA410 [notes8.pdf](https://q.utoronto.ca/courses/296804/files?preview=24301082) document. \n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67c0149",
   "metadata": {},
   "source": [
    "## Deriving the Matrix Condition Number of $x=f_{A}(b)=A^{-1}b$ \n",
    "\n",
    "\n",
    "When computing a solultion $x=f_{A}(b)=A^{-1}b$ to $Ax = b$ for an invertible linear transformation (***square full rank***) $A$, either\n",
    "\n",
    "$$\\require{cancel} \\underset{\\Large \\text{$f_A$ is well-conditioned}}{b+\\epsilon_b \\approx b \\Longrightarrow f_A(b+\\epsilon_b) \\approx f_A(b)} \\quad \\text{ or } \\quad \\underset{\\Large \\text{$f_A$ is ill-conditioned}}{b+\\epsilon_b \\approx b \\cancel{\\Longrightarrow} f_A(b+\\epsilon_b) \\approx f_A(b)}$$\n",
    "\n",
    "Since actual value $\\tilde x = [A^{-1}b]_c$ obtainable in the ${\\rm I\\!F}$ ***floating-point*** representation of ${\\rm I\\!R}$ will actually be a solution to a different problem \n",
    "\n",
    "$$A \\tilde x = \\tilde b \\quad \\text{with} \\quad \\tilde x = x + \\epsilon_x \\; \\text{and}\\; \\tilde b = b + \\epsilon_b.$$\n",
    "\n",
    "then, for any reasonable ***vector norm*** $||\\cdot||$ measuring magnitude, $A$ is called ***well-conditioned*** if \n",
    "\n",
    "- whenever $\\frac{||\\epsilon_b||}{||b||}$ is small $\\quad\\quad\\quad\\quad\\quad\\quad\\;\\;$  whenever the \"nearby problem\" actually being solved is \"close\"\n",
    "- $\\Longrightarrow$ then $\\frac{||\\epsilon_x||}{||x||}$ is also small $\\quad\\quad\\quad\\quad\\quad\\;\\Longrightarrow$ the solution is also \"close\"\n",
    "\n",
    "\n",
    "So a matrix $A$ is ***well-conditioned*** with respect to $x = f_A(b) = A^{-1}b$ if small changes in the input $b$ to the function, do not produces large changes in the output $x$.  Contrarily, $A$ is ***ill-conditioned*** if $f_A(b) = A^{-1}b$ is highly volatile relative to small changes in the input $b$.\n",
    "\n",
    "The ***condition*** of $A$ in $f_A(b) = A^{-1}b$ can actually be given a precise numeric quantification on the basis of the ***induced matrix norm*** $||A|| = \\underset{x\\not=0}{\\max} \\frac{||Ax||}{||x||}$ since\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{it implies } && ||b|| = {} &  ||Ax|| \\leq ||A|| \\; ||x|| \\\\\n",
    "\\text{and thus } && \\frac{1}{||x||} \\leq {} & \\frac{||A||}{||b||} \\\\ \\\\\n",
    "\\text{and since } && \\epsilon_x = {} & A^{-1} \\epsilon_b \\quad \\text{ (by the definition of $\\epsilon_x$ and $\\epsilon_b$)},\\\\\n",
    "&&||\\epsilon_x|| = {} & ||A^{-1} \\epsilon_b|| \\leq {} ||A^{-1}|| \\; ||\\epsilon_b||\\\\\\\\\n",
    "\\text{the product } &&\\frac{||\\epsilon_x||}{||x||} \\leq {} & \\underbrace{||A||\\;||A^{-1}||}_{\\Large \\kappa(A)}\\;\\frac{||\\epsilon_b||}{||b||} \\quad \\text{follows}\\\\ \n",
    "\\end{align*}\n",
    "\n",
    "Thus the [condition number](https://math.stackexchange.com/questions/4116544/any-example-of-condition-number-of-matrix-less-than-1) of $A$ for the problem of solving for $x$ in $Ax=b$ is defined as \n",
    "\n",
    "$$\\Large \\kappa(A) = ||A||\\;||A^{-1}||\\quad  \\quad  1 \\leq \\kappa(A) < \\infty$$\n",
    "\n",
    "and explicitly bounds how small $\\frac{||\\epsilon_x||}{||x||}$ will be relative to $\\frac{||\\epsilon_b||}{||b||}$, characterizing how rapidly the output of the function $x = f(b) = A^{-1}b$ can change for small changes in the input $b$. Large $\\kappa(A)$ means $\\frac{||\\epsilon_x||}{||x||}$ may not be small even if $\\frac{||\\epsilon_b||}{||b||}$ is small, so $A$ is ***well-conditioned*** if $\\kappa(A)$ is small and ***ill-conditioned*** otherwise. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d727f3f1",
   "metadata": {},
   "source": [
    "### The Matrix Condition Number using the $L_2$ norm\n",
    "\n",
    "For the $L_2$ ***induced matrix norm***, and a ***symmetric real valued*** matrix $A$ (which thus has ***real eigenvalues***)\n",
    "\n",
    "\\begin{align*}\n",
    "||A||_2 = {} & \\underset{x\\not=0}{\\max} \\frac{||Ax||_2}{||x||_2}\n",
    "= \\underset{||x||_2=1}{\\max} \\frac{||A(cx)||_2}{||(cx)||_2} \n",
    "= \\underset{||x||_2=1}{\\max} \\frac{|c|||Ax||_2}{|c|||x||_2} \n",
    "= \\underset{||x||_2=1}{\\max} ||Ax||_2 \\\\\n",
    "= {} & \\underset{i}{\\max} |\\lambda_i| \\quad \\text{ (the largest magnitude eigenvalue for square $A$)}\\\\\n",
    "\\color{gray}{=} {} & \\color{gray}{\\sqrt{\\rho(A^TA)} \\quad \\text{ (the square root of the spectral radius of the gramian of non-square $A$)}} \n",
    "\\end{align*}\n",
    "\n",
    "and for ***symmetric*** and ***diagonizable*** ([***normal***](https://en.wikipedia.org/wiki/Normal_matrix)) $A$ with a real-valued ***orthogonal eigendecomposition*** \n",
    "\n",
    "$$\\begin{align*}\n",
    "||A^{-1}||_2  = {} & ||\\overset{\\text{eigendecom-}}{\\overbrace{(V\\Lambda V^T)}^{\\text{position of } A}} {}^{-1}||_2  = ||V\\Lambda^{-1}V^T||_2  \\\\\n",
    " = {} & \\frac{1}{|\\lambda_\\min^A|} \\quad \\text{ (the reciprocal of the smallest magnitude eigenvalues of $A$)}\\end{align*}$$\n",
    "\n",
    "Thus, for the $L_2$ ***induced matrix norm***, the ***condition number*** $\\kappa(A) = ||A||_2\\;||A^{-1}||_2$ depends on the relative magnitudes of the smallest and largest ***eigenvalues*** with\n",
    "\n",
    "$$\\underset{\\text{the ratio of the largest and smallest eigenvalues}}{\\kappa(A) = ||A||_2 ||A^{-1}||_2 = \\frac{|\\lambda_\\max^A|}{|\\lambda_\\min^A|}} \\quad \\text{ and } \\quad \\frac{||\\epsilon_x||_2}{||x||_2} \\leq \\frac{|\\lambda_\\max^A|}{|\\lambda_\\min^A|}\\frac{||\\epsilon_b||_2}{||b||_2}$$\n",
    "\n",
    "\n",
    "<!--\n",
    "> Thus for ***square full rank*** $A$, the previously noted bound on the ***spectral radius***\n",
    ">\n",
    "> $$\\rho(A_{n\\times n}) = \\max_i |\\lambda_i| = ||A||_2 \\leq \\begin{array}{c} \\max_i \\sum_j |A_{ij}| \\\\ \\max_j \\sum_i |A_{ij}|\\end{array}$$\n",
    "> \n",
    "> follows since for ***eigenvalue*** and ***eigenvector*** pair $\\lambda_i$ and $v_i$, $|\\lambda_i| \\leq ||A||_p$ as seen from \n",
    "$$|\\lambda_i| = |\\lambda_i| \\,||v_i||_p = ||\\lambda_i v_i||_p = \\overbrace{||A v_i||_p \\leq ||A||_p ||v_i||_p}^{\\text{by definition of the induced norm}} = ||A||_p \\cancel{||v_i||_p}^1$$\n",
    ">\n",
    "> and the bounds are the ***Manhattan*** and ***Chebyshev distance*** ***induced matrix norms*** (as shown in Keith Knight's STA410 [class notes](https://q.utoronto.ca/courses/244990/files?preview=18669503))\n",
    ">\n",
    "> $$||A||_1=\\max_j \\sum_i |A_{ij}| \\quad \\text{ and } \\quad ||A||_\\infty =\\max_i \\sum_j |A_{ij}|$$\n",
    "-->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
