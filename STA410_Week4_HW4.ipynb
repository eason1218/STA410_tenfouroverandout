{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2803cfbb",
   "metadata": {},
   "source": [
    "# STA410 Week 4 Homework (4 points)\n",
    "\n",
    "Due 12 PM Jan 31 (before class starts)\n",
    "\n",
    "\n",
    "1. **Paired or individual assignment.** Work may be shared within pairs without restriction, but collaborations beyond the pairs must be limited to \"hints\" and may not share complete solutions.\n",
    "\n",
    "\n",
    "2. You are encouraged to adapt code you find available online **(including using AI chat bot suppport tools)** into your notebook; however, if you do so please provide a link to the utilized resource. ***If you don't and multiple students have the same answer an academic integrity proceeding may be undertaken.***  \n",
    "\n",
    "\n",
    "3. **Library imports are limited** to only libraries imported in the starter code and the [standard python modules](https://docs.python.org/3/py-modindex.html). Automated code tests that fail because of additional library imports will not recieve credit. Unless a problem instructs differently you may use any functions available from the Python stdlib and the libraries imported in the starter code.\n",
    "\n",
    "\n",
    "<details><summary><span style=\"color: blue; text-decoration: underline; cursor: pointer;\">Additional Details</span></summary>\n",
    "\n",
    "> **Do not delete, replace, or rearranged cells.** This erases `cell ids` upon which automated code tests are based. The \"Edit > Undo Delete Cells\" option in the notebook editor might be helpful; otherwise, redownload the notebook (so it has the correct required `cells ids`) and repopulate it with your answers (assuming you don't overwrite them when you redownload the notebook). ***You may add cells for scratch work*** but if required answers are not submitted through the provided cells where the answers are requested your answers may not be marked. Due to potential problems with `cell ids` **the only environments supported in this class are** [UofT JupyterHub](https://datatools.utoronto.ca/) or [Google Colab](https://colab.research.google.com/)\n",
    ">\n",
    "> **No jupyter shortcut commands** such as `! python script.py 10` or `%%timeit` may be included in the final submission as they will cause subsequent automated code tests to fail.\n",
    ">\n",
    "> **No cells may have any runtime errors** because this causes subsequent automated code tests to fail and you will not get marks for tests which fail because of previous runtime errors. ***Restart and re-run the cells in your notebook to ensure there are no runtime errors before submitting your work.***\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fb385d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you may use any functions available from the following library imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15eb3de",
   "metadata": {},
   "source": [
    "## Student and Contribution\n",
    "\n",
    "Are you working with a partner to complete this assignment?  \n",
    "- If not, assign  the value of `None` into the variable `Partner`.\n",
    "- If so, assign the name of the person you worked with into the variable `Partner`.\n",
    "    - Format the name as `\"<First Name> <Last Name>\"` as a `str` type, e.g., \"Scott Schwartz\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de783f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Partner = #None\n",
    "# This cell will produce a runtime error until you assign a value to this variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca745bd5",
   "metadata": {},
   "source": [
    "What was your contribution in completing the code for this assignments problems?  \n",
    "Assign one of the following into each of the `Contribution` variable below.\n",
    "\n",
    "- `\"I worked alone\"`\n",
    "- `\"I contributed more than my partner\"`\n",
    "- `\"My partner and I contributed equally\"`\n",
    "- `\"I contributed less than my partner\"`\n",
    "- `\"I did not contribute\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a33dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Contribution = #\"I worked alone\"\n",
    "# This cell will produce a runtime error until you assign a value to this variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f042ae",
   "metadata": {},
   "source": [
    "# Sequential Importance Sampling for Hidden Markov Models\n",
    "\n",
    "A simple version of a **Normal-Normal Hidden Markov Model (HMM)** is \n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "Y_t \\sim {} & N(X_t, \\sigma) \\quad\\;  \\textrm{observed for $t=1,\\cdots,T$ }\\\\\n",
    "X_t \\sim {} & N(X_{t-1}, 1) \\quad \\textrm{not unobserved}\\\\\n",
    "% X_0 \\equiv {} & 0 \\equiv \\tilde X_0 \n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "and a **sequential importance sampling proposal** for this **HMM** using the so-called [slash distribution](https://en.wikipedia.org/wiki/Slash_distribution) is\n",
    "\n",
    "$\\tilde X_t = \\tilde X_{t-1} + Z_t/U_t \\quad \\textrm{ for } Z_t \\sim N(0, 1) \\textrm{ and } \n",
    "U_t \\sim U(0, 1)$\n",
    "\n",
    "The **sequential normalized importance weight** up the $t^{th}$ step in the **importance sampling proposal** $   \\mathbf{\\tilde X}_t = (\\tilde X_1, \\cdots, \\tilde X_t)$  \n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\require{cancel}\n",
    "   w_1^* & = {} \\frac{p_{X_1|Y_1}(\\tilde x_1|y_1)}{q_{\\tilde X_1}(\\tilde x_1)} = \\frac{p_{Y_1 | X_1}(y_1 | \\tilde x_1) p_{X_1}(\\tilde x_1) \\overset{\\text{normalization}}{\\cancel{/ p_{Y_1}(y_1)}}}{q_{\\tilde X_1}(\\tilde x_1) } \\quad\\quad\\;\\; \\textrm{ with initial } &&\\textrm{$p_{X_1}(\\tilde x_1)$ and $q_{\\tilde X_1}(\\tilde x_1)$ based on $\\tilde X_0=0$}\\\\\n",
    "   & \\longrightarrow \\quad w_{1j} = \\frac{w_{j1}^*}{\\sum_j w_{j1}^*} \\quad \\textrm{ for $j^{th}$ proposal } \\quad \\tilde x_{j1} \\\\ \\\\\n",
    "    w_t^* & = {} \\frac{p_{\\mathbf{X}_t,\\mathbf{Y}_t}(\\mathbf{\\tilde x}_t,\\mathbf{y}_t)/p_{\\mathbf{Y}_t}(\\mathbf{y}_t)}{q_{\\mathbf{\\tilde X}_t}(\\mathbf{\\tilde x}_t)}  \\\\   \n",
    "   & = {} \\underbrace{\n",
    "   \\frac{p_{\\mathbf{X}_{t-1},\\mathbf{Y}_{t-1}}(\\mathbf{\\tilde x}_{t-1},\\mathbf{y}_{t-1})/p_{\\mathbf{Y}_{t-1}}(\\mathbf{y}_{t-1})}{q_{\\mathbf{\\tilde X}_{t-1}}(\\mathbf{\\tilde x}_{t-1})}}_{w^*_{t-1}}\n",
    "    \\frac{1}{\\underset{\\text{normalization}}{\\cancel{p_{\\mathbf{Y}_t}(\\mathbf{y}_t)}}}\n",
    "    \\overset{\\textrm{HMM Markovian conditional dependence} }{\\frac{p_{Y_t|X_{t}}( y_t | \\tilde x_t) p_{X_t|X_{t-1}}(\\tilde x_t | \\tilde x_{t-1})}{ q_{\\tilde X_t | \\tilde X_{t-1}}(\\tilde x_t | \\tilde x_{t-1})}}  && \\begin{array}{l}q_{\\tilde X_t | \\tilde X_{t-1}}(\\tilde x_t | \\tilde x_{t-1}) \\textrm{ is not } p_{\\tilde X_t | \\tilde X_{t-1}}(\\tilde x_t | \\tilde x_{t-1})\\\\\\textrm{ so these only cancel if } \\tilde x_t \\sim N(\\tilde x_{t-1},1) \\end{array} \\\\ \n",
    "& = {} w_{t-1}^* w^*_{t|t-1} \\\\ \n",
    "&   \\longrightarrow \\quad w_{1j} = \\frac{ w^*_{j(t-1)} w^*_{j(t|t-1)}  }{\\sum_j w^*_{j(t-1)} w^*_{j(t|t-1)} } \n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "\n",
    "> This problem is inspired by Chapter 6.3.2 **Sequential Monte Carlo** and particularly draws upon sections 6.3.2.5 **Particle Filters** on page 179, 6.3.2.4 **Sequential Importance Sampling for Hidden Markov Models** on pages 175-176, 6.3.2.3 **Weight Degeneracy, Rejuvenation, and Effective Sample Size**, and Example 6.3 **Slash Distribution** pages 165-166 of the Givens and Hoeting **Computational Statistics** textbook.\n",
    ">\n",
    "> Errata: the right side equation 6.33 on page 175 in section 6.3.2.4 **Sequential Importance Sampling for Hidden Markov Models** has the typo $f_t$ which should be $f_{t-1}$ and the left side of the equation is wrong and should either be $f_{t}(x_{1:t},y_t|y_{1:(t-1)})$, or the normalizing constant $p(y_t|y_{1:(t-1)})$ could be added as a denomenator on the right hand side, or the equation could have alternatively been specified in terms of joint distributions rather than conditional distributions as $f_{t}(x_{1:t},y_{1:t}) = f_{t-1}(x_{1:(t-1)},y_{1:(t-1)})p_x(x_t|x_{t-1})p_y(y_t|x_t)$, e.g., as is done [here](https://www.almoststochastic.com/2013/08/sequential-importance-sampling.html). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af394560",
   "metadata": {},
   "outputs": [],
   "source": [
    "T,noise_to_signal = 100,1\n",
    "x,y = np.zeros(T),np.zeros(T)\n",
    "np.random.seed(9)  # np.random.seed(9)\n",
    "x[0], y[0] = 0, x[0] + stats.norm(scale=noise_to_signal).rvs()\n",
    "for t in range(1,T):\n",
    "    x[t] = x[t-1] + stats.norm.rvs()\n",
    "    y[t] = x[t] + stats.norm(scale=noise_to_signal).rvs()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(x, label='Signal')\n",
    "plt.plot(y, label='Signal+Noise')\n",
    "plt.title(\"How well can the original signal be detected under the HMM specification given above?\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c9f375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define slash_rvs based on `stats.` methods which provides samples from the slash distribution\n",
    "slash_rvs = lambda x: x  # input x will be $\\tilde x_{t-1}$ (an np.array)\n",
    "                         # output will be $\\tilde x$ as above (an np.array)\n",
    "    \n",
    "# define slash_pdf based on `np.` methods which provides a pdf evaluation of the slash distribution \n",
    "slash_pdf = lambda x: x # input x will be the $(\\tilde x - \\tilde x_{t-1})$ (an np.array)\n",
    "                         # output will be the density evaluations (an np.array)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801f208a",
   "metadata": {},
   "source": [
    "- p1q1: define density $p_{Y_t|X_{t}}( y_t | \\tilde x_t)$  \n",
    "- p1q2: define density $p_{X_t|X_{t-1}}(\\tilde x_t | \\tilde x_{t-1})$  \n",
    "\n",
    "- p1q3: what is $\\frac{p_{\\mathbf{X}_t,\\mathbf{Y}_t}(\\mathbf{\\tilde x}_t|\\mathbf{y}_t)}{q_{\\mathbf{\\tilde X}_t}(\\mathbf{\\tilde x}_t)}$ equal to?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2ffae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1q1 = lambda xt,yt,noise_to_signal: xt*yt*noise_to_signal\n",
    "p1q2 = lambda xt,xtm1: xt*xtm1\n",
    "p1q3 = \"<A|B|C|D>\"\n",
    "# A: `w_star_t`\n",
    "# B: `w_star_t_minus_1`\n",
    "# C: `w_star_t * w_star_t_minus_1`\n",
    "# D: None of the above\n",
    "\n",
    "p1q4 = 'In the specification given above \"normalization cancellation\" refers to '+\\\n",
    "       \"<conditional distributions|markov properties|importance weight standardization|normality>\"\n",
    "\n",
    "p1q5 = \"Markov here means <A|B|C|D>\"\n",
    "# A: Y_t and X_{t-1} are conditionally independent given X_t \" Y_t _||_ X_{t-1} | X_t \"\n",
    "# B: X_t and X_{t-1} are conditionally independent given X_{t-2} \" X_t _||_ X_{t-1} | X_{t-2} \"\n",
    "# C: X_t and X_{t-2} are conditionally independent given X_{t-1} \" X_t _||_ X_{t-2} | X_{t-1} \"\n",
    "# D: tilde X_t and Z_{t} are conditionally independent given U_{t} \" X_t _||_ Z_t | U_t \"\n",
    "# where \"A _||_ B | C\" is my ascii attempt at the standard conditional independence notation\n",
    "# https://en.wikipedia.org/wiki/Conditional_independence#Conditional_independence_of_events\n",
    "\n",
    "p1q6 = 'The \"HMM Markovian conditional dependence\" notation in the specification above refers to '+\\\n",
    "       \"<A|B|C|D>\"\n",
    "# A: p(x_t, y_t)/p(y_t) = p(x_t|y_t)\n",
    "# B: p(x_t, y_t) = p(y_t|x_t)p(x_t)\n",
    "# C: p(y_t|x_t) = p(y_t|x_t,...)\n",
    "# D: q(x_t|x_{t-1}) = q(x_t|x_{t-1},...)\n",
    "\n",
    "p1q7 = \"Under the given HMM specification p(y_t|x_t) is the same as <A|B|C|D>\"\n",
    "# A: p(y_t|x_t,y_{t-1},x_{t-1})\n",
    "# B: p(y_t,x_t|y_{t-1})\n",
    "# C: p(y_t,x_t|y_{t-1},x_{t-1},...)\n",
    "# D: q(x_t|x_{t-1})\n",
    "p1q8 = \"Under the given HMM specification p(y_t,x_t|x_{t-1}) is the same as <A|B|C|D>\"\n",
    "\n",
    "p1q9 = \"The distribution used for sequential proposal generation is a \"+\\\n",
    "       \"<uniform|normal|standard normal|slash|normal-inside-of-a-normal> distribution\"\n",
    "    \n",
    "p1q10 = \"The cancelling notation is the term which creates <A|B|C|D>\"\n",
    "# A: the full joint q(x) over all x [combining the denomenators]\n",
    "# B: the conditional probability p(x_{t-1}|y_{t-1},...) [on the left]\n",
    "# C: the conditional probability p(x_t,y_t|y_{t-1},x_{t-1},...) [on the right]\n",
    "# D: the full joint p(x) over all x [combining the numerators]    \n",
    "\n",
    "p1q11 = \"The product of the q's in the denomenator of the second line for w*_t \"+\\\n",
    "        \"is of the form p(a,b) = p(a|b)p(b); but, ignoring these q terms the \"+\\\n",
    "        \"remainder of the product in the second line for w*_t is of the form <A|B|C|D>\"\n",
    "# A: p(a|b)p(b) = p(a,b)\n",
    "# B: p(a|b,c)p(b|c) = p(a,b|c)\n",
    "# C: p(a,b)/p(b) = p(a|b)\n",
    "# D: p(a)p(b) = p(a,b)\n",
    "\n",
    "p1q12 = \"In the figure above j indexes over the <y-axis|x-axis|neither axis>\"\n",
    "\n",
    "p1q13 = \"The effective sample size related to this method is <A|B|C|D> and is based on \"+\\\n",
    "        \"the relative importance of <a collection of|points in a> trajector<ies|y>\"\n",
    "# A: 1/sum_j w*_{tj}**2\n",
    "# B: 1/sum_j w_{tj}**2\n",
    "# C: 1/sum_t w*_{tj}**2\n",
    "# D: 1/sum_t w_{tj}**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ddf9c4",
   "metadata": {},
   "source": [
    "# Part 2: Implementation\n",
    "\n",
    "- Define the sequential importance sampling weights function `SIS_w` below\n",
    "- Do not change the `normalize` and `bootstrap_indices` functions\n",
    "    - the details are technical and may be safely ignored for the purposes of this assignment\n",
    "- Within the sequential importance algorithm defined by the `SIR` function \n",
    "    - complete the initialization of `x_[:,0]` and `w_star[:,0]` (at the first timepoint)\n",
    "    - complete the `extension of SIS proposal from x_[:,t-1] to x_[:,t]`\n",
    "    - complete the calculation of `n_eff[t]` and the final update of `w_[:,t]` resulting from rejuvenation\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768ffd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "p2q1 = \"The length of each of the vector inputs into `SIS_w` is \"+\\\n",
    "       \"<1|n|T|nXn|n x T|TxT>\"\n",
    "\n",
    "p2q2 = \"The length of y input into `SIS_w` is \"+\\\n",
    "       \"<1|n|T|nXn|n x T|TxT>\"\n",
    "\n",
    "p2q3 = \"The shape of the data as structured in the algorithm \"+\\\n",
    "       \"given by the `SIR` function is <1|n|T|nXn|n x T|TxT>\"\n",
    "\n",
    "p2q4 = \"The shape of the effective sample size data structure is \"+\\\n",
    "       \"<1|n|T|nXn|n x T|TxT>\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fbcdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 600  # number of particles (i.e., importance sampling proposals)\n",
    "# other distributions as specified and assume `noise_to_signal` known\n",
    "def SIS_w(y, x_t, x_t_minus_1, w_star_t_minus_1, proposal_pdf):\n",
    "    # create unnormalized sequential importance weights\n",
    "    # based on the inputs, stats.norm, and proposal_pdf \n",
    "    pass\n",
    "    \n",
    "    w_star_t = None \n",
    "\n",
    "    return w_star_t_minus_1*w_star_t\n",
    "\n",
    "\n",
    "# normalizing weights for resampling has a known problem which is avoided by this function\n",
    "# https://github.com/numpy/numpy/issues/8317\n",
    "def normalize(w, badp = True):\n",
    "    while badp:\n",
    "        # `stats.multinomial` does the following \"correction\" \n",
    "        w[w==min(w[:-1][w[:-1]>0])] = 0\n",
    "        w[-1] = 1. - w[:-1].sum()\n",
    "        w = w/np.sort(w)[:-1].sum()\n",
    "        # which can accidentally cause the checks below to fail\n",
    "        badp = np.any(w < 0, axis=-1) | np.any(w > 1, axis=-1) | w[:-1].sum() > 1\n",
    "    return w        \n",
    "\n",
    "\n",
    "# Helper function for particle filter bootstrap rejuvenation\n",
    "def bootstrap_indices(ireps):\n",
    "    return np.concatenate([r*[i] for i,r in enumerate(ireps)]).flatten().astype(int)\n",
    "    \n",
    "    \n",
    "# sequential importance resampling\n",
    "# https://en.wikipedia.org/wiki/Particle_filter#Sequential_Importance_Resampling_(SIR)    \n",
    "def SIR(y, n, rejuvenation_threshold, proposal_rvs, proposal_pdf):\n",
    "    T = y.shape[0]\n",
    "    # assume x0 = x_0 = 0\n",
    "    # x_ will be shorthand for x_tilde\n",
    "    x_,w_star_t,n_eff,x_bar = np.zeros((n,T)), np.ones((n,T)), np.ones(T)*n, np.zeros(T)\n",
    "    x_[:,0] = None\n",
    "    w_star[:,0] = None\n",
    "    \n",
    "    for t in range(1, T):\n",
    "        # extension of SIS proposal from x_[:,t-1] to x_[:,t]\n",
    "        x_[:,t] = None\n",
    "        w_star_t[:,t] = SIS_w(y[t], x_[:,t].copy(), x_[:,t-1].copy(), w_star_t[:,t-1].copy(), proposal_pdf)  \n",
    "        # normalizing SIS weights for subsequent bootstrapping, i.e., particle filter rejuvenation\n",
    "        w = w_star_t[:,t]/w_star_t[:,t].sum()  # this \"should\" be sufficient\n",
    "        n_eff[t] = None\n",
    "        x_bar[t] = (x_[:,t]*w).sum()  # Something to consider: what is the variance of this estimate?\n",
    "        # How does it compare to the true cumulative MSE of the bootstrap particle filter estimator?\n",
    "        # $$\\frac{\\sum_{t=1}^T\\left(\\frac{\\sum_{j=1}^n \\tilde x_{jt}}{n} - x_t\\right)^2}{n}$$ \n",
    "\n",
    "        # Sequence weights decay over time as a bad proposals at time t erode overall proposal quality\n",
    "        # The weights can be rejuvenated, however, by using a bootstrapping step in the partical filter\n",
    "        if (n_eff[t] < rejuvenation_threshold) or (t==(T-1)):\n",
    "            \n",
    "            try:  # bootstrapped partical filter rejuvenation with SIS weights\n",
    "                bs_samp = stats.multinomial(n, p=w).rvs(size=1)[0]\n",
    "            except:  # if the issue noted above in `normalize` in encountered\n",
    "                bs_samp = stats.multinomial(n, p=normalize(w)).rvs(size=1)[0]\n",
    "            \n",
    "            x_[:,:(t+1)] = x_[bootstrap_indices(bs_samp),:(t+1)]\n",
    "            w_star_t[:,t] = None # bootstrapped sample is an i.i.d. sample of the particles\n",
    "            \n",
    "    return x_, x_bar, n_eff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8f2edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "p2q5 = \"The particles in the sequential importance resampling algorithm are \"+\\\n",
    "       \"<rejuvenated|standardized> when the <effective sample size|partical number> \"+\\\n",
    "       \"is <low|increased|iterated|sufficient> as determined by <w|w_star|n|t|T>\"\n",
    "\n",
    "p2q6 = \"Rejuvenation is based upon <importance|uniformly random|weighted bootstrap> \"+\\\n",
    "       \"sampling of <particles|time points|observed data|weights> using a \"+\\\n",
    "       \"<multinomial|normal|normal-within-a-normal|slash|uniform> distribution with inputs \"+\\\n",
    "       \"<n and 1/n|n and w|n and w_star|x_t|x_[:,t]|x_[:,t-1]|x_[:,t]-x_[:,t-1]>\"\n",
    "\n",
    "p2q7 = \"The rejuvenation step itself draws <i.i.d.|sequentially markov dependent> samples\"\n",
    "\n",
    "p2q8 = 0  # The numeric value of unnormalized importance sampling weights upon rejuvenation\n",
    "p2q9 = 0  # The numeric value of the effective sample size upon rejuvenation (for the setup below)\n",
    "\n",
    "p2q10 = \"The <rejuvenation|proposal> process will <increase|reduce> the diversity of \"+\\\n",
    "        \"the particles which will then <be unique|contain dupliates> so that \"+\\\n",
    "        \"the `extension of SIS proposal from x_[:,t-1] to x_[:,t]` will create \"+\\\n",
    "        \"<more|less> homogenous importance weights and hence a better \"+\\\n",
    "        \"<importance|uniformly random|weighted bootstrap|multinomial|normal|slash> sample\"\n",
    "   \n",
    "p2q11 = \"The <particles|weights> in sequential importance resampling refer to \"+\\\n",
    "        \"the collection of trajectory <enpoints|sequences|normalizations> and \"+\\\n",
    "        \"<bootstrapping based rejuvinations|importance sampling weights|particle filtering> \"+\\\n",
    "        \"refers to \"+\\\n",
    "        \"<bootstrapping based rejuvinations|importance sampling weights|particle filtering> \"+\\\n",
    "        \"which are based on \"+\\\n",
    "        \"<bootstrapping based rejuvinations|importance sampling weights|particle filtering>\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80c0702",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 600  # number of particles (i.e., importance sampling proposals)\n",
    "noise_to_signal = 1  # assumed to be known and latent distribution standard deviation 1 as given\n",
    "\n",
    "np.random.seed(9)\n",
    "x[0], y[0] = 0, x[0] + stats.norm(scale=noise_to_signal).rvs()\n",
    "for t in range(1,T):\n",
    "    x[t] = x[t-1] + stats.norm.rvs()\n",
    "    y[t] = x[t] + stats.norm(scale=noise_to_signal).rvs()\n",
    "            \n",
    "np.random.seed(20)\n",
    "x_, x_bar, effective_sample_size = SIR(y.copy(), n, n/3, slash_rvs, slash_pdf)\n",
    "            \n",
    "def plotit():\n",
    "    fig,ax = plt.subplots(2,1, figsize=(15,10))\n",
    "    for i in range(n):\n",
    "        ax[0].plot(x_[i,:], color='gray')\n",
    "    ax[0].plot(x, 'k-', label='latent HMM')\n",
    "    ax[0].plot(y, 'k:', label='Observation')\n",
    "    ax[0].plot(x_bar, 'r', label='SIS HMM estimate')\n",
    "    ax[0].legend(facecolor='gray', framealpha=.5)  \n",
    "    ax[1].plot(effective_sample_size)\n",
    "    ax[1].set_title(\"Effective Sample Size\");\n",
    "    \n",
    "plotit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da72c3e1",
   "metadata": {},
   "source": [
    "> If you think this model is interesting, \n",
    "> Marie Auger-Méthé (UBC) and [our local expert on these methods] Vianey Leos Barajas are organizing the final meeting of our CANSSI collaborative research team on hidden Markov models for biologging data. As part of this meeting, we are organizing a symposium on hidden Markov models. I would like to invite you all and your graduate students to attend virtually.\n",
    "> \n",
    "> If you are interested, please fill in the registration form by Jan 30, 2025: https://docs.google.com/forms/d/e/1FAIpQLSc41-zggGf9XUQ0aEosTIH8fiUct6EJ4DQD5vf5lNPDG7Lhtg/viewform?usp=sharing \n",
    "Join us to discover how hidden Markov models can detect a wide range of phenomena in time series data, from stellar flares, animal behaviours, to musical interpretation! Our two externally invited speakers are Dr. Daniel MacDonald (UBC) and Dr. Jonathan Williams (NC State). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0dadb7",
   "metadata": {},
   "source": [
    "# Part 3: Slice Sampling\n",
    "\n",
    "But super quick so you can devote time to the prelecture reading rather than another pummeling avalanche of my questions walking you through various aspects of this...\n",
    "\n",
    "Slice sampling creates a sequence of $(x,y)_t$ pairs which alternatingly moves vertically by sampling $y_t$ uniformly over $[0, f(x_{t-1})]$ and then horizontally at height $y_t$ by sampling $x_{t}$ uniformly over $[a,b]$ such that $y_t < f(x_t)$. This process produces a sequence of $(x,y)_t$ pairs which \"lie beneath $f$\" in the same spirit as rejection sampling. Indeed, in the example below, if $[a,b]$ is always $[0,1]$ this method will in fact just be rejection sampling exactly. \n",
    "\n",
    "So slice sampling is like cleaver rejection sampling that knows how to make sure the proposals it makes always stay beneath the function, unlike rejection sampling which will generate proposals above the target function which get rejected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43713892",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_f_at_y(f, x, y, x_grid=np.linspace(0,1,51)):\n",
    "    \n",
    "    # find interval of grid points where f(x_grid) > y \n",
    "    # then extend the enterval so endpoints f(a)<y and f(b)<y \n",
    "    x_grid_delta = x_grid[1]-x_grid[0]\n",
    "    a,b = x_grid[f(x_grid)>y][[0,-1]]+[-x_grid_delta,x_grid_delta]\n",
    "    # a,b = x_grid[0,-1]  # make the interval all of x_grid\n",
    "    \n",
    "    x_ = a + stats.uniform().rvs()*(b-a)    \n",
    "    if f(x_)>y:\n",
    "        return x_,1  # in 1 try if f(x_)>y and \"x_ is under f\" \n",
    "    elif x_ < x:  # or if \"x_ was above f on the left side of the interval\" \n",
    "        x_l,x_r = x_,b\n",
    "    else:\n",
    "        x_l,x_r = a,x_  # or if \"x_ was above f on the right side of the interval\"\n",
    "    return slice_f_at_y_(f, x, y, x_l, x_r, tot = 2)  # try again with a reduced interval\n",
    "\n",
    "def slice_f_at_y_(f, x, y, x_l=0, x_r=1, tot=1):\n",
    "    \n",
    "    x_ = x_l + stats.uniform().rvs()*(x_r-x_l)\n",
    "    \n",
    "    if f(x_)>y:\n",
    "        return x_,tot\n",
    "    elif x_ < x:\n",
    "        x_l = x_\n",
    "    else:\n",
    "        x_r = x_\n",
    "    return slice_f_at_y_(f, x, y, x_l, x_r, tot = tot+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f80eda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_grid = np.linspace(0,1,1000)\n",
    "f = lambda x: stats.beta(1.5,3).pdf(x)\n",
    "plt.plot(x_grid, f(x_grid))\n",
    "    \n",
    "m = 1000\n",
    "x = np.zeros([m+1,3])\n",
    "x[:2,0] = 0.25\n",
    "\n",
    "plot_trace = 10\n",
    "for t in range(1,m):\n",
    "    \n",
    "    x[t,1] = stats.uniform().rvs()*f(x[t,0])\n",
    "    if t < plot_trace:\n",
    "        plt.plot([x[t,0]]*2, [x[t-1,1],x[t,1]], 'k')\n",
    "    \n",
    "    x[t+1,0],x[t+1,2] = slice_f_at_y(f, x[t,0], x[t,1])\n",
    "    if t < plot_trace:\n",
    "        if t==1:\n",
    "            plt.plot([x[t,0], x[t+1,0]], [x[t,1]]*2, 'k--', label=str(plot_trace)+ \" iterations\")\n",
    "        plt.plot([x[t,0], x[t+1,0]], [x[t,1]]*2, 'k--')\n",
    "    \n",
    "plt.hist(x[:,0], density=True, label=str(m)+\" iterations\\n x values only\")\n",
    "plt.legend();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c326f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "p3q1 = \"Compared to rejection sampling, slice sampling requires <A|B|C|D>\"\n",
    "# A: an extra uniform sample to produce and x sample \n",
    "# B: an (ideally efficient) calculation of an interval [a,b]\n",
    "# C: both of the above\n",
    "# D: none of the above\n",
    "\n",
    "p3q2 = \"Slice sampling creates <i.i.d.|sequentially markov dependent> samples\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31de5dc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34504457",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
